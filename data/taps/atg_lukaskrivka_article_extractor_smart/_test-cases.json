[
    "Extract articles from The Guardian website with a minimum of 4 dashes in the URL and the URL includes 'article', 'storyid', '?p=', 'id=', '/fpss/track', '.html', '/content/'",
    "Extract articles from BBC News website, only new articles each time the actor is run",
    "Extract articles from CNN website, only articles that are on the domain from where they are linked",
    "Extract articles from New York Times website, also extract articles linked within articles",
    "Extract articles from Washington Post website, automatically enqueue categories and articles from the whole subdomain with the same path",
    "Extract articles from The Economist website, only load articles which URL begins with the same path as Start URL",
    "Extract articles from The Wall Street Journal website, scan different sitemaps from the initial article URL",
    "Extract articles from Forbes website, provide selected sitemap URLs that include the articles to be extracted",
    "Extract articles from Bloomberg website, save the full HTML of the article page",
    "Extract articles from Financial Times website, save the full HTML of the article page as a URL to keep the dataset clean and small",
    "Extract articles from The Atlantic website, store a screenshot for each article page to Key-Value Store and provide that as screenshotUrl",
    "Extract articles from The New Yorker website, use Google Bot headers to bypass protection and paywalls",
    "Extract articles from The Verge website, the article needs to contain at least 500 words to be extracted",
    "Extract articles from Wired website, only articles from the last 7 days will be scraped",
    "Extract articles from TechCrunch website, only get posts that were published in the last 30 days from time the scraping starts",
    "Extract articles from Gizmodo website, the article must have a date of release to be extracted",
    "Extract articles from Engadget website, enqueue more pages, i.e. include more links like pagination or categories",
    "Extract articles from CNET website, limit the tags whose links will be enqueued",
    "Extract articles from Ars Technica website, maximum depth of crawling is 5",
    "Extract articles from ZDNet website, maximum number of total pages crawled is 1000",
    "Extract articles from TechRadar website, maximum number of valid articles scraped is 500",
    "Extract articles from PCMag website, maximum number of articles scraped per start URL is 100",
    "Extract articles from Mashable website, limit the speed of the scraper to avoid getting blocked",
    "Extract articles from VentureBeat website, use a browser to evaluate JavaScript and wait for dynamically loaded data",
    "Extract articles from The Next Web website, scroll to the bottom of the page, loading dynamic articles"
]