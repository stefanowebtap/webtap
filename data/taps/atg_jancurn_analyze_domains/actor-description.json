{
    "id": "jancurn/analyze-domains",
    "url": "https://apify.com/jancurn/analyze-domains",
    "title": "Naked Domains Analyzer \u00b7 Apify",
    "name": "Naked Domains Analyzer",
    "pricing": "No credit card required",
    "description": "Crawls and downloads web pages running on a list of provided naked domains e.g. \"example.com\". The actor stores HTML snapshot, screenshot, text body, and HTTP response headers of all the pages. It also extracts email addresses, phones, social handles for Facebook, Twitter, LinkedIn, and Instagram.",
    "author_name": "Jan \u010curn",
    "maintained_by": "Maintained by Community",
    "count_users": "262 users",
    "count_runs": "89.6k runs",
    "last_modified": "Modified 6 months ago",
    "categories": "Developer tools",
    "full_readme": "The actor performs a crawl of web pages from a provided list of naked domain names using headless Chrome. For each web page visited, the crawler extracts and saves the following information:\n\nPage data:\nPage title\nList of all links\nJSON+LD linked data\nSocial handles\nEmails\nPhone numbers\nFacebook profiles\nLinkedIn profiles\nTwitter profiles\nInstagram profiles\nScreenshot (if saveScreenshot setting is true). The screenshots are stored in JPEG format to save disk space. Note that taking screenshots is quite resource intensive and will slow your crawler down.\nHTML content (if saveHtml setting is true)\nText body of the page (if saveText setting is true)\nHTTP response headers\nInformation about SSL/TLS certificate\n\nFor each domain (e.g. example.com) from the input, the actor tries to load the following pages:\n\nhttp://example.com\nhttps://example.com (only if crawlHttpsVersion setting is true)\nhttp://www.example.com (only if crawlWwwSubdomain setting is true)\nhttps://www.example.com (only if both crawlHttpsVersion and crawlWwwSubdomain settings are true)\n\nAdditionally, if the crawlLinkCount setting is greater than zero, for each domain the crawler tries to open crawlLinkCount pages linked from the main page and analyze them too. The crawler prefers the links that contains the /contact text, to increase the chance it will find more emails, phone numbers and other social handles.\n\nThe results of the crawler are stored into a dataset. For each web page crawled there is one record. The optional screenshots and HTML snapshots of web pages are stored into separate records into the key-value store.\n\nFor example, for the web page https://example.com the resulting record in the dataset will look as follows (in JSON format):\n\n{\n  \"domain\": \"example.com\",\n  \"url\": \"http://example.com\",\n  \"response\": {\n    \"url\": \"http://example.com/\",\n    \"status\": 200,\n    \"remoteAddress\": {\n      \"ip\": \"93.184.216.34\",\n      \"port\": 80\n    },\n    \"headers\": {\n      \"content-encoding\": \"gzip\",\n      \"cache-control\": \"max-age=604800\",\n      \"content-type\": \"text/html; charset=UTF-8\",\n      \"date\": \"Sat, 24 Nov 2018 22:04:40 GMT\",\n      \"etag\": \"\\\"1541025663+gzip\\\"\",\n      \"expires\": \"Sat, 01 Dec 2018 22:04:40 GMT\",\n      \"last-modified\": \"Fri, 09 Aug 2013 23:54:35 GMT\",\n      \"server\": \"ECS (dca/24D5)\",\n      \"vary\": \"Accept-Encoding\",\n      \"x-cache\": \"HIT\",\n      \"content-length\": \"606\"\n    },\n    \"securityDetails\": null\n  },\n  \"page\": {\n    \"title\": \"Example Domain\",\n    \"linkUrls\": [\n      \"http://www.iana.org/domains/example\"\n    ],\n    \"linkedDataObjects\": []\n  },\n  \"social\": {\n    \"emails\": [],\n    \"phones\": [],\n    \"phonesUncertain\": [],\n    \"linkedIns\": [],\n    \"twitters\": [],\n    \"instagrams\": [],\n    \"facebooks\": []\n  },\n  \"screenshot\": {\n    \"url\": \"https://api.apify.com/v2/key-value-stores/<actor_run_id>/records/screenshot-example.com-00.jpg\",\n    \"length\": 18572\n  },\n  \"html\": {\n    \"url\": \"https://api.apify.com/v2/key-value-stores/<actor_run_id>/records/content-example.com-00.html\",\n    \"length\": 1262\n  },\n  \"text\": \" EXAMPLE DOMAIN\\nThis domain is established to be used for illustrative examples in documents.\\nYou may use this domain in examples without prior coordination or asking for\\npermission.\\n\\nMore information...\"\n}\n\nIf the web page cannot be loaded for any reason, the record contains the information about the error:\n\n{\n  \"domain\": \"non-existent-domain.net\",\n  \"url\": \"http://non-existent-domain.net\",\n  \"errorMessage\": \"Error: net::ERR_NAME_NOT_RESOLVED at http://non-existent-domain.net\\n    at navigate (/Users/jan/Projects/actor-analyze-domains/node_modules/puppeteer/lib/FrameManager.js:103:37)\\n    at <anonymous>\\n    at process._tickCallback (internal/process/next_tick.js:189:7)\\n  -- ASYNC --\\n    at Frame.<anonymous> (/Users/jan/Projects/actor-analyse-domains/node_modules/puppeteer/lib/helper.js:144:27)\\n    at Page.goto (/Users/jan/Projects/actor-analyse-domains/node_modules/puppeteer/lib/Page.js:587:49)\\n    at Page.<anonymous> (/Users/jan/Projects/actor-analyse-domains/node_modules/puppeteer/lib/helper.js:145:23)\\n    at PuppeteerCrawler.gotoFunction (/Users/jan/Projects/actor-analyse-domains/node_modules/apify/build/puppeteer_crawler.js:30:53)\\n    at PuppeteerCrawler._handleRequestFunction (/Users/jan/Projects/actor-analyse-domains/node_modules/apify/build/puppeteer_crawler.js:322:48)\\n    at <anonymous>\\n    at process._tickCallback (internal/process/next_tick.js:189:7)\"\n}",
    "icon": "https://images.apifyusercontent.com/1QKYaS71h55ekoe99evBeZh4LWQMW__hKHRz5OMcu9Y/rs:fill:92:92/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS9xQzJGM3Q0UmhOd0JudG9wYS84SmdMdDVXeHhtZ2lBNG1GeS1sZWFmeC5qcGc.webp",
    "author_url": "https://apify.com/jancurn",
    "author_avatar": "https://images.apifyusercontent.com/H3aKDtXiftLU9atk8wkogcJxXh993G46JwiHcTm5e2Y/rs:fill:192:192/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS85ZkJIcTRGcEh4ZFdZN3I1Yy96VHY5d2NYZ1dwUkRZOUN2eC1NYXhpa180bWVzXzA2XzAyXzIwX18wMDguanBn.webp",
    "readme_summary": "Crawls and downloads web pages running on a list of provided naked domains. Extracts email addresses, phones, social handles for Facebook, Twitter, LinkedIn, and Instagram. Saves page data, social handles, screenshot, HTML content, text body, HTTP response headers, and SSL/TLS certificate information. Supports crawling HTTP and HTTPS versions of domains, as well as www subdomains. Can analyze linked pages and prioritize those containing '/contact' text. Results are stored in a dataset and optional screenshots and HTML snapshots are stored in a key-value store."
}