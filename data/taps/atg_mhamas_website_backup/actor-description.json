{
    "id": "mhamas/website-backup",
    "url": "https://apify.com/mhamas/website-backup",
    "title": "Website Backup \u00b7 Apify",
    "name": "Website Backup",
    "pricing": "No credit card required",
    "description": "Enables to create a backup of any website by crawling it, so that you don\u2019t lose any content by accident. Ideal e.g. for your personal or company blog.",
    "author_name": "Matej Hamas",
    "maintained_by": "Maintained by Community",
    "count_users": "179 users",
    "count_runs": "6.2k runs",
    "last_modified": "Modified over 2 years ago",
    "categories": "Automation",
    "full_readme": "Apify Actor - Website Backup\nDescription\n\nThe purpose of this actor is to enable creation of website backups by recursively crawling them. For example, we\u2019d use it to make regular backups of https://blog.apify.com/, so that we don\u2019t lose any content by accident. Although such backup cannot be automatically restored, it\u2019s better than losing data completely.\n\nGiven URL entry points, the actors recursively crawls the links found on the pages using a provided CSS selector and create a separate MHTML snapshot of each page. Each snapshot is taken after the full page is rendered with Puppeteer crawler and includes all the content such as images and CSS. Hence, it can be used on any HTML / JS / Wordpress web sites which don't require authentication.\n\nInput parameters\nField\tType\tDescription\nstartURLs\tarray\tList of URL entry points\nlinkSelector\tstring\tCSS selector matching elements with 'href' attributes that should be enqueued\nmaxRequestsPerCrawl\tinteger\tThe maximum number of pages that the scraper will load. The scraper will stop when this limit is reached. It's always a good idea to set this limit in order to prevent excess platform usage for misconfigured scrapers. Note that the actual number of pages loaded might be slightly higher than this value.If set to 0, there is no limit.\nmaxCrawlingDepth\tinteger\tDefines how many links away from the StartURLs will the scraper descend. 0 means unlimited.\nmaxConcurrency\tinteger\tDefines how many pages can be processed by the scraper in parallel. The scraper automatically increases and decreases concurrency based on available system resources. Use this option to set a hard limit.\ncustomKeyValueStore\tstring\tUse custom named key value store for saving results. If the key value store with this name doesn't yet exist, it's created. The snapshots of the pages will be saved in the key value store.\ncustomDataset\tstring\tUse custom named dataset for saving metadata. If the dataset with this name doesn't yet exist, it's created. The metadata about the snapshots of the pages will be saves in the dataset.\nproxyConfiguration\tobject\tChoose to use no proxy, Apify Proxy, or provide custom proxy URLs.\nsameOrigin\tboolean\tOnly backup URLs with the same origin as any of the start URL origins. E.g. when turned on for a single start URL https://blog.apify.com, only links with prefix https://blog.apify.com will be backed up recursively.\ntimeoutForSingleUrlInSeconds\tinteger\tTimeout in seconds for doing a backup of a single URL. Try to increase this timeout in case you see an error Error: handlePageFunction timed out after X seconds. .\nnavigationTimeoutInSeconds\tinteger\tTimeout in seconds in which the navigation needs to finish. Try to increase this if you see an error Navigation timeout of XXX ms exceeded\nsearchParamsToIgnore\tarray\tNames of URL search parameters (such as 'source', 'sourceid', etc.) that should be ignored in the URLs when crawling.\nOutput\n\nSingle zip file containing MHTML snapshot and its metadata is stored in a key value store (default or named depending on the input argument) for each URL visited. The key for each zip file includes a timestamp, URL hash and the URL in a human readable form. Note that the Apify platform only supports certain characters and limits the length of the key to 256 characters (that is why e.g. / is removed). Apart from the key value store, metadata for the crawled webpages are also stored in a dataset (default or named).\n\nCompute unit consumption\n\nAn example run which did a backup of 323 webpages under <a href='https://blog.apify.com\">blog.apify.com, configured with 8192 Mb of memory and lasting 12 minutes consumed 1.6617 compute units.",
    "icon": "https://images.apifyusercontent.com/5ZsDPAnx6cJErjTC-jz3UThXTs6A_YZX2DelCQCfZqA/rs:fill:92:92/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS80ZWt0SERKSjkyUDZUWjh3Vi9aSzNpU0tFaUpiSGc3cWVGRy1pbWFnZXNfJTI4MiUyOS5wbmc.webp",
    "author_url": "https://apify.com/mhamas",
    "author_avatar": "https://images.apifyusercontent.com/l_Ef9Ua3pNo01A-lU9qKAr_chvXmjbSPvAX9b5YyiIk/rs:fill:192:192/aHR0cHM6Ly9saDUuZ29vZ2xldXNlcmNvbnRlbnQuY29tLy1SY0ZDYnk4N0V3by9BQUFBQUFBQUFBSS9BQUFBQUFBQUFBQS9BTVp1dWNscmlieTFoY3FWQXE4cXNBMml4RkFvV003dGJ3L3Bob3RvLmpwZw.webp",
    "readme_summary": "# Apify Actor - Website Backup\n\nEnables to create a backup of any website by crawling it, so that you don\u2019t lose any content by accident. Ideal e.g. for your personal or company blog.\n\nInput parameters:\n- startURLs: List of URL entry points\n- linkSelector: CSS selector matching elements with 'href' attributes that should be enqueued\n- maxRequestsPerCrawl: The maximum number of pages that the scraper will load\n- maxCrawlingDepth: Defines how many links away from the StartURLs will the scraper descend\n- maxConcurrency: Defines how many pages can be processed by the scraper in parallel\n- customKeyValueStore: Use custom named key value store for saving results\n- customDataset: Use custom named dataset for saving metadata\n- proxyConfiguration: Choose to use no proxy, Apify Proxy, or provide custom proxy URLs\n- sameOrigin: Only backup URLs with the same origin as any of the start URL origins\n- timeoutForSingleUrlInSeconds: Timeout in seconds for doing a backup of a single URL\n- navigationTimeoutInSeconds: Timeout in seconds in which the navigation needs to finish\n- searchParamsToIgnore: Names of URL search parameters that should be ignored in the URLs when crawling\n\nOutput: Single zip fi"
}