{
    "id": "trudax/reddit-scraper-lite",
    "url": "https://apify.com/trudax/reddit-scraper-lite",
    "title": "Free Reddit Scraper \u00b7 Apify",
    "name": "Reddit Scraper Lite",
    "pricing": "Pay $3.50 for 1,000 results",
    "description": "Reddit web scraper to crawl posts, comments, communities, and users without login.",
    "author_name": "Gustavo Rudiger",
    "maintained_by": "Maintained by Community",
    "count_users": "1.7k users",
    "count_runs": "52.3k runs",
    "last_modified": "Modified 11 days ago",
    "categories": "Social media",
    "full_readme": "What does Reddit Scraper do?\n\nOur unofficial Reddit API will get data from Reddit with no limitations or authentication. It enables you to extract posts and comments together with some user info without login. It is built on top of Apify SDK, and you can run it both on the Apify platform and locally.\n\nReddit Scraper allows you to:\n\nscrape subreddits (communities) with top posts\nscrape Reddit posts with title and text, username, number of comments, votes, media elements.\nget Reddit comments, timestamps, points, usernames, post and comment URLs.\nscrape user details, their most recent posts and comments.\nsort scraped data by categories of Relevance, Hot, Top, and New.\nscrape data using a specific URL or by keyword.\nWhat data can I extract from Reddit?\n\ud83d\udccc Popular subreddits\t\ud83d\udd0d Subreddit details\n\ud83d\udccb Subreddit name\t\ud83d\udc65 Number of members\n\ud83c\udf10 Community URL\t\ud83d\udcda Category\n\ud83d\udccc Reddit posts\t\ud83d\udcac Reddit comments\n\ud83d\udcc3 Title and text\t\u23f1 Timestamps\n\ud83d\udc64 Username\t\ud83d\udd17 Post and comment URLs\n\ud83d\udc4d Votes\t\ud83d\udcf7 Media elements\n\ud83d\udc64 User details\t\ud83d\udcc4 Recent posts and comments\nHow much will it cost to scrape Reddit?\n\nReddit Scraper on the Apify platform will give you 1,000 results for less than $4 in platform usage credits. That should be covered by the free $5 in monthly credits you get on every Apify Free plan.\n\nBut if you need to get more data regularly from Reddit, you should grab an Apify subscription. We recommend our $49/month Starter plan - with that one, you can get well over 10,000 results every month! Watch this video for a few helpful tips on how to pick a plan.\n\nHow to scrape Reddit?\n\nReddit Scraper doesn't require any coding skills to start using it.\n\nCreate a free Apify account using your email.\nOpen the Reddit Scraper.\nAdd one or more subreddits, users or post URLs to scrape their information.\nClick \"Start\" and wait for the scraper to extract the data.\nDownload your data in JSON, XML, CSV, Excel, or HTML format.\n\nIf you're unsure where to start, just follow our step-by-step guide or see our short video tutorial. The tutorial steps can be also be used for Reddit Scraper Lite.\n\nHow to use scraped Reddit data\nKeep track of discussions about your brand or product across Reddit communities.\nResearch the topics that interest you and get a wide range of opinions.\nKeep an eye on debates over high stakes subjects such as finance, politics, new technology, and news in general.\nWatch out for new trends, attitudes, and PR opportunities.\nAutomatically track mentions of the business or topic that interests you.\nScrape Reddit comments to kick off and support your sentiment analysis.\nInput parameters\n\nIf this Actor is run on the Apify platform, there are two ways you can scrape Reddit:\n\nby Start URLs field - this will get you all details from any Reddit URL, no matter whether it's a post, a user, or a community.\nor by Search Term field - this will scrape all data from Reddit in Communities, Posts, and People for a specific keyword.\nHow to scrape Reddit by URLs\n\nAlmost any URL from Reddit will return a dataset. If the URL is not supported, the scraper will display a message before scraping the page.\n\nInput examples:\n\nHere are some examples of URLs that can be scraped.\n\nscraping communities: https://www.reddit.com/r/worldnews/\n\nscraping channels within communities: https://www.reddit.com/r/worldnews/hot\n\nscraping popular communities: https://www.reddit.com/subreddits/leaderboard/crypto/\n\nscraping users: https://www.reddit.com/user/lukaskrivka\n\nscraping user comments: https://www.reddit.com/user/lukaskrivka/comments/\n\nscraping posts: https://www.reddit.com/r/learnprogramming/comments/lp1hi4/is_webscraping_a_good_skill_to_learn_as_a_beginner/\n\nscraping popular posts: https://www.reddit.com/r/popular/\n\nscraping search results:\n\nfor users/communities: https://www.reddit.com/search/?q=news&type=sr%2Cuser\n\nfor posts: https://www.reddit.com/search/?q=news\n\nNote: if you use a search URL as a parameter for startUrls, it will only scrape for posts. If you want to search for communities and users, use the search field or a specific URL instead.\n\nHow to scrape Reddit by search term\n\nSearch Term or searches - the keywords you want to search via the Reddit's search engine. You can keep one field or add as many as you want. Don't use this field if you're using the startUrls parameter.\n\nSearch type or type - indicates which part of Reddit you're scraping: \"Posts\" or \"Communities and users\".\n\nSort search or sort - will sort search results by Relevance, Hot, Top, New or most amount of Comments.\n\nFilter by date or time - will filter the search by the last hour, day, week, month or year. Only available if you're scraping Posts.\n\nTo see the full list of parameters, their default values, and how to set the values of your own, head over to Input Schema tab.\n\nInput example\n\nThis is an example of how your input will look like if you decide to scrape all Reddit communities that contain the keyword parrot. Results will be sorted by the newest first.\n\n{\n  \"maxItems\": 10,\n  \"maxPostCount\": 10,\n  \"maxComments\": 10,\n  \"maxCommunitiesCount\": 10,\n  \"maxUserCount\": 10,\n  \"maxLeaderBoardItems\": 10,\n  \"scrollTimeout\": 40,\n  \"proxy\": {\n    \"useApifyProxy\": true\n  },\n  \"searches\": [\"parrots\"],\n  \"type\": \"community\",\n  \"sort\": \"new\",\n  \"time\": \"all\"\n}\nResults\n\nThe output from scraping Reddit is stored in the dataset. Each post, comment, user or community is stored as an item inside the dataset. After the run is finished, you can download the scraped data onto your computer or export to any web app in various data formats (JSON, CSV, XML, RSS, HTML Table). Here's a few examples of the outputs you can get for different types of inputs:\n\n\ud83d\udcdd Example Reddit post\n{\n  \"id\": \"t3_144w7sn\",\n  \"parsedId\": \"144w7sn\",\n  \"url\": \"https://www.reddit.com/r/HonkaiStarRail/comments/144w7sn/my_luckiest_10x_pull_yet/\",\n  \"username\": \"YourKingLives\",\n  \"title\": \"My Luckiest 10x Pull Yet\",\n  \"communityName\": \"r/HonkaiStarRail\",\n  \"parsedCommunityName\": \"HonkaiStarRail\",\n  \"body\": \"URL: https://i.redd.it/yod3okjkgx4b1.jpg\\nThumbnail: https://b.thumbs.redditmedia.com/lm9KxS4laQWgx4uOoioM3N7-tBK3GLPrxb9da2hGtjs.jpg\\nImages:\\n\\thttps://preview.redd.it/yod3okjkgx4b1.jpg?auto=webp&amp;v=enabled&amp;s=be5faf0250e19138b82c7bbe5e7406fa46da4e73\\n\",\n  \"html\": null,\n  \"numberOfComments\": 0,\n  \"upVotes\": 1,\n  \"isVideo\": false,\n  \"isAd\": false,\n  \"over18\": false,\n  \"createdAt\": \"2023-06-09T05:23:15.000Z\",\n  \"scrapedAt\": \"2023-06-09T05:23:28.409Z\",\n  \"dataType\": \"post\"\n},\n\ud83d\udcac Example Reddit comment\n{\n  \"id\": \"t1_jnhqrgg\",\n  \"parsedId\": \"jnhqrgg\",\n  \"url\": \"https://www.reddit.com/r/NewsWithJingjing/comments/144v5c3/theres_no_flag_large_enough/jnhqrgg/\",\n  \"parentId\": \"t3_144v5c3\",\n  \"username\": \"smokecat20\",\n  \"category\": \"NewsWithJingjing\",\n  \"communityName\": \"r/NewsWithJingjing\",\n  \"body\": \"A true patriot.\",\n  \"createdAt\": \"2023-06-09T05:00:00.000Z\",\n  \"scrapedAt\": \"2023-06-09T05:23:32.025Z\",\n  \"upVotes\": 3,\n  \"numberOfreplies\": 0,\n  \"html\": \"&lt;div class=\\\"md\\\"&gt;&lt;p&gt;A true patriot.&lt;/p&gt;\\n&lt;/div&gt;\",\n  \"dataType\": \"comment\"\n}\n\ud83d\udc65 Example Reddit community\n{\n  \"id\": \"2qlhq\",\n  \"name\": \"t5_2qlhq\",\n  \"title\": \"Pizza\",\n  \"headerImage\": \"https://b.thumbs.redditmedia.com/jq9ytPEOecwd5bmGIvNQzjTPE9hdd0kB9XGa--wq55A.png\",\n  \"description\": \"The home of pizza on reddit. An educational community devoted to the art of pizza making.\",\n  \"over18\": false,\n  \"createdAt\": \"2008-08-26T00:03:48.000Z\",\n  \"scrapedAt\": \"2023-06-09T05:16:55.443Z\",\n  \"numberOfMembers\": 569724,\n  \"url\": \"https://www.reddit.com/r/Pizza/\",\n  \"dataType\": \"community\"\n}\n\ud83d\udc64 Example Reddit user\n{\n  \"id\": \"c3h2qmv\",\n  \"url\": \"https://www.reddit.com/user/jancurn/\",\n  \"username\": \"jancurn\",\n  \"userIcon\": \"https://www.redditstatic.com/avatars/defaults/v2/avatar_default_7.png\",\n  \"karma\": 4,\n  \"description\": \"\",\n  \"over18\": false,\n  \"createdAt\": \"2018-09-10T15:13:39.000Z\",\n  \"scrapedAt\": \"2023-06-09T05:21:14.409Z\",\n  \"dataType\": \"user\"\n}\nOnly need a few Reddit results?\n\nUse our super fast dedicated Reddit Scraper Lite if you want to scrape Reddit data on a smaller scale. Just enter one or more Reddit URLs or keywords and click to scrape.\n\nNotes for developers\nLimiting results with maxItems\n\nIf you need to limit the scope of your search, you can do that by setting the max number of posts you want to scrape inside a community or user. You can also set a limit to the number of comments for each post. You can limit the number of communities and the number of leaderboards by using the following parameters:\n\n{\n  \"maxItems\": 100,\n  \"maxPostCount\": 50,\n  \"maxComments\": 10,\n  \"maxCommunitiesCount\": 5,\n  \"maxUserCount\": 5,\n  \"maxLeaderBoardsItems\": 5\n}\n\nYou can also set maxItems to prevent a very long run of the Actor. This parameter will stop your scraper when it reaches the number of results you've indicated, so you need to be careful not to trim your results.\n\nSee the Input Schema tab for the full list of the ways to restrict Reddit Scraper using these parameters: maxItems, maxPostCount, maxComments, maxCommunitiesCount, maxLeaderBoardItems\n\nExtend output function\n\nYou can use this function to update the result output of this Actor. You can choose what data from the page you want to scrape. The output from this function will get merged with the result output.\n\nThe return value of this function has to be an object!\n\nYou can return fields to achieve 3 different things:\n\nAdd a new field - Return object with a field that is not in the result output\nChange a field - Return an existing field with a new value\nRemove a field - Return an existing field with a value undefined\nasync () => {\n  return {\n    pageTitle: document.querySelector('title').innerText,\n  };\n};\n\nThis example will add the title of the page to the final object:\n\n{\n  \"id\": \"2qlhq\",\n  \"name\": \"t5_2qlhq\",\n  \"title\": \"Pizza\",\n  \"headerImage\": \"https://b.thumbs.redditmedia.com/jq9ytPEOecwd5bmGIvNQzjTPE9hdd0kB9XGa--wq55A.png\",\n  \"description\": \"The home of pizza on reddit. An educational community devoted to the art of pizza making.\",\n  \"over18\": false,\n  \"createdAt\": \"2008-08-26T00:03:48.000Z\",\n  \"scrapedAt\": \"2023-06-09T05:16:55.443Z\",\n  \"numberOfMembers\": 569724,\n  \"url\": \"https://www.reddit.com/r/Pizza/\",\n  \"dataType\": \"community\"\n  \"pageTitle\": \"homemade chicken cheese masala pasta\"\n}\nFAQ\nIs Reddit scraping legal?\n\nWhile scraping publicly available data from Reddit is generally allowed, it's important to comply with Reddit's terms of service and respect the site's usage policies. It's recommended to use the scraper responsibly, avoid excessive requests, and ensure that the scraped data is used in compliance with applicable laws and regulations. You can read more about compliance with ToS in our blogpost.\n\nCan I use Reddit API to scrape Reddit?\n\nThe Reddit API is currently free. However, Reddit has specific API rules, and free access to the API will be restricted in the future due to concerns over data usage. In addition, Reddit API has been recently announced to become a paid service. Using a Reddit web scraper such as this one as an API has advantages such over the official one such as not requiring authentication, special authorization for commercial use, or registration for a token.\n\nHow can I scrape Reddit comments?\n\nReddit Scraper allows scraping specific parts of Reddit, including comments. You can extract posts and comments along with user information, such as timestamps, number of votes, usernames, post URL, and comment URLs. This enables you to gather comprehensive commment data from subreddits and Reddit users.\n\nIs it necessary to use cookies for accessing logged-in content when scraping Reddit?\n\nNo, it is not required. As of May 2023, Reddit maintains its data publicly accessible and does not enforce a login barrier.\n\nDo you need proxies for scraping Reddit?\n\nIt is highly recommended. Subreddits are open for access and do not require a login to retrieve information. Typically, using proxies is necessary to ensure successful Reddit scraping. While some results can be obtained with datacenter proxies, residential proxies are preferred for Reddit scraping. Fortunately, our Free plan offers a trial of Apify Proxy, which get you started.\n\nCan I export or import scraped Reddit data using API?\n\nYes. The Apify API gives you programmatic access to the Apify platform. The API is organized around RESTful HTTP endpoints that enable you to manage, schedule, and run any Apify Actor, including this one. The API also lets you access any datasets, monitor Actor performance, fetch results, create and update versions, and more.\n\nTo access the API using Node.js, use the apify-client NPM package. To access the API using Python, use the apify-client PyPi package.\n\nCheck out the Apify API reference docs for full details or click on the API tab for code examples.\n\nHow can I build a Reddit scraper in Python?\n\nYou can create your own Reddit web scraper using a Python scraper template directly on the Apify platform and keep production there. Alternatively, you can develop it locally on your computer and only push it to the Apify cloud during deployment.",
    "icon": "https://images.apifyusercontent.com/niZ4X22Y_Yc6QrqABs0Q7PNEbJAOR2PAqjGuPHSSYRY/rs:fill:92:92/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS9vQXVDSXgzSXROcnMyb2tqUS8zaEJneWc0RURCc29wUWl4RC1ycTM2a2wxeGp4cjAxLnBuZw.webp",
    "author_url": "https://apify.com/trudax",
    "author_avatar": "https://images.apifyusercontent.com/vZqCDyipjIub5QI0eFCKZryzoVgsZsClOYKcu2YPWHE/rs:fill:192:192/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS9pNHBMUWE5RmV5VHE4UndURC9TTG1tZ2did1pNcUh6ekU3YS1HdXN0YXZvLmpwZw.webp",
    "readme_summary": "Reddit web scraper to crawl posts, comments, communities, and users without login."
}