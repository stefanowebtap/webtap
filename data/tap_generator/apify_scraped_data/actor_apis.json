{"apify/web-scraper": {"id": 757, "url": "https://apify.com/apify/web-scraper/api/client/nodejs", "title": "Apify API and Web Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"runMode\": \"DEVELOPMENT\",\n    \"startUrls\": [\n        {\n            \"url\": \"https://crawlee.dev\"\n        }\n    ],\n    \"linkSelector\": \"a[href]\",\n    \"globs\": [\n        {\n            \"glob\": \"https://crawlee.dev/*/*\"\n        }\n    ],\n    \"pseudoUrls\": [],\n    \"excludes\": [\n        {\n            \"glob\": \"/**/*.{png,jpg,jpeg,pdf}\"\n        }\n    ],\n    \"pageFunction\": // The function accepts a single argument: the \"context\" object.\n    // For a complete list of its properties and functions,\n    // see https://apify.com/apify/web-scraper#page-function \n    async function pageFunction(context) {\n        // This statement works as a breakpoint when you're trying to debug your code. Works only with Run mode: DEVELOPMENT!\n        // debugger; \n    \n        // jQuery is handy for finding DOM elements and extracting data from them.\n        // To use it, make sure to enable the \"Inject jQuery\" option.\n        const $ = context.jQuery;\n        const pageTitle = $('title').first().text();\n        const h1 = $('h1').first().text();\n        const first_h2 = $('h2').first().text();\n        const random_text_from_the_page = $('p').first().text();\n    \n    \n        // Print some information to actor log\n        context.log.info(`URL: ${context.request.url}, TITLE: ${pageTitle}`);\n    \n        // Manually add a new page to the queue for scraping.\n       await context.enqueueRequest({ url: 'http://www.example.com' });\n    \n        // Return an object with the data extracted from the page.\n        // It will be stored to the resulting dataset.\n        return {\n            url: context.request.url,\n            pageTitle,\n            h1,\n            first_h2,\n            random_text_from_the_page\n        };\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"initialCookies\": [],\n    \"waitUntil\": [\n        \"networkidle2\"\n    ],\n    \"preNavigationHooks\": `// We need to return array of (possibly async) functions here.\n        // The functions accept two arguments: the \"crawlingContext\" object\n        // and \"gotoOptions\".\n        [\n            async (crawlingContext, gotoOptions) => {\n                // ...\n            },\n        ]`,\n    \"postNavigationHooks\": `// We need to return array of (possibly async) functions here.\n        // The functions accept a single argument: the \"crawlingContext\" object.\n        [\n            async (crawlingContext) => {\n                // ...\n            },\n        ]`,\n    \"breakpointLocation\": \"NONE\",\n    \"customData\": {}\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/web-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"globs": [{"glob": "https://crawlee.dev/*/*"}], "runMode": "DEVELOPMENT", "excludes": [{"glob": "/**/*.{png,jpg,jpeg,pdf}"}], "startUrls": [{"url": "https://crawlee.dev"}], "waitUntil": ["networkidle2"], "customData": {}, "pseudoUrls": [], "linkSelector": "a[href]", "initialCookies": [], "breakpointLocation": "NONE", "preNavigationHooks": "// We need to return array of (possibly async) functions here.\n        // The functions accept two arguments: the \"crawlingContext\" object\n        // and \"gotoOptions\".\n        [\n            async (crawlingContext, gotoOptions) => {\n                // ...\n            },\n        ]", "proxyConfiguration": {"useApifyProxy": true}, "postNavigationHooks": "// We need to return array of (possibly async) functions here.\n        // The functions accept a single argument: the \"crawlingContext\" object.\n        [\n            async (crawlingContext) => {\n                // ...\n            },\n        ]"}, "actor_id": "apify/web-scraper"}, "apify/google-search-scraper": {"id": 758, "url": "https://apify.com/apify/google-search-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udd0e Google Search scraper and SERP API interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"queries\": \"Food in NYC\",\n    \"maxPagesPerQuery\": 1,\n    \"resultsPerPage\": 100,\n    \"customDataFunction\": async ({ input, $, request, response, html }) => {\n      return {\n        pageTitle: $('title').text(),\n      };\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/google-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"queries": "Food in NYC", "resultsPerPage": 100, "maxPagesPerQuery": 1}, "actor_id": "apify/google-search-scraper"}, "compass/crawler-google-places": {"id": 759, "url": "https://apify.com/compass/crawler-google-places/api/client/nodejs", "title": "Apify API and \ud83d\udccd Google Maps Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchStringsArray\": [\n        \"restaurant\"\n    ],\n    \"locationQuery\": \"New York, USA\",\n    \"maxCrawledPlacesPerSearch\": 50,\n    \"language\": \"en\",\n    \"maxImages\": 0,\n    \"maxReviews\": 0,\n    \"countryCode\": \"\",\n    \"allPlacesNoSearchAction\": \"\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"compass/crawler-google-places\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"language": "en", "maxImages": 0, "maxReviews": 0, "countryCode": "", "locationQuery": "New York, USA", "searchStringsArray": ["restaurant"], "allPlacesNoSearchAction": "", "maxCrawledPlacesPerSearch": 50}, "actor_id": "compass/crawler-google-places"}, "apify/website-content-crawler": {"id": 760, "url": "https://apify.com/apify/website-content-crawler/api/client/nodejs", "title": "Apify API and Website Content Crawler interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://docs.apify.com/academy/web-scraping-for-beginners\"\n        }\n    ],\n    \"includeUrlGlobs\": [],\n    \"excludeUrlGlobs\": [],\n    \"initialCookies\": [],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"removeElementsCssSelector\": \"nav, footer, script, style, noscript, svg,\\n[role=\\\"alert\\\"],\\n[role=\\\"banner\\\"],\\n[role=\\\"dialog\\\"],\\n[role=\\\"alertdialog\\\"],\\n[role=\\\"region\\\"][aria-label*=\\\"skip\\\" i],\\n[aria-modal=\\\"true\\\"]\",\n    \"clickElementsCssSelector\": \"[aria-expanded=\\\"false\\\"]\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/website-content-crawler\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://docs.apify.com/academy/web-scraping-for-beginners"}], "initialCookies": [], "excludeUrlGlobs": [], "includeUrlGlobs": [], "proxyConfiguration": {"useApifyProxy": true}, "clickElementsCssSelector": "[aria-expanded=\"false\"]", "removeElementsCssSelector": "nav, footer, script, style, noscript, svg,\n[role=\"alert\"],\n[role=\"banner\"],\n[role=\"dialog\"],\n[role=\"alertdialog\"],\n[role=\"region\"][aria-label*=\"skip\" i],\n[aria-modal=\"true\"]"}, "actor_id": "apify/website-content-crawler"}, "junglee/amazon-crawler": {"id": 761, "url": "https://apify.com/junglee/amazon-crawler/api/client/nodejs", "title": "Apify API and Scrape Amazon reviews, prices, products, and ASINs interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"categoryOrProductUrls\": [\n        {\n            \"url\": \"https://www.amazon.com/s?i=specialty-aps&bbn=16225009011&rh=n%3A%2116225009011%2Cn%3A2811119011&ref=nav_em__nav_desktop_sa_intl_cell_phones_and_accessories_0_2_5_5\"\n        }\n    ],\n    \"maxItems\": 100,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"junglee/amazon-crawler\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 100, "proxyConfiguration": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "categoryOrProductUrls": [{"url": "https://www.amazon.com/s?i=specialty-aps&bbn=16225009011&rh=n%3A%2116225009011%2Cn%3A2811119011&ref=nav_em__nav_desktop_sa_intl_cell_phones_and_accessories_0_2_5_5"}]}, "actor_id": "junglee/amazon-crawler"}, "apify/instagram-profile-scraper": {"id": 762, "url": "https://apify.com/apify/instagram-profile-scraper/api/client/nodejs", "title": "Apify API and Instagram Profile Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"usernames\": [\n        \"humansofny\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/instagram-profile-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"usernames": ["humansofny"]}, "actor_id": "apify/instagram-profile-scraper"}, "clockworks/tiktok-scraper": {"id": 763, "url": "https://apify.com/clockworks/tiktok-scraper/api/client/nodejs", "title": "Apify API and TikTok Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"hashtags\": [\n        \"followforfollowback\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"clockworks/tiktok-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"hashtags": ["followforfollowback"]}, "actor_id": "clockworks/tiktok-scraper"}, "quacker/twitter-scraper": {"id": 764, "url": "https://apify.com/quacker/twitter-scraper/api/client/nodejs", "title": "Apify API and Twitter Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"handles\": [\n        \"Apify\"\n    ],\n    \"tweetsDesired\": 100,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"quacker/twitter-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"handles": ["Apify"], "proxyConfig": {"useApifyProxy": true}, "tweetsDesired": 100}, "actor_id": "quacker/twitter-scraper"}, "drobnikj/gpt-scraper": {"id": 765, "url": "https://apify.com/drobnikj/gpt-scraper/api/client/nodejs", "title": "Apify API and GPT Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://news.ycombinator.com/\"\n        }\n    ],\n    \"globs\": [],\n    \"linkSelector\": \"a[href]\",\n    \"instructions\": \"Get from the page the post with the most points and returns it as JSON in format:\\npostTitle\\npostUrl\\npointsCount\",\n    \"targetSelector\": \"\",\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"title\": {\n                \"type\": \"string\",\n                \"description\": \"Page title\"\n            },\n            \"description\": {\n                \"type\": \"string\",\n                \"description\": \"Page description\"\n            }\n        },\n        \"required\": [\n            \"title\",\n            \"description\"\n        ]\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"drobnikj/gpt-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"globs": [], "schema": {"type": "object", "required": ["title", "description"], "properties": {"title": {"type": "string", "description": "Page title"}, "description": {"type": "string", "description": "Page description"}}}, "startUrls": [{"url": "https://news.ycombinator.com/"}], "instructions": "Get from the page the post with the most points and returns it as JSON in format:\npostTitle\npostUrl\npointsCount", "linkSelector": "a[href]", "targetSelector": "", "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "drobnikj/gpt-scraper"}, "equidem/ai-product-matcher": {"id": 766, "url": "https://apify.com/equidem/ai-product-matcher/api/client/nodejs", "title": "Apify API and \ud83c\udfaf Match products with AI interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"dataset1_ids\": [\n        \"GYVCj4hEeqnX3dJyu\"\n    ],\n    \"dataset2_ids\": [\n        \"OmzHV4VEByO4KohMF\"\n    ],\n    \"input_mapping\": {\n        \"eshop1\": {\n            \"id\": \"url\",\n            \"name\": \"name\",\n            \"price\": \"price\",\n            \"short_description\": \"shortDescription\",\n            \"long_description\": \"longDescription\",\n            \"specification\": \"specification\",\n            \"code\": [\n                \"sku\",\n                \"productModel\"\n            ]\n        },\n        \"eshop2\": {\n            \"id\": \"url\",\n            \"name\": \"name\",\n            \"price\": \"price\",\n            \"short_description\": \"shortDescription\",\n            \"long_description\": \"longDescription\",\n            \"specification\": \"specification\",\n            \"code\": [\n                \"sku\",\n                \"productModel\"\n            ]\n        }\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"equidem/ai-product-matcher\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"dataset1_ids": ["GYVCj4hEeqnX3dJyu"], "dataset2_ids": ["OmzHV4VEByO4KohMF"], "input_mapping": {"eshop1": {"id": "url", "code": ["sku", "productModel"], "name": "name", "price": "price", "specification": "specification", "long_description": "longDescription", "short_description": "shortDescription"}, "eshop2": {"id": "url", "code": ["sku", "productModel"], "name": "name", "price": "price", "specification": "specification", "long_description": "longDescription", "short_description": "shortDescription"}}}, "actor_id": "equidem/ai-product-matcher"}, "vdrmota/contact-info-scraper": {"id": 767, "url": "https://apify.com/vdrmota/contact-info-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udce9 Contact Details Scraper and Email Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://apify.com\"\n        }\n    ],\n    \"maxRequestsPerStartUrl\": 20,\n    \"maxDepth\": 2,\n    \"maxRequests\": 9999999\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"vdrmota/contact-info-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDepth": 2, "startUrls": [{"url": "https://apify.com"}], "maxRequests": 9999999, "maxRequestsPerStartUrl": 20}, "actor_id": "vdrmota/contact-info-scraper"}, "jakubbalada/content-checker": {"id": 768, "url": "https://apify.com/jakubbalada/content-checker/api/client/nodejs", "title": "Apify API and Free content checker to monitor website changes interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.apify.com/change-log\",\n    \"contentSelector\": \"article\",\n    \"screenshotSelector\": \"article\",\n    \"sendNotificationText\": \"Apify found a new change!\",\n    \"proxy\": {\n        \"useApifyProxy\": false\n    },\n    \"navigationTimeout\": 30000\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jakubbalada/content-checker\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.apify.com/change-log", "proxy": {"useApifyProxy": false}, "contentSelector": "article", "navigationTimeout": 30000, "screenshotSelector": "article", "sendNotificationText": "Apify found a new change!"}, "actor_id": "jakubbalada/content-checker"}, "glenn/gif-scroll-animation": {"id": 769, "url": "https://apify.com/glenn/gif-scroll-animation/api/client/nodejs", "title": "Apify API and Create animated GIFs of any scrolling web page interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://crawlee.dev/\",\n    \"proxyOptions\": {\n        \"useApifyProxy\": true\n    },\n    \"frameRate\": 7,\n    \"scrollPercentage\": 10,\n    \"recordingTimeBeforeAction\": 1000\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"glenn/gif-scroll-animation\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://crawlee.dev/", "frameRate": 7, "proxyOptions": {"useApifyProxy": true}, "scrollPercentage": 10, "recordingTimeBeforeAction": 1000}, "actor_id": "glenn/gif-scroll-animation"}, "emastra/google-trends-scraper": {"id": 770, "url": "https://apify.com/emastra/google-trends-scraper/api/client/nodejs", "title": "Apify API and Google Trends Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchTerms\": [\n        \"webscraping\"\n    ],\n    \"timeRange\": \"\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"emastra/google-trends-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"timeRange": "", "searchTerms": ["webscraping"]}, "actor_id": "emastra/google-trends-scraper"}, "apify/send-mail": {"id": 771, "url": "https://apify.com/apify/send-mail/api/client/nodejs", "title": "Apify API and Send Email - Webhook to send mail interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"to\": \"info@apify.com, hello@apify.com\",\n    \"subject\": \"Send Email notification\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/send-mail\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"to": "info@apify.com, hello@apify.com", "subject": "Send Email notification"}, "actor_id": "apify/send-mail"}, "apify/puppeteer-scraper": {"id": 772, "url": "https://apify.com/apify/puppeteer-scraper/api/client/nodejs", "title": "Apify API and Puppeteer Scraper for headless Chrome interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://crawlee.dev\"\n        }\n    ],\n    \"globs\": [\n        {\n            \"glob\": \"https://crawlee.dev/*/*\"\n        }\n    ],\n    \"pseudoUrls\": [],\n    \"excludes\": [\n        {\n            \"glob\": \"/**/*.{png,jpg,jpeg,pdf}\"\n        }\n    ],\n    \"linkSelector\": \"a\",\n    \"pageFunction\": async function pageFunction(context) {\n        const { page, request, log } = context;\n        const title = await page.title();\n        log.info(`URL: ${request.url} TITLE: ${title}`);\n        return {\n            url: request.url,\n            title\n        };\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"initialCookies\": [],\n    \"waitUntil\": [\n        \"networkidle2\"\n    ],\n    \"preNavigationHooks\": `// We need to return array of (possibly async) functions here.\n        // The functions accept two arguments: the \"crawlingContext\" object\n        // and \"gotoOptions\".\n        [\n            async (crawlingContext, gotoOptions) => {\n                const { page } = crawlingContext;\n                // ...\n            },\n        ]`,\n    \"postNavigationHooks\": `// We need to return array of (possibly async) functions here.\n        // The functions accept a single argument: the \"crawlingContext\" object.\n        [\n            async (crawlingContext) => {\n                const { page } = crawlingContext;\n                // ...\n            },\n        ]`,\n    \"customData\": {}\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/puppeteer-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"globs": [{"glob": "https://crawlee.dev/*/*"}], "excludes": [{"glob": "/**/*.{png,jpg,jpeg,pdf}"}], "startUrls": [{"url": "https://crawlee.dev"}], "waitUntil": ["networkidle2"], "customData": {}, "pseudoUrls": [], "linkSelector": "a", "initialCookies": [], "preNavigationHooks": "// We need to return array of (possibly async) functions here.\n        // The functions accept two arguments: the \"crawlingContext\" object\n        // and \"gotoOptions\".\n        [\n            async (crawlingContext, gotoOptions) => {\n                const { page } = crawlingContext;\n                // ...\n            },\n        ]", "proxyConfiguration": {"useApifyProxy": true}, "postNavigationHooks": "// We need to return array of (possibly async) functions here.\n        // The functions accept a single argument: the \"crawlingContext\" object.\n        [\n            async (crawlingContext) => {\n                const { page } = crawlingContext;\n                // ...\n            },\n        ]"}, "actor_id": "apify/puppeteer-scraper"}, "yin/yelp-scraper": {"id": 773, "url": "https://apify.com/yin/yelp-scraper/api/client/nodejs", "title": "Apify API and Free Yelp Web Scraper - get reviews and more interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchTerms\": [\n        \"Pizza\"\n    ],\n    \"locations\": [\n        \"New York\"\n    ],\n    \"searchLimit\": 10,\n    \"maxImages\": 1,\n    \"reviewLimit\": 5,\n    \"reviewsLanguage\": \"ALL\",\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"yin/yelp-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "locations": ["New York"], "maxImages": 1, "reviewLimit": 5, "searchLimit": 10, "searchTerms": ["Pizza"], "reviewsLanguage": "ALL"}, "actor_id": "yin/yelp-scraper"}, "apify/facebook-groups-scraper": {"id": 774, "url": "https://apify.com/apify/facebook-groups-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udc65 Facebook Groups Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.facebook.com/groups/874728723021553\"\n        }\n    ],\n    \"resultsLimit\": 20,\n    \"viewOption\": \"CHRONOLOGICAL\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/facebook-groups-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.facebook.com/groups/874728723021553"}], "viewOption": "CHRONOLOGICAL", "resultsLimit": 20}, "actor_id": "apify/facebook-groups-scraper"}, "apify/facebook-hashtag-scraper": {"id": 775, "url": "https://apify.com/apify/facebook-hashtag-scraper/api/client/nodejs", "title": "Apify API and #\ufe0f\u20e3 Facebook Hashtag Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"keywordList\": [\n        \"webscraping\"\n    ],\n    \"resultsLimit\": 50\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/facebook-hashtag-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"keywordList": ["webscraping"], "resultsLimit": 50}, "actor_id": "apify/facebook-hashtag-scraper"}, "maxcopell/tripadvisor": {"id": 776, "url": "https://apify.com/maxcopell/tripadvisor/api/client/nodejs", "title": "Apify API and \ud83c\udf34 Scrape Tripadvisor hotels, restaurants, and things to do interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"locationFullName\": \"Chicago\",\n    \"maxItems\": 10,\n    \"language\": \"en\",\n    \"currency\": \"USD\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"maxcopell/tripadvisor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"currency": "USD", "language": "en", "maxItems": 10, "locationFullName": "Chicago"}, "actor_id": "maxcopell/tripadvisor"}, "lukaskrivka/google-sheets": {"id": 777, "url": "https://apify.com/lukaskrivka/google-sheets/api/client/nodejs", "title": "Apify API and Fast Google Sheets import and export tool interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"transformFunction\": `// Uncomment this code only if you don't use \"Deduplicate by field\" or \"Deduplicate by equality\"\n        // This code behaves as if there was no transform function\n        /*({ spreadsheetData, datasetData }) => {\n            return spreadsheetData.concat(datasetData);\n        }*/`\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/google-sheets\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"transformFunction": "// Uncomment this code only if you don't use \"Deduplicate by field\" or \"Deduplicate by equality\"\n        // This code behaves as if there was no transform function\n        /*({ spreadsheetData, datasetData }) => {\n            return spreadsheetData.concat(datasetData);\n        }*/"}, "actor_id": "lukaskrivka/google-sheets"}, "lukaskrivka/article-extractor-smart": {"id": 778, "url": "https://apify.com/lukaskrivka/article-extractor-smart/api/client/nodejs", "title": "Apify API and Scrape and download articles and news interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.theguardian.com\"\n        }\n    ],\n    \"isUrlArticleDefinition\": {\n        \"minDashes\": 4,\n        \"hasDate\": true,\n        \"linkIncludes\": [\n            \"article\",\n            \"storyid\",\n            \"?p=\",\n            \"id=\",\n            \"/fpss/track\",\n            \".html\",\n            \"/content/\"\n        ]\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"extendOutputFunction\": ($) => {\n        const result = {};\n        // Uncomment to add a title to the output\n        // result.pageTitle = $('title').text().trim();\n    \n        return result;\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/article-extractor-smart\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.theguardian.com"}], "proxyConfiguration": {"useApifyProxy": true}, "isUrlArticleDefinition": {"hasDate": true, "minDashes": 4, "linkIncludes": ["article", "storyid", "?p=", "id=", "/fpss/track", ".html", "/content/"]}}, "actor_id": "lukaskrivka/article-extractor-smart"}, "apify/instagram-scraper": {"id": 779, "url": "https://apify.com/apify/instagram-scraper/api/client/nodejs", "title": "Apify API and Instagram Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"directUrls\": [\n        \"https://www.instagram.com/humansofny/\"\n    ],\n    \"resultsType\": \"details\",\n    \"resultsLimit\": 200,\n    \"searchType\": \"hashtag\",\n    \"searchLimit\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/instagram-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"directUrls": ["https://www.instagram.com/humansofny/"], "searchType": "hashtag", "resultsType": "details", "searchLimit": 1, "resultsLimit": 200}, "actor_id": "apify/instagram-scraper"}, "apify/facebook-posts-scraper": {"id": 780, "url": "https://apify.com/apify/facebook-posts-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udcdd Facebook Posts Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.facebook.com/humansofnewyork/\"\n        }\n    ],\n    \"resultsLimit\": 20\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/facebook-posts-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.facebook.com/humansofnewyork/"}], "resultsLimit": 20}, "actor_id": "apify/facebook-posts-scraper"}, "shanes/tweet-flash": {"id": 781, "url": "https://apify.com/shanes/tweet-flash/api/client/nodejs", "title": "Apify API and Tweet Flash - Twitter Scraper (Working Again as of 10/18) interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"shanes/tweet-flash\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"max_tweets":500,"language":"any","use_experimental_scraper":false,"user_info":"user info and replying info","max_attempts":5}, "actor_id": "shanes/tweet-flash"}, "junglee/amazon-reviews-scraper": {"id": 782, "url": "https://apify.com/junglee/amazon-reviews-scraper/api/client/nodejs", "title": "Apify API and Scrape Amazon reviews with ratings, description and images interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"productUrls\": [\n        {\n            \"url\": \"https://www.amazon.com/Apple-iPhone-64GB-Midnight-Green/dp/B08BHHSB6M\"\n        }\n    ],\n    \"maxReviews\": 100,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    },\n    \"extendedOutputFunction\": ($) => { return {} }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"junglee/amazon-reviews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxReviews": 100, "productUrls": [{"url": "https://www.amazon.com/Apple-iPhone-64GB-Midnight-Green/dp/B08BHHSB6M"}], "proxyConfiguration": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}}, "actor_id": "junglee/amazon-reviews-scraper"}, "web.harvester/twitter-scraper": {"id": 783, "url": "https://apify.com/web.harvester/twitter-scraper/api/client/nodejs", "title": "Apify API and Twitter Tweets and Profiles Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"handles\": [\n        \"elonmusk\"\n    ],\n    \"userQueries\": [],\n    \"tweetsDesired\": 100,\n    \"profilesDesired\": 10,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"web.harvester/twitter-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"handles": ["elonmusk"], "proxyConfig": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "userQueries": [], "tweetsDesired": 100, "profilesDesired": 10}, "actor_id": "web.harvester/twitter-scraper"}, "clockworks/tiktok-sound-scraper": {"id": 784, "url": "https://apify.com/clockworks/tiktok-sound-scraper/api/client/nodejs", "title": "Apify API and \ud83c\udfb6 Tiktok Sound Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"musics\": [\n        \"https://www.tiktok.com/music/Oh-No-Instrumental-6889520563052645121\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"clockworks/tiktok-sound-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"musics": ["https://www.tiktok.com/music/Oh-No-Instrumental-6889520563052645121"]}, "actor_id": "clockworks/tiktok-sound-scraper"}, "clockworks/free-tiktok-scraper": {"id": 785, "url": "https://apify.com/clockworks/free-tiktok-scraper/api/client/nodejs", "title": "Apify API and Free TikTok Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"hashtags\": [\n        \"followforfollowback\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"clockworks/free-tiktok-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"hashtags": ["followforfollowback"]}, "actor_id": "clockworks/free-tiktok-scraper"}, "lukaskrivka/website-checker": {"id": 786, "url": "https://apify.com/lukaskrivka/website-checker/api/client/nodejs", "title": "Apify API and Website Checker interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urlsToCheck\": [\n        {\n            \"url\": \"https://www.amazon.com/b?ie=UTF8&node=11392907011\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"SHADER\",\n            \"BUYPROXIES94952\",\n            \"RESIDENTIAL\"\n        ]\n    },\n    \"repeatChecksOnProvidedUrls\": 10,\n    \"maxNumberOfPagesCheckedPerDomain\": 1000\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/website-checker\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urlsToCheck": [{"url": "https://www.amazon.com/b?ie=UTF8&node=11392907011"}], "proxyConfiguration": {"useApifyProxy": true, "apifyProxyGroups": ["SHADER", "BUYPROXIES94952", "RESIDENTIAL"]}, "repeatChecksOnProvidedUrls": 10, "maxNumberOfPagesCheckedPerDomain": 1000}, "actor_id": "lukaskrivka/website-checker"}, "voyager/booking-scraper": {"id": 787, "url": "https://apify.com/voyager/booking-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udece Booking hotel Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"New York\",\n    \"maxItems\": 10,\n    \"sortBy\": \"distance_from_search\",\n    \"starsCountFilter\": \"any\",\n    \"currency\": \"USD\",\n    \"language\": \"en-gb\",\n    \"minMaxPrice\": \"0-999999\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"voyager/booking-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"search": "New York", "sortBy": "distance_from_search", "currency": "USD", "language": "en-gb", "maxItems": 10, "minMaxPrice": "0-999999", "starsCountFilter": "any"}, "actor_id": "voyager/booking-scraper"}, "streamers/youtube-scraper": {"id": 788, "url": "https://apify.com/streamers/youtube-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udcf9 YouTube Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchKeywords\": \"Crawlee\",\n    \"maxResults\": 10,\n    \"maxResultsShorts\": 0,\n    \"maxResultStreams\": 0\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"streamers/youtube-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxResults": 10, "searchKeywords": "Crawlee", "maxResultStreams": 0, "maxResultsShorts": 0}, "actor_id": "streamers/youtube-scraper"}, "apify/facebook-pages-scraper": {"id": 789, "url": "https://apify.com/apify/facebook-pages-scraper/api/client/nodejs", "title": "Apify API and Facebook Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.facebook.com/copperkettleyqr/\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/facebook-pages-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.facebook.com/copperkettleyqr/"}]}, "actor_id": "apify/facebook-pages-scraper"}, "apify/instagram-hashtag-scraper": {"id": 790, "url": "https://apify.com/apify/instagram-hashtag-scraper/api/client/nodejs", "title": "Apify API and Instagram Hashtag Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"hashtags\": [\n        \"webscraping\"\n    ],\n    \"resultsLimit\": 20\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/instagram-hashtag-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"hashtags": ["webscraping"], "resultsLimit": 20}, "actor_id": "apify/instagram-hashtag-scraper"}, "apify/facebook-ads-scraper": {"id": 791, "url": "https://apify.com/apify/facebook-ads-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udce2 Facebook Ads Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.facebook.com/drive4quantix/?ref=page_internal\"\n        }\n    ],\n    \"resultsLimit\": 99999\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/facebook-ads-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.facebook.com/drive4quantix/?ref=page_internal"}], "resultsLimit": 99999}, "actor_id": "apify/facebook-ads-scraper"}, "apify/facebook-comments-scraper": {"id": 792, "url": "https://apify.com/apify/facebook-comments-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udcac Facebook Comments Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.facebook.com/humansofnewyork/posts/pfbid0BbKbkisExKGSKuhee9a7i86RwRuMKFC8NSkKStB7CsM3uXJuAAfZLrkcJMXxhH4Yl\"\n        }\n    ],\n    \"resultsLimit\": 50\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/facebook-comments-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.facebook.com/humansofnewyork/posts/pfbid0BbKbkisExKGSKuhee9a7i86RwRuMKFC8NSkKStB7CsM3uXJuAAfZLrkcJMXxhH4Yl"}], "resultsLimit": 50}, "actor_id": "apify/facebook-comments-scraper"}, "apify/facebook-page-contact-information": {"id": 793, "url": "https://apify.com/apify/facebook-page-contact-information/api/client/nodejs", "title": "Apify API and \ud83d\udcf1Scrape contact info from Facebook Pages interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"pages\": [\n        \"https://www.facebook.com/humansofnewyork/\",\n        \"humansofnewyork\"\n    ],\n    \"language\": \"en-US\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/facebook-page-contact-information\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"pages": ["https://www.facebook.com/humansofnewyork/", "humansofnewyork"], "language": "en-US"}, "actor_id": "apify/facebook-page-contact-information"}, "clockworks/tiktok-comments-scraper": {"id": 794, "url": "https://apify.com/clockworks/tiktok-comments-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udcac TikTok Comments Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"postURLs\": [\n        \"https://www.tiktok.com/@bellapoarch/video/6862153058223197445\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"clockworks/tiktok-comments-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"postURLs": ["https://www.tiktok.com/@bellapoarch/video/6862153058223197445"]}, "actor_id": "clockworks/tiktok-comments-scraper"}, "lukaskrivka/auto-gpt": {"id": 1026, "url": "https://apify.com/lukaskrivka/auto-gpt/api/client/nodejs", "title": "Apify API and Auto GPT interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/auto-gpt\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukaskrivka/auto-gpt"}, "apify/facebook-photos-scraper": {"id": 795, "url": "https://apify.com/apify/facebook-photos-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udcf8 Facebook Photos Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.facebook.com/humansofnewyork\"\n        }\n    ],\n    \"resultsLimit\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/facebook-photos-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.facebook.com/humansofnewyork"}], "resultsLimit": 10}, "actor_id": "apify/facebook-photos-scraper"}, "apify/facebook-reviews-scraper": {"id": 796, "url": "https://apify.com/apify/facebook-reviews-scraper/api/client/nodejs", "title": "Apify API and \u2b50\ufe0f Facebook Reviews Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.facebook.com/copperkettleyqr/reviews\"\n        }\n    ],\n    \"resultsLimit\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/facebook-reviews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.facebook.com/copperkettleyqr/reviews"}], "resultsLimit": 10}, "actor_id": "apify/facebook-reviews-scraper"}, "apify/facebook-events-scraper": {"id": 797, "url": "https://apify.com/apify/facebook-events-scraper/api/client/nodejs", "title": "Apify API and \ud83d\uddd3 Facebook Events Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchQueries\": [\n        \"Sport New York\"\n    ],\n    \"startUrls\": [],\n    \"maxEvents\": 30\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/facebook-events-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxEvents": 30, "startUrls": [], "searchQueries": ["Sport New York"]}, "actor_id": "apify/facebook-events-scraper"}, "streamers/youtube-comments-scraper": {"id": 798, "url": "https://apify.com/streamers/youtube-comments-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udcac Youtube Comments Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.youtube.com/watch?v=xObhZ0Ga7EQ\"\n        }\n    ],\n    \"maxComments\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"streamers/youtube-comments-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.youtube.com/watch?v=xObhZ0Ga7EQ"}], "maxComments": 10}, "actor_id": "streamers/youtube-comments-scraper"}, "streamers/youtube-shorts-scraper": {"id": 799, "url": "https://apify.com/streamers/youtube-shorts-scraper/api/client/nodejs", "title": "Apify API and \u25b6 Youtube Shorts Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"channels\": [\n        \"rainbowicecream9780\"\n    ],\n    \"maxResultsShorts\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"streamers/youtube-shorts-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"channels": ["rainbowicecream9780"], "maxResultsShorts": 10}, "actor_id": "streamers/youtube-shorts-scraper"}, "voyager/booking-reviews-scraper": {"id": 800, "url": "https://apify.com/voyager/booking-reviews-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udcab Booking Reviews Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.booking.com/hotel/cz/old-town-square-apartments.html?aid=304142&&dest_id=-553173;dest_type=city;dist=0;group_adults=2\"\n        }\n    ],\n    \"maxReviewsPerHotel\": 9999999,\n    \"sortReviewsBy\": \"f_relevance\",\n    \"reviewScores\": [\n        \"ALL\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"voyager/booking-reviews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.booking.com/hotel/cz/old-town-square-apartments.html?aid=304142&&dest_id=-553173;dest_type=city;dist=0;group_adults=2"}], "reviewScores": ["ALL"], "sortReviewsBy": "f_relevance", "maxReviewsPerHotel": 9999999}, "actor_id": "voyager/booking-reviews-scraper"}, "apify/instagram-post-scraper": {"id": 814, "url": "https://apify.com/apify/instagram-post-scraper/api/client/nodejs", "title": "Apify API and Free Instagram Post Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"username\": [\n        \"zelenskiy_official\"\n    ],\n    \"resultsLimit\": 30\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/instagram-post-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"username": ["zelenskiy_official"], "resultsLimit": 30}, "actor_id": "apify/instagram-post-scraper"}, "apify/beautifulsoup-scraper": {"id": 801, "url": "https://apify.com/apify/beautifulsoup-scraper/api/client/nodejs", "title": "Apify API and BeautifulSoup Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://crawlee.dev\"\n        }\n    ],\n    \"maxCrawlingDepth\": 1,\n    \"requestTimeout\": 10,\n    \"linkSelector\": \"a[href]\",\n    \"linkPatterns\": [\n        \".*crawlee\\\\.dev.*\"\n    ],\n    \"pageFunction\": \"from typing import Any\\n\\n# See the context section in readme to find out what fields you can access \\n# https://apify.com/vdusek/beautifulsoup-scraper#context    \\ndef page_function(context: Context) -> Any:\\n    url = context.request['url']\\n    title = context.soup.title.string if context.soup.title else None\\n    return {'url': url, 'title': title}\\n\",\n    \"soupFeatures\": \"html.parser\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/beautifulsoup-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://crawlee.dev"}], "linkPatterns": [".*crawlee\\.dev.*"], "linkSelector": "a[href]", "pageFunction": "from typing import Any\n\n# See the context section in readme to find out what fields you can access \n# https://apify.com/vdusek/beautifulsoup-scraper#context    \ndef page_function(context: Context) -> Any:\n    url = context.request['url']\n    title = context.soup.title.string if context.soup.title else None\n    return {'url': url, 'title': title}\n", "soupFeatures": "html.parser", "requestTimeout": 10, "maxCrawlingDepth": 1, "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "apify/beautifulsoup-scraper"}, "maxcopell/zillow-detail-scraper": {"id": 802, "url": "https://apify.com/maxcopell/zillow-detail-scraper/api/client/nodejs", "title": "Apify API and \ud83c\udfe1 Zillow Detail Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.zillow.com/homedetails/17-Zelma-Dr-Greenville-SC-29617/11026031_zpid/\"\n        },\n        {\n            \"url\": \"https://www.zillow.com/apartments/san-antonio-tx/westpond/5XkVjF/\"\n        },\n        {\n            \"url\": \"https://www.zillow.com/community/greens-at-indian-river-preserve/39987_plid/\"\n        },\n        {\n            \"url\": \"https://www.zillow.com/b/Cashiers-NC/35.09787,-83.080475_ll/\"\n        },\n        {\n            \"url\": \"https://www.zillow.com/community/greens-at-indian-river-preserve/2072679238_zpid/\"\n        },\n        {\n            \"url\": \"https://www.zillow.com/b/2202-sandlewood-cv-san-antonio-tx-C2tftd/\"\n        }\n    ],\n    \"searchResultsDatasetId\": \"MCrom9yPhGqh8X1Yr\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"maxcopell/zillow-detail-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.zillow.com/homedetails/17-Zelma-Dr-Greenville-SC-29617/11026031_zpid/"}, {"url": "https://www.zillow.com/apartments/san-antonio-tx/westpond/5XkVjF/"}, {"url": "https://www.zillow.com/community/greens-at-indian-river-preserve/39987_plid/"}, {"url": "https://www.zillow.com/b/Cashiers-NC/35.09787,-83.080475_ll/"}, {"url": "https://www.zillow.com/community/greens-at-indian-river-preserve/2072679238_zpid/"}, {"url": "https://www.zillow.com/b/2202-sandlewood-cv-san-antonio-tx-C2tftd/"}], "searchResultsDatasetId": "MCrom9yPhGqh8X1Yr"}, "actor_id": "maxcopell/zillow-detail-scraper"}, "maxcopell/zillow-zip-search": {"id": 803, "url": "https://apify.com/maxcopell/zillow-zip-search/api/client/nodejs", "title": "Apify API and Zillow ZIP Code Search Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"zipCodes\": [\n        \"10014\",\n        \"07306\"\n    ],\n    \"priceMax\": 400000\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"maxcopell/zillow-zip-search\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"priceMax": 400000, "zipCodes": ["10014", "07306"]}, "actor_id": "maxcopell/zillow-zip-search"}, "apify/threads-profile-api-scraper": {"id": 804, "url": "https://apify.com/apify/threads-profile-api-scraper/api/client/nodejs", "title": "Apify API and Meta Threads Profile Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"usernames\": [\n        \"guinnessworldrecords\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/threads-profile-api-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"usernames": ["guinnessworldrecords"]}, "actor_id": "apify/threads-profile-api-scraper"}, "apify/instagram-api-scraper": {"id": 805, "url": "https://apify.com/apify/instagram-api-scraper/api/client/nodejs", "title": "Apify API and Instagram API Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"directUrls\": [\n        \"https://www.instagram.com/humansofny/\"\n    ],\n    \"resultsType\": \"details\",\n    \"resultsLimit\": 200,\n    \"searchType\": \"hashtag\",\n    \"searchLimit\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/instagram-api-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"directUrls": ["https://www.instagram.com/humansofny/"], "searchType": "hashtag", "resultsType": "details", "searchLimit": 1, "resultsLimit": 200}, "actor_id": "apify/instagram-api-scraper"}, "streamers/youtube-channel-scraper": {"id": 806, "url": "https://apify.com/streamers/youtube-channel-scraper/api/client/nodejs", "title": "Apify API and \ud83c\udfce\ud83d\udca8 Fast Youtube Channel Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.youtube.com/@Apify\"\n        }\n    ],\n    \"maxResults\": 10,\n    \"maxResultsShorts\": 0,\n    \"maxResultStreams\": 0\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"streamers/youtube-channel-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.youtube.com/@Apify"}], "maxResults": 10, "maxResultStreams": 0, "maxResultsShorts": 0}, "actor_id": "streamers/youtube-channel-scraper"}, "apify/playwright-scraper": {"id": 807, "url": "https://apify.com/apify/playwright-scraper/api/client/nodejs", "title": "Apify API and Playwright Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://crawlee.dev\"\n        }\n    ],\n    \"globs\": [\n        {\n            \"glob\": \"https://crawlee.dev/*/*\"\n        }\n    ],\n    \"pseudoUrls\": [],\n    \"excludes\": [\n        {\n            \"glob\": \"/**/*.{png,jpg,jpeg,pdf}\"\n        }\n    ],\n    \"linkSelector\": \"a\",\n    \"pageFunction\": async function pageFunction(context) {\n        const { page, request, log } = context;\n        const title = await page.title();\n        log.info(`URL: ${request.url} TITLE: ${title}`);\n        return {\n            url: request.url,\n            title\n        };\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"initialCookies\": [],\n    \"launcher\": \"chromium\",\n    \"waitUntil\": \"networkidle\",\n    \"preNavigationHooks\": `// We need to return array of (possibly async) functions here.\n        // The functions accept two arguments: the \"crawlingContext\" object\n        // and \"gotoOptions\".\n        [\n            async (crawlingContext, gotoOptions) => {\n                const { page } = crawlingContext;\n                // ...\n            },\n        ]`,\n    \"postNavigationHooks\": `// We need to return array of (possibly async) functions here.\n        // The functions accept a single argument: the \"crawlingContext\" object.\n        [\n            async (crawlingContext) => {\n                const { page } = crawlingContext;\n                // ...\n            },\n        ]`,\n    \"customData\": {}\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/playwright-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"globs": [{"glob": "https://crawlee.dev/*/*"}], "excludes": [{"glob": "/**/*.{png,jpg,jpeg,pdf}"}], "launcher": "chromium", "startUrls": [{"url": "https://crawlee.dev"}], "waitUntil": "networkidle", "customData": {}, "pseudoUrls": [], "linkSelector": "a", "initialCookies": [], "preNavigationHooks": "// We need to return array of (possibly async) functions here.\n        // The functions accept two arguments: the \"crawlingContext\" object\n        // and \"gotoOptions\".\n        [\n            async (crawlingContext, gotoOptions) => {\n                const { page } = crawlingContext;\n                // ...\n            },\n        ]", "proxyConfiguration": {"useApifyProxy": true}, "postNavigationHooks": "// We need to return array of (possibly async) functions here.\n        // The functions accept a single argument: the \"crawlingContext\" object.\n        [\n            async (crawlingContext) => {\n                const { page } = crawlingContext;\n                // ...\n            },\n        ]"}, "actor_id": "apify/playwright-scraper"}, "clockworks/tiktok-profile-scraper": {"id": 808, "url": "https://apify.com/clockworks/tiktok-profile-scraper/api/client/nodejs", "title": "Apify API and \u266a Tiktok Profile Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"profiles\": [\n        \"apifytech\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"clockworks/tiktok-profile-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"profiles": ["apifytech"]}, "actor_id": "clockworks/tiktok-profile-scraper"}, "quacker/twitter-url-scraper": {"id": 809, "url": "https://apify.com/quacker/twitter-url-scraper/api/client/nodejs", "title": "Apify API and Twitter URL Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://twitter.com/apify\"\n        }\n    ],\n    \"tweetsDesired\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"quacker/twitter-url-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://twitter.com/apify"}], "tweetsDesired": 100}, "actor_id": "quacker/twitter-url-scraper"}, "clockworks/tiktok-hashtag-scraper": {"id": 810, "url": "https://apify.com/clockworks/tiktok-hashtag-scraper/api/client/nodejs", "title": "Apify API and \u266a Tiktok Hashtag Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"hashtags\": [\n        \"followforfollowback\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"clockworks/tiktok-hashtag-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"hashtags": ["followforfollowback"]}, "actor_id": "clockworks/tiktok-hashtag-scraper"}, "clockworks/tiktok-video-scraper": {"id": 811, "url": "https://apify.com/clockworks/tiktok-video-scraper/api/client/nodejs", "title": "Apify API and \u266a Tiktok Video Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"postURLs\": [\n        \"https://www.tiktok.com/@apifytech/video/7200360993149553925\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"clockworks/tiktok-video-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"postURLs": ["https://www.tiktok.com/@apifytech/video/7200360993149553925"]}, "actor_id": "clockworks/tiktok-video-scraper"}, "apify/facebook-likes-scraper": {"id": 812, "url": "https://apify.com/apify/facebook-likes-scraper/api/client/nodejs", "title": "Apify API and \u2764\ufe0e Facebook Likes Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.facebook.com/humansofnewyork/posts/pfbid0BbKbkisExKGSKuhee9a7i86RwRuMKFC8NSkKStB7CsM3uXJuAAfZLrkcJMXxhH4Yl\"\n        }\n    ],\n    \"resultsLimit\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/facebook-likes-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.facebook.com/humansofnewyork/posts/pfbid0BbKbkisExKGSKuhee9a7i86RwRuMKFC8NSkKStB7CsM3uXJuAAfZLrkcJMXxhH4Yl"}], "resultsLimit": 10}, "actor_id": "apify/facebook-likes-scraper"}, "apify/instagram-comment-scraper": {"id": 813, "url": "https://apify.com/apify/instagram-comment-scraper/api/client/nodejs", "title": "Apify API and Instagram Comment Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"directUrls\": [\n        \"https://www.instagram.com/p/CH-MgQOn-7E/\",\n        \"https://www.instagram.com/p/Bi-hISIghYe/\"\n    ],\n    \"resultsLimit\": 24\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/instagram-comment-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"directUrls": ["https://www.instagram.com/p/CH-MgQOn-7E/", "https://www.instagram.com/p/Bi-hISIghYe/"], "resultsLimit": 24}, "actor_id": "apify/instagram-comment-scraper"}, "mtrunkat/url-list-download-html": {"id": 815, "url": "https://apify.com/mtrunkat/url-list-download-html/api/client/nodejs", "title": "Apify API and Download HTML from URLs interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"requestListSources\": [\n        {\n            \"url\": \"https://apify.com\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"maxRequestRetries\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mtrunkat/url-list-download-html\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxRequestRetries": 1, "proxyConfiguration": {"useApifyProxy": true}, "requestListSources": [{"url": "https://apify.com"}]}, "actor_id": "mtrunkat/url-list-download-html"}, "epctex/traffic-generator": {"id": 816, "url": "https://apify.com/epctex/traffic-generator/api/client/nodejs", "title": "Apify API and Ultimate Traffic Generator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.example.com\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/traffic-generator\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": ["https://www.example.com"]}, "actor_id": "epctex/traffic-generator"}, "dtrungtin/airbnb-scraper": {"id": 817, "url": "https://apify.com/dtrungtin/airbnb-scraper/api/client/nodejs", "title": "Apify API and Free Airbnb data scraper \ud83c\udfe0 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"locationQuery\": \"Sacramento, California\",\n    \"maxListings\": 10,\n    \"startUrls\": [],\n    \"maxReviews\": 10,\n    \"calendarMonths\": 0,\n    \"currency\": \"USD\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"maxConcurrency\": 50,\n    \"limitPoints\": 100,\n    \"timeoutMs\": 300000\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/airbnb-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"currency": "USD", "startUrls": [], "timeoutMs": 300000, "maxReviews": 10, "limitPoints": 100, "maxListings": 10, "locationQuery": "Sacramento, California", "calendarMonths": 0, "maxConcurrency": 50, "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "dtrungtin/airbnb-scraper"}, "anchor/email-phone-extractor": {"id": 818, "url": "https://apify.com/anchor/email-phone-extractor/api/client/nodejs", "title": "Apify API and Email \u2709\ufe0f & Phone \u260e\ufe0f Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://apify.com/contact\"\n        }\n    ],\n    \"maxDepth\": 2,\n    \"maxRequests\": 5,\n    \"pseudoUrls\": [\n        \".*\"\n    ],\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"anchor/email-phone-extractor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDepth": 2, "startUrls": [{"url": "https://apify.com/contact"}], "pseudoUrls": [".*"], "maxRequests": 5, "proxyConfig": {"useApifyProxy": true}}, "actor_id": "anchor/email-phone-extractor"}, "apify/hello-world": {"id": 819, "url": "https://apify.com/apify/hello-world/api/client/nodejs", "title": "Apify API and Hello World Example - Simple actor example interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/hello-world\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/hello-world"}, "petr_cermak/transfermarkt": {"id": 820, "url": "https://apify.com/petr_cermak/transfermarkt/api/client/nodejs", "title": "Apify API and \u26bd Scrape Transfermarkt interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.transfermarkt.com/lionel-messi/profil/spieler/28003\"\n        }\n    ],\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    },\n    \"crawlDepth\": 1,\n    \"pageDepth\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/transfermarkt\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"pageDepth": 1, "startUrls": [{"url": "https://www.transfermarkt.com/lionel-messi/profil/spieler/28003"}], "crawlDepth": 1, "proxyConfig": {"useApifyProxy": true}}, "actor_id": "petr_cermak/transfermarkt"}, "petr_cermak/yellow-pages-scraper": {"id": 821, "url": "https://apify.com/petr_cermak/yellow-pages-scraper/api/client/nodejs", "title": "Apify API and Yellow Pages Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"Dentist\",\n    \"location\": \"Los Angeles\",\n    \"maxItems\": 200,\n    \"extendOutputFunction\": ($, record) => {\n        return {};\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/yellow-pages-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"search": "Dentist", "location": "Los Angeles", "maxItems": 200, "proxyConfiguration": {"useApifyProxy": false}}, "actor_id": "petr_cermak/yellow-pages-scraper"}, "mstephen190/proxy-scraper": {"id": 822, "url": "https://apify.com/mstephen190/proxy-scraper/api/client/nodejs", "title": "Apify API and Fast proxy tester and scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mstephen190/proxy-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "mstephen190/proxy-scraper"}, "compass/easy-google-maps": {"id": 823, "url": "https://apify.com/compass/easy-google-maps/api/client/nodejs", "title": "Apify API and Fast Google Maps Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"restaurant\",\n    \"location\": \"New York\",\n    \"maxCrawledPlacesPerSearch\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"compass/easy-google-maps\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"search": "restaurant", "location": "New York", "maxCrawledPlacesPerSearch": 10}, "actor_id": "compass/easy-google-maps"}, "junglee/free-amazon-product-scraper": {"id": 824, "url": "https://apify.com/junglee/free-amazon-product-scraper/api/client/nodejs", "title": "Apify API and Free Amazon Product Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"categoryUrls\": [\n        {\n            \"url\": \"https://www.amazon.com/s?i=specialty-aps&bbn=16225009011&rh=n%3A%2116225009011%2Cn%3A2811119011&ref=nav_em__nav_desktop_sa_intl_cell_phones_and_accessories_0_2_5_5\"\n        }\n    ],\n    \"maxItems\": 50,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"junglee/free-amazon-product-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 50, "categoryUrls": [{"url": "https://www.amazon.com/s?i=specialty-aps&bbn=16225009011&rh=n%3A%2116225009011%2Cn%3A2811119011&ref=nav_em__nav_desktop_sa_intl_cell_phones_and_accessories_0_2_5_5"}], "proxyConfiguration": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}}, "actor_id": "junglee/free-amazon-product-scraper"}, "apify/screenshot-url": {"id": 825, "url": "https://apify.com/apify/screenshot-url/api/client/nodejs", "title": "Apify API and Website Screenshot Generator - Screenshot URL interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.apify.com/\",\n    \"waitUntil\": \"networkidle0\",\n    \"delay\": 0,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/screenshot-url\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.apify.com/", "delay": 0, "proxy": {"useApifyProxy": true}, "waitUntil": "networkidle0"}, "actor_id": "apify/screenshot-url"}, "trudax/reddit-scraper-lite": {"id": 826, "url": "https://apify.com/trudax/reddit-scraper-lite/api/client/nodejs", "title": "Apify API and Free Reddit Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.reddit.com/r/pasta/comments/vwi6jx/pasta_peperoni_and_ricotta_cheese_how_to_make/\"\n        }\n    ],\n    \"maxItems\": 10,\n    \"maxPostCount\": 10,\n    \"maxComments\": 10,\n    \"maxCommunitiesCount\": 2,\n    \"maxUserCount\": 2,\n    \"scrollTimeout\": 40,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"trudax/reddit-scraper-lite\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "maxItems": 10, "startUrls": [{"url": "https://www.reddit.com/r/pasta/comments/vwi6jx/pasta_peperoni_and_ricotta_cheese_how_to_make/"}], "maxComments": 10, "maxPostCount": 10, "maxUserCount": 2, "scrollTimeout": 40, "maxCommunitiesCount": 2}, "actor_id": "trudax/reddit-scraper-lite"}, "hooli/easy-google-scraper": {"id": 827, "url": "https://apify.com/hooli/easy-google-scraper/api/client/nodejs", "title": "Apify API and \ud83c\udfce\ud83d\udca8 Fast Google Scraping interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": [\n        \"Hotels in NYC\"\n    ],\n    \"countryCode\": \"\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hooli/easy-google-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"search": ["Hotels in NYC"], "countryCode": ""}, "actor_id": "hooli/easy-google-scraper"}, "misceres/seo-audit-tool": {"id": 828, "url": "https://apify.com/misceres/seo-audit-tool/api/client/nodejs", "title": "Apify API and SEO Audit Tool - Free website SEO check interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrl\": \"https://apify.com\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    },\n    \"maxRequestsPerCrawl\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"misceres/seo-audit-tool\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrl": "https://apify.com", "maxRequestsPerCrawl": 10}, "actor_id": "misceres/seo-audit-tool"}, "maxcopell/zillow-api-scraper": {"id": 829, "url": "https://apify.com/maxcopell/zillow-api-scraper/api/client/nodejs", "title": "Apify API and Scrape Zillow real estate listings \ud83c\udfd8\ufe0f interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"Los Angeles\",\n    \"type\": \"all\",\n    \"maxItems\": 100,\n    \"maxLevel\": 1,\n    \"maxRetries\": 10,\n    \"maxConcurrency\": 10,\n    \"extendOutputFunction\": async ({ data, item, customData, Apify }) => {\n        return item;\n    },\n    \"extendScraperFunction\": async ({ label, page, request, customData, Apify }) => {\n        if (label === 'SETUP') {\n            // before crawler.run()\n        } else if (label === 'GOTO') {\n            // inside handleGotoFunction\n        } else if (label === 'HANDLE') {\n            // inside handlePageFunction\n        } else if (label === 'FINISH') {\n            // after crawler.run()\n        }\n    },\n    \"customData\": {}\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"maxcopell/zillow-api-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"type": "all", "search": "Los Angeles", "maxItems": 100, "maxLevel": 1, "customData": {}, "maxRetries": 10, "maxConcurrency": 10}, "actor_id": "maxcopell/zillow-api-scraper"}, "m0uka/bulk-image-downloader": {"id": 830, "url": "https://apify.com/m0uka/bulk-image-downloader/api/client/nodejs", "title": "Apify API and Bulk Image Downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://apify.com\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"m0uka/bulk-image-downloader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://apify.com"}]}, "actor_id": "m0uka/bulk-image-downloader"}, "trudax/reddit-scraper": {"id": 831, "url": "https://apify.com/trudax/reddit-scraper/api/client/nodejs", "title": "Apify API and \ud83e\udd16 Reddit Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.reddit.com/r/pasta/comments/vwi6jx/pasta_peperoni_and_ricotta_cheese_how_to_make/\"\n        }\n    ],\n    \"maxItems\": 10,\n    \"maxPostCount\": 10,\n    \"maxComments\": 10,\n    \"maxCommunitiesCount\": 2,\n    \"maxUserCount\": 2,\n    \"scrollTimeout\": 40,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"trudax/reddit-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "maxItems": 10, "startUrls": [{"url": "https://www.reddit.com/r/pasta/comments/vwi6jx/pasta_peperoni_and_ricotta_cheese_how_to_make/"}], "maxComments": 10, "maxPostCount": 10, "maxUserCount": 2, "scrollTimeout": 40, "maxCommunitiesCount": 2}, "actor_id": "trudax/reddit-scraper"}, "apify/legacy-phantomjs-crawler": {"id": 832, "url": "https://apify.com/apify/legacy-phantomjs-crawler/api/client/nodejs", "title": "Apify API and Legacy PhantomJS Crawler - Crawl websites, extract data interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"key\": \"START\",\n            \"value\": \"https://www.example.com/\"\n        }\n    ],\n    \"crawlPurls\": [\n        {\n            \"key\": \"MY_LABEL\",\n            \"value\": \"https://www.example.com/[.*]\"\n        }\n    ],\n    \"clickableElementsSelector\": \"a:not([rel=nofollow])\",\n    \"pageFunction\": function pageFunction(context) {\n        // called on every page the crawler visits, use it to extract data from it\n        var $ = context.jQuery;\n        var result = {\n            title: $('title').text(),\n            myValue: $('TODO').text()\n        };\n        return result;\n    },\n    \"interceptRequest\": function interceptRequest(context, newRequest) {\n        // called whenever the crawler finds a link to a new page,\n        // use it to override default behavior\n        return newRequest;\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/legacy-phantomjs-crawler\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"key": "START", "value": "https://www.example.com/"}], "crawlPurls": [{"key": "MY_LABEL", "value": "https://www.example.com/[.*]"}], "clickableElementsSelector": "a:not([rel=nofollow])"}, "actor_id": "apify/legacy-phantomjs-crawler"}, "alexey/glassdoor-jobs-scraper": {"id": 833, "url": "https://apify.com/alexey/glassdoor-jobs-scraper/api/client/nodejs", "title": "Apify API and Glassdoor Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"CEO\",\n    \"location\": \"\",\n    \"locationstate\": \"\",\n    \"maxResults\": 30,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"alexey/glassdoor-jobs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "query": "CEO", "location": "", "maxResults": 30, "locationstate": ""}, "actor_id": "alexey/glassdoor-jobs-scraper"}, "apify/instagram-reel-scraper": {"id": 839, "url": "https://apify.com/apify/instagram-reel-scraper/api/client/nodejs", "title": "Apify API and Free Instagram Post Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"username\": [\n        \"zelenskiy_official\"\n    ],\n    \"resultsLimit\": 30\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/instagram-reel-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"username": ["zelenskiy_official"], "resultsLimit": 30}, "actor_id": "apify/instagram-reel-scraper"}, "epctex/dnb-scraper": {"id": 834, "url": "https://apify.com/epctex/dnb-scraper/api/client/nodejs", "title": "Apify API and Dun & Bradstreet Data Insights interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.dnb.com/business-directory/industry-analysis.commercial_and_industrial_machinery_and_equipment_rental_and_leasing.html\",\n        \"https://www.dnb.com/perspectives/corporate-compliance/achieve-greater-fatca-and-crs-compliance.html\",\n        \"https://www.dnb.com/business-directory/company-profiles.alibaba_(china)_technology_co_ltd.e3529c2476c569b9912000545d3314d6.html\"\n    ],\n    \"maxItems\": 50,\n    \"extendOutputFunction\": ($) => {\n        const result = {};\n        // Uncomment to add a title to the output\n        // result.title = $('title').text().trim();\n    \n        return result;\n    },\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/dnb-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "maxItems": 50, "startUrls": ["https://www.dnb.com/business-directory/industry-analysis.commercial_and_industrial_machinery_and_equipment_rental_and_leasing.html", "https://www.dnb.com/perspectives/corporate-compliance/achieve-greater-fatca-and-crs-compliance.html", "https://www.dnb.com/business-directory/company-profiles.alibaba_(china)_technology_co_ltd.e3529c2476c569b9912000545d3314d6.html"]}, "actor_id": "epctex/dnb-scraper"}, "timwhite/youtube-channel-data-scraper": {"id": 835, "url": "https://apify.com/timwhite/youtube-channel-data-scraper/api/client/nodejs", "title": "Apify API and How To Scrape Emails From YouTube Channels interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"keywords\": [\n        \"reactjs\"\n    ],\n    \"limit\": 5,\n    \"maxRequestsPerCrawl\": 100,\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.youtube.com/c/SydneySerena/\"\n        }\n    ],\n    \"handlePageTimeoutSecs\": 30,\n    \"maxRequestRetries\": 3,\n    \"minConcurrency\": 1,\n    \"maxConcurrency\": 100,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"timwhite/youtube-channel-data-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"limit": 5, "keywords": ["reactjs"], "startUrls": [{"url": "https://www.youtube.com/c/SydneySerena/"}], "maxConcurrency": 100, "minConcurrency": 1, "maxRequestRetries": 3, "proxyConfiguration": {"useApifyProxy": true}, "maxRequestsPerCrawl": 100, "handlePageTimeoutSecs": 30}, "actor_id": "timwhite/youtube-channel-data-scraper"}, "epctex/google-shopping-scraper": {"id": 836, "url": "https://apify.com/epctex/google-shopping-scraper/api/client/nodejs", "title": "Apify API and Google Shopping Data Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.google.com/search?q=android&source=lnms&tbm=shop&tbs=vw:l\"\n    ],\n    \"queries\": [\n        \"iPhone\"\n    ],\n    \"maxItems\": 60,\n    \"maxItemsPerQuery\": 100,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"GOOGLE_SERP\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/google-shopping-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["GOOGLE_SERP"]}, "queries": ["iPhone"], "maxItems": 60, "startUrls": ["https://www.google.com/search?q=android&source=lnms&tbm=shop&tbs=vw:l"], "maxItemsPerQuery": 100}, "actor_id": "epctex/google-shopping-scraper"}, "petr_cermak/anti-captcha-recaptcha": {"id": 837, "url": "https://apify.com/petr_cermak/anti-captcha-recaptcha/api/client/nodejs", "title": "Apify API and \ud83e\uddf0 Free anticaptcha reCAPTCHA tool interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"proxyType\": \"http\",\n    \"proxyAddress\": \"8.8.8.8\",\n    \"proxyPort\": 8080,\n    \"proxyLogin\": \"theLogin\",\n    \"proxyPassword\": \"thePassword\",\n    \"userAgent\": \"Opera 6.0\",\n    \"cookies\": \"name=value; name2=value2\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/anti-captcha-recaptcha\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"cookies": "name=value; name2=value2", "proxyPort": 8080, "proxyType": "http", "userAgent": "Opera 6.0", "proxyLogin": "theLogin", "proxyAddress": "8.8.8.8", "proxyPassword": "thePassword"}, "actor_id": "petr_cermak/anti-captcha-recaptcha"}, "lukaskrivka/google-maps-with-contact-details": {"id": 838, "url": "https://apify.com/lukaskrivka/google-maps-with-contact-details/api/client/nodejs", "title": "Apify API and \ud83d\udce9\ud83d\udccd Google Maps Email Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerStartUrl\": 20,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/google-maps-with-contact-details\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxyConfig": {"useApifyProxy": true}, "maxRequestsPerStartUrl": 20}, "actor_id": "lukaskrivka/google-maps-with-contact-details"}, "anchor/linkedin-company-url-finder": {"id": 840, "url": "https://apify.com/anchor/linkedin-company-url-finder/api/client/nodejs", "title": "Apify API and LinkedIn Company URL Finder interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"queries\": \"Tesla\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"anchor/linkedin-company-url-finder\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"queries": "Tesla"}, "actor_id": "anchor/linkedin-company-url-finder"}, "mtnfranke/telegram-scraper": {"id": 841, "url": "https://apify.com/mtnfranke/telegram-scraper/api/client/nodejs", "title": "Apify API and Telegram Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"phone\": \"+491234567890\",\n    \"chatIDs\": [],\n    \"resultsPerChat\": 100,\n    \"dateFilter\": \"\",\n    \"proxy\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mtnfranke/telegram-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"phone": "+491234567890", "proxy": {"useApifyProxy": false}, "chatIDs": [], "dateFilter": "", "resultsPerChat": 100}, "actor_id": "mtnfranke/telegram-scraper"}, "compass/google-maps-reviews-scraper": {"id": 842, "url": "https://apify.com/compass/google-maps-reviews-scraper/api/client/nodejs", "title": "Apify API and Google Maps Reviews Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.google.com/maps/place/Yellowstone+National+Park/@44.5857951,-110.5140571,9z/data=!3m1!4b1!4m5!3m4!1s0x5351e55555555555:0xaca8f930348fe1bb!8m2!3d44.427963!4d-110.588455?hl=en-GB\"\n        }\n    ],\n    \"maxReviews\": 100,\n    \"language\": \"en\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"compass/google-maps-reviews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"language": "en", "startUrls": [{"url": "https://www.google.com/maps/place/Yellowstone+National+Park/@44.5857951,-110.5140571,9z/data=!3m1!4b1!4m5!3m4!1s0x5351e55555555555:0xaca8f930348fe1bb!8m2!3d44.427963!4d-110.588455?hl=en-GB"}], "maxReviews": 100}, "actor_id": "compass/google-maps-reviews-scraper"}, "lukaskrivka/actor-fail-manager": {"id": 843, "url": "https://apify.com/lukaskrivka/actor-fail-manager/api/client/nodejs", "title": "Apify API and Actor fail manager interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/actor-fail-manager\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukaskrivka/actor-fail-manager"}, "curious_coder/apollo-io-scraper": {"id": 844, "url": "https://apify.com/curious_coder/apollo-io-scraper/api/client/nodejs", "title": "Apify API and Apollo.io leads scraper \u2705 Updated: 18 Oct interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startPage\": 1,\n    \"count\": 25,\n    \"minDelay\": 2,\n    \"maxDelay\": 7\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/apollo-io-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"count": 25, "maxDelay": 7, "minDelay": 2, "startPage": 1}, "actor_id": "curious_coder/apollo-io-scraper"}, "epctex/aliexpress-scraper": {"id": 845, "url": "https://apify.com/epctex/aliexpress-scraper/api/client/nodejs", "title": "Apify API and AliExpress Data Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://aliexpress.com/category/100003109/women-clothing.html\",\n        \"https://www.aliexpress.com/item/32940810951.html\"\n    ],\n    \"maxItems\": 10,\n    \"language\": \"en_US\",\n    \"shipTo\": \"US\",\n    \"currency\": \"USD\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    },\n    \"extendOutputFunction\": ($) => { return {} }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/aliexpress-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "shipTo": "US", "currency": "USD", "language": "en_US", "maxItems": 10, "startUrls": ["https://aliexpress.com/category/100003109/women-clothing.html", "https://www.aliexpress.com/item/32940810951.html"]}, "actor_id": "epctex/aliexpress-scraper"}, "jancurn/extract-metadata": {"id": 846, "url": "https://apify.com/jancurn/extract-metadata/api/client/nodejs", "title": "Apify API and Metadata Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urls\": [\n        \"https://www.apify.com/\",\n        \"https://blog.apify.com\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jancurn/extract-metadata\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": ["https://www.apify.com/", "https://blog.apify.com"], "proxy": {"useApifyProxy": true}}, "actor_id": "jancurn/extract-metadata"}, "misceres/indeed-scraper": {"id": 847, "url": "https://apify.com/misceres/indeed-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udcbc Indeed Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"position\": \"web developer\",\n    \"country\": \"US\",\n    \"location\": \"San Francisco\",\n    \"maxItems\": 50,\n    \"maxConcurrency\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"misceres/indeed-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"country": "US", "location": "San Francisco", "maxItems": 50, "position": "web developer", "maxConcurrency": 5}, "actor_id": "misceres/indeed-scraper"}, "m0uka/similarweb-scraper": {"id": 848, "url": "https://apify.com/m0uka/similarweb-scraper/api/client/nodejs", "title": "Apify API and Similarweb Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"websites\": [\n        \"apify.com\"\n    ],\n    \"proxyConfigurationOptions\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"m0uka/similarweb-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"websites": ["apify.com"], "proxyConfigurationOptions": {"useApifyProxy": true}}, "actor_id": "m0uka/similarweb-scraper"}, "apify/instagram-followers-count-scraper": {"id": 849, "url": "https://apify.com/apify/instagram-followers-count-scraper/api/client/nodejs", "title": "Apify API and Instagram Profile Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"usernames\": [\n        \"humansofny\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/instagram-followers-count-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"usernames": ["humansofny"]}, "actor_id": "apify/instagram-followers-count-scraper"}, "web.harvester/easy-twitter-search-scraper": {"id": 850, "url": "https://apify.com/web.harvester/easy-twitter-search-scraper/api/client/nodejs", "title": "Apify API and Easy Twitter Search Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchQueries\": [\n        \"career\"\n    ],\n    \"tweetsDesired\": 100,\n    \"language\": \"any\",\n    \"proxyConfig\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"web.harvester/easy-twitter-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"language": "any", "proxyConfig": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "searchQueries": ["career"], "tweetsDesired": 100}, "actor_id": "web.harvester/easy-twitter-search-scraper"}, "apify/page-analyzer": {"id": 851, "url": "https://apify.com/apify/page-analyzer/api/client/nodejs", "title": "Apify API and Page Scraping Analyzer interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\",\n    \"keywords\": [\n        \"A Light in the Attic\",\n        \"51.77\",\n        \"In stock\",\n        \"22 available\",\n        \"a897fe39b1053632\",\n        \"It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\"\n    ],\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/page-analyzer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html", "keywords": ["A Light in the Attic", "51.77", "In stock", "22 available", "a897fe39b1053632", "It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more"], "proxyConfig": {"useApifyProxy": true}}, "actor_id": "apify/page-analyzer"}, "jupri/woocommerce": {"id": 852, "url": "https://apify.com/jupri/woocommerce/api/client/nodejs", "title": "Apify API and WooCommerce Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://saris-extensions.co.uk\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/woocommerce\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://saris-extensions.co.uk"}, "actor_id": "jupri/woocommerce"}, "anchor/linkedin-people-finder": {"id": 853, "url": "https://apify.com/anchor/linkedin-people-finder/api/client/nodejs", "title": "Apify API and LinkedIn Profile URL People Finder interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"queries\": \"John Malkovich\\nMona lisa\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"anchor/linkedin-people-finder\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"queries": "John Malkovich\nMona lisa"}, "actor_id": "anchor/linkedin-people-finder"}, "epctex/twitter-profile-scraper": {"id": 854, "url": "https://apify.com/epctex/twitter-profile-scraper/api/client/nodejs", "title": "Apify API and Twitter Profile Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://twitter.com/apify\"\n    ],\n    \"maxItems\": 20,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/twitter-profile-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "maxItems": 20, "startUrls": ["https://twitter.com/apify"]}, "actor_id": "epctex/twitter-profile-scraper"}, "epctex/ultimate-proxy-scraper": {"id": 855, "url": "https://apify.com/epctex/ultimate-proxy-scraper/api/client/nodejs", "title": "Apify API and Proxy Scrape Master interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxItems\": 50,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/ultimate-proxy-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "maxItems": 50}, "actor_id": "epctex/ultimate-proxy-scraper"}, "epctex/clutchco-scraper": {"id": 856, "url": "https://apify.com/epctex/clutchco-scraper/api/client/nodejs", "title": "Apify API and Clutch.co Data Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"api\",\n    \"mode\": \"profiles\",\n    \"maxItems\": 50,\n    \"endPage\": 100,\n    \"extendOutputFunction\": ($) => { return {} },\n    \"customMapFunction\": (object) => { return object },\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/clutchco-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "profiles", "proxy": {"useApifyProxy": true}, "search": "api", "endPage": 100, "maxItems": 50}, "actor_id": "epctex/clutchco-scraper"}, "misceres/kickstarter-search": {"id": 857, "url": "https://apify.com/misceres/kickstarter-search/api/client/nodejs", "title": "Apify API and Kickstarter Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"Gadgets\",\n    \"maxResults\": 50,\n    \"category\": \"All\",\n    \"location\": \"United States\",\n    \"sort\": \"newest\",\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"misceres/kickstarter-search\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"sort": "newest", "query": "Gadgets", "category": "All", "location": "United States", "maxResults": 50, "proxyConfig": {"useApifyProxy": true}}, "actor_id": "misceres/kickstarter-search"}, "epctex/google-play-scraper": {"id": 858, "url": "https://apify.com/epctex/google-play-scraper/api/client/nodejs", "title": "Apify API and Google Play Store Insights Analyzer interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://play.google.com/store/search?q=hello&c=apps&hl=tr&gl=US\",\n        \"https://play.google.com/store/apps/developer?id=Mattel163+Limited\",\n        \"https://play.google.com/store/apps/details?id=com.tinybuildgames.helloneighbor&hl=tr&gl=US\"\n    ],\n    \"endReviewsPage\": 2,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/google-play-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": ["https://play.google.com/store/search?q=hello&c=apps&hl=tr&gl=US", "https://play.google.com/store/apps/developer?id=Mattel163+Limited", "https://play.google.com/store/apps/details?id=com.tinybuildgames.helloneighbor&hl=tr&gl=US"], "endReviewsPage": 2}, "actor_id": "epctex/google-play-scraper"}, "mtrunkat/article-text-extractor": {"id": 859, "url": "https://apify.com/mtrunkat/article-text-extractor/api/client/nodejs", "title": "Apify API and Article Text Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.bbc.com/news/world-asia-china-48659073\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mtrunkat/article-text-extractor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.bbc.com/news/world-asia-china-48659073"}, "actor_id": "mtrunkat/article-text-extractor"}, "inutil_labs/wscrp-free": {"id": 860, "url": "https://apify.com/inutil_labs/wscrp-free/api/client/nodejs", "title": "Apify API and Whatsapp Profiles Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"numbers\": [\n        \"34655719560\"\n    ],\n    \"token\": \"\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"inutil_labs/wscrp-free\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"token": "", "numbers": ["34655719560"]}, "actor_id": "inutil_labs/wscrp-free"}, "alexey/pinterest-crawler": {"id": 861, "url": "https://apify.com/alexey/pinterest-crawler/api/client/nodejs", "title": "Apify API and Pinterest Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"pinterest.com/poltronafrau\",\n        \"bellinthewoods\"\n    ],\n    \"maxPinsCnt\": 50,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"alexey/pinterest-crawler\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": ["pinterest.com/poltronafrau", "bellinthewoods"], "maxPinsCnt": 50, "proxyConfig": {"useApifyProxy": true}}, "actor_id": "alexey/pinterest-crawler"}, "epctex/binance-futures-leaderboard": {"id": 869, "url": "https://apify.com/epctex/binance-futures-leaderboard/api/client/nodejs", "title": "Apify API and Binance Data Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.binance.com/en/futures-activity/leaderboard/user/cm?encryptedUid=708A41D98F05FD23F8423AB41514E489\",\n        \"https://www.binance.com/en/futures-activity/leaderboard/futures\"\n    ],\n    \"maxItems\": 20,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/binance-futures-leaderboard\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "maxItems": 20, "startUrls": ["https://www.binance.com/en/futures-activity/leaderboard/user/cm?encryptedUid=708A41D98F05FD23F8423AB41514E489", "https://www.binance.com/en/futures-activity/leaderboard/futures"]}, "actor_id": "epctex/binance-futures-leaderboard"}, "pocesar/shopify-scraper": {"id": 862, "url": "https://apify.com/pocesar/shopify-scraper/api/client/nodejs", "title": "Apify API and Shopify Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.decathlon.com\"\n        }\n    ],\n    \"maxRequestsPerCrawl\": 10,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    },\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 10,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/shopify-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.decathlon.com"}], "customData": {}, "proxyConfig": {"useApifyProxy": true}, "maxConcurrency": 10, "maxRequestRetries": 3, "maxRequestsPerCrawl": 10}, "actor_id": "pocesar/shopify-scraper"}, "bebity/linkedin-jobs-scraper": {"id": 863, "url": "https://apify.com/bebity/linkedin-jobs-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udd25 LinkedIn Jobs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"companyName\": [\n        \"Google\",\n        \"Microsoft\"\n    ],\n    \"companyId\": [\n        \"76987811\",\n        \"1815218\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bebity/linkedin-jobs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"companyId": ["76987811", "1815218"], "companyName": ["Google", "Microsoft"]}, "actor_id": "bebity/linkedin-jobs-scraper"}, "epctex/darkweb-scraper": {"id": 864, "url": "https://apify.com/epctex/darkweb-scraper/api/client/nodejs", "title": "Apify API and Deep Web Data Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"http://xjfbpuj56rdazx4iolylxplbvyft2onuerjeimlcqwaihp3s6r4xebqd.onion/\"\n    ],\n    \"maxDepth\": 5,\n    \"maxPages\": 20,\n    \"extendOutputFunction\": ($) => { return {} }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/darkweb-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDepth": 5, "maxPages": 20, "startUrls": ["http://xjfbpuj56rdazx4iolylxplbvyft2onuerjeimlcqwaihp3s6r4xebqd.onion/"]}, "actor_id": "epctex/darkweb-scraper"}, "dtrungtin/imdb-scraper": {"id": 865, "url": "https://apify.com/dtrungtin/imdb-scraper/api/client/nodejs", "title": "Apify API and IMDb Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.imdb.com/search/title/?title_type=feature&user_rating=9.5\"\n        }\n    ],\n    \"maxItems\": 50,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"extendOutputFunction\": ($) => { return {} }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/imdb-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 50, "startUrls": [{"url": "https://www.imdb.com/search/title/?title_type=feature&user_rating=9.5"}], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "dtrungtin/imdb-scraper"}, "lordflotrox/linkedin-profile": {"id": 866, "url": "https://apify.com/lordflotrox/linkedin-profile/api/client/nodejs", "title": "Apify API and LinkedIn Profile interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.linkedin.com/in/ryanroslansky/\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lordflotrox/linkedin-profile\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.linkedin.com/in/ryanroslansky/"}, "actor_id": "lordflotrox/linkedin-profile"}, "petrpatek/covid-19-aggregator": {"id": 867, "url": "https://apify.com/petrpatek/covid-19-aggregator/api/client/nodejs", "title": "Apify API and Coronavirus stats across the World interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petrpatek/covid-19-aggregator\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petrpatek/covid-19-aggregator"}, "lukaskrivka/dedup-datasets": {"id": 868, "url": "https://apify.com/lukaskrivka/dedup-datasets/api/client/nodejs", "title": "Apify API and Merge, Dedup & Transform Datasets interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"preDedupTransformFunction\": async (items, { Apify }) => {\n        return items;\n    },\n    \"postDedupTransformFunction\": async (items, { Apify }) => {\n        return items;\n    },\n    \"customInputData\": {}\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/dedup-datasets\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customInputData": {}}, "actor_id": "lukaskrivka/dedup-datasets"}, "netscrape/free-tiktok-profile-scraper": {"id": 870, "url": "https://apify.com/netscrape/free-tiktok-profile-scraper/api/client/nodejs", "title": "Apify API and Free Tiktok Profile Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"tags\": [\n        \"#fyp\",\n        \"#viral\"\n    ],\n    \"result_count\": 30,\n    \"min_follower_count\": 0\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"netscrape/free-tiktok-profile-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"tags": ["#fyp", "#viral"], "result_count": 30, "min_follower_count": 0}, "actor_id": "netscrape/free-tiktok-profile-scraper"}, "apify/instagram-tagged-scraper": {"id": 871, "url": "https://apify.com/apify/instagram-tagged-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udd16 Instagram Mentions and Tagged Posts Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"username\": [\n        \"zelenskiy_official\"\n    ],\n    \"resultsLimit\": 30\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/instagram-tagged-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"username": ["zelenskiy_official"], "resultsLimit": 30}, "actor_id": "apify/instagram-tagged-scraper"}, "maxcopell/tripadvisor-reviews": {"id": 872, "url": "https://apify.com/maxcopell/tripadvisor-reviews/api/client/nodejs", "title": "Apify API and \ud83d\udcab Scrape Tripadvisor reviews interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.tripadvisor.com/Hotel_Review-g60763-d208453-Reviews-Hilton_New_York_Times_Square-New_York_City_New_York.html\"\n        }\n    ],\n    \"maxReviews\": 50,\n    \"language\": \"en\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"maxcopell/tripadvisor-reviews\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"language": "en", "startUrls": [{"url": "https://www.tripadvisor.com/Hotel_Review-g60763-d208453-Reviews-Hilton_New_York_Times_Square-New_York_City_New_York.html"}], "maxReviews": 50}, "actor_id": "maxcopell/tripadvisor-reviews"}, "epctex/realtor-scraper": {"id": 873, "url": "https://apify.com/epctex/realtor-scraper/api/client/nodejs", "title": "Apify API and Realtor.com Property Data Extractor \ud83c\udfd8\ufe0f interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.realtor.com/realestateandhomes-search/Las-Vegas_NV\",\n        \"https://www.realtor.com/realestateandhomes-detail/8209-Spring-Arts-Ave_Las-Vegas_NV_89129_M18560-54834\",\n        \"https://www.realtor.com/realestateagents/5a28883df695ab0010dfe28b\"\n    ],\n    \"maxItems\": 5,\n    \"endPage\": 1,\n    \"search\": \"las vegas\",\n    \"extendOutputFunction\": ($) => { return {} },\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/realtor-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "search": "las vegas", "endPage": 1, "maxItems": 5, "startUrls": ["https://www.realtor.com/realestateandhomes-search/Las-Vegas_NV", "https://www.realtor.com/realestateandhomes-detail/8209-Spring-Arts-Ave_Las-Vegas_NV_89129_M18560-54834", "https://www.realtor.com/realestateagents/5a28883df695ab0010dfe28b"]}, "actor_id": "epctex/realtor-scraper"}, "apify/facebook-url-to-id": {"id": 874, "url": "https://apify.com/apify/facebook-url-to-id/api/client/nodejs", "title": "Apify API and Facebook URL to ID interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"fbUrls\": [\n        {\n            \"url\": \"https://www.facebook.com/nintendo\"\n        },\n        {\n            \"url\": \"https://www.facebook.com/NintendoAmerica/posts/pfbid02JESEPSvyiLGjvewuFYhviAwQjTdxZW1ZfnWTeCzxVWgZLT3xgoLHVDwvuenVRyKKl\"\n        },\n        {\n            \"url\": \"https://www.facebook.com/NintendoAmerica/posts/pfbid02AB8FNERMDYwpsgVJVYaEQ8K2vpH4JLzPTvTB71WqiMfNQhu99ZMBmqvKZSt2tqT4l\"\n        },\n        {\n            \"url\": \"https://www.facebook.com/groups/germtheory.vs.terraintheory/permalink/5671657859577842/\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/facebook-url-to-id\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"fbUrls": [{"url": "https://www.facebook.com/nintendo"}, {"url": "https://www.facebook.com/NintendoAmerica/posts/pfbid02JESEPSvyiLGjvewuFYhviAwQjTdxZW1ZfnWTeCzxVWgZLT3xgoLHVDwvuenVRyKKl"}, {"url": "https://www.facebook.com/NintendoAmerica/posts/pfbid02AB8FNERMDYwpsgVJVYaEQ8K2vpH4JLzPTvTB71WqiMfNQhu99ZMBmqvKZSt2tqT4l"}, {"url": "https://www.facebook.com/groups/germtheory.vs.terraintheory/permalink/5671657859577842/"}]}, "actor_id": "apify/facebook-url-to-id"}, "trudax/upwork-scraper": {"id": 875, "url": "https://apify.com/trudax/upwork-scraper/api/client/nodejs", "title": "Apify API and Upwork Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.upwork.com/search/profiles/\"\n    ],\n    \"maxItems\": 1,\n    \"searchFor\": \"talent\",\n    \"extendOutputFunction\": async () => {\n      return { timestamp: Date.now() }\n    \n    },\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"trudax/upwork-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "maxItems": 1, "searchFor": "talent", "startUrls": ["https://www.upwork.com/search/profiles/"]}, "actor_id": "trudax/upwork-scraper"}, "curious_coder/telegram-scraper": {"id": 876, "url": "https://apify.com/curious_coder/telegram-scraper/api/client/nodejs", "title": "Apify API and Telegram scraper and adder interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"addUsersToGroup\",\n    \"authToken\": \"\",\n    \"groupUrl\": \"https://t.me/BitSquad\",\n    \"startPage\": 1,\n    \"count\": 10,\n    \"minDelay\": 1,\n    \"maxDelay\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/telegram-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"count": 10, "action": "addUsersToGroup", "groupUrl": "https://t.me/BitSquad", "maxDelay": 10, "minDelay": 1, "authToken": "", "startPage": 1}, "actor_id": "curious_coder/telegram-scraper"}, "dtrungtin/ebay-items-scraper": {"id": 877, "url": "https://apify.com/dtrungtin/ebay-items-scraper/api/client/nodejs", "title": "Apify API and eBay Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.ebay.com/sch/i.html?_from=R40&_trksid=p2499334.m570.l1313&_nkw=massage%2Brecliner%2Bchair&_sacat=6024\"\n        }\n    ],\n    \"maxItems\": 10,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/ebay-items-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 10, "startUrls": [{"url": "https://www.ebay.com/sch/i.html?_from=R40&_trksid=p2499334.m570.l1313&_nkw=massage%2Brecliner%2Bchair&_sacat=6024"}], "proxyConfig": {"useApifyProxy": true}}, "actor_id": "dtrungtin/ebay-items-scraper"}, "jancurn/residential-proxy-probe": {"id": 878, "url": "https://apify.com/jancurn/residential-proxy-probe/api/client/nodejs", "title": "Apify API and Residential Proxy Probe interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"countryCode\": \"us\",\n    \"dmaCodes\": \"524 602\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jancurn/residential-proxy-probe\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"dmaCodes": "524 602", "countryCode": "us"}, "actor_id": "jancurn/residential-proxy-probe"}, "lukaskrivka/keywords-extractor": {"id": 879, "url": "https://apify.com/lukaskrivka/keywords-extractor/api/client/nodejs", "title": "Apify API and Keywords Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://apify.com\"\n        }\n    ],\n    \"keywords\": [\n        \"apify\",\n        \"web\",\n        \"scraping\",\n        \"automation\"\n    ],\n    \"linkSelector\": \"a[href]\",\n    \"pseudoUrls\": [\n        {\n            \"purl\": \"https://apify.com/[.*]\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/keywords-extractor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"keywords": ["apify", "web", "scraping", "automation"], "startUrls": [{"url": "https://apify.com"}], "pseudoUrls": [{"purl": "https://apify.com/[.*]"}], "linkSelector": "a[href]", "proxyConfiguration": {"useApifyProxy": false}}, "actor_id": "lukaskrivka/keywords-extractor"}, "netscrape/tiktok-profile-scraper": {"id": 880, "url": "https://apify.com/netscrape/tiktok-profile-scraper/api/client/nodejs", "title": "Apify API and Tiktok Profile Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"tags\": [\n        \"#fyp\",\n        \"#viral\"\n    ],\n    \"result_count\": 100,\n    \"min_follower_count\": 0\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"netscrape/tiktok-profile-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"tags": ["#fyp", "#viral"], "result_count": 100, "min_follower_count": 0}, "actor_id": "netscrape/tiktok-profile-scraper"}, "drobnikj/extended-gpt-scraper": {"id": 881, "url": "https://apify.com/drobnikj/extended-gpt-scraper/api/client/nodejs", "title": "Apify API and Extended GPT Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://news.ycombinator.com/\"\n        }\n    ],\n    \"globs\": [],\n    \"linkSelector\": \"a[href]\",\n    \"instructions\": \"Gets the post with the most points from the page and returns it as JSON in this format: \\npostTitle\\npostUrl\\npointsCount\",\n    \"model\": \"gpt-3.5-turbo\",\n    \"targetSelector\": \"\",\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"title\": {\n                \"type\": \"string\",\n                \"description\": \"Page title\"\n            },\n            \"description\": {\n                \"type\": \"string\",\n                \"description\": \"Page description\"\n            }\n        },\n        \"required\": [\n            \"title\",\n            \"description\"\n        ]\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"drobnikj/extended-gpt-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"globs": [], "model": "gpt-3.5-turbo", "schema": {"type": "object", "required": ["title", "description"], "properties": {"title": {"type": "string", "description": "Page title"}, "description": {"type": "string", "description": "Page description"}}}, "startUrls": [{"url": "https://news.ycombinator.com/"}], "instructions": "Gets the post with the most points from the page and returns it as JSON in this format: \npostTitle\npostUrl\npointsCount", "linkSelector": "a[href]", "targetSelector": "", "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "drobnikj/extended-gpt-scraper"}, "jupri/g2-explorer": {"id": 882, "url": "https://apify.com/jupri/g2-explorer/api/client/nodejs", "title": "Apify API and G2 Product Reviews Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"apify\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/g2-explorer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "apify"}, "actor_id": "jupri/g2-explorer"}, "vbartonicek/topuniversities-scraper": {"id": 883, "url": "https://apify.com/vbartonicek/topuniversities-scraper/api/client/nodejs", "title": "Apify API and Top Universities Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"basic\",\n    \"maxItems\": 10,\n    \"year\": \"2022\",\n    \"country\": \"All countries\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"vbartonicek/topuniversities-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "basic", "year": "2022", "proxy": {"useApifyProxy": true}, "country": "All countries", "maxItems": 10}, "actor_id": "vbartonicek/topuniversities-scraper"}, "hooli/google-trending-searches": {"id": 884, "url": "https://apify.com/hooli/google-trending-searches/api/client/nodejs", "title": "Apify API and Google Trending Searches interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"geo\": \"US\",\n    \"outputMode\": \"complete\",\n    \"fromDate\": \"today\",\n    \"toDate\": \"3 days\",\n    \"maxItems\": 100,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    },\n    \"extendOutputFunction\": async ({ data, item, request, customData, fromDate, toDate, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ data, item, request, addUrl, customData, fromDate, toDate, extendOutputFunction, Apify }) => {\n     \n    },\n    \"customData\": {}\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hooli/google-trending-searches\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"geo": "US", "proxy": {"useApifyProxy": true}, "toDate": "3 days", "fromDate": "today", "maxItems": 100, "customData": {}, "outputMode": "complete"}, "actor_id": "hooli/google-trending-searches"}, "spidoosho/bing-search-scraper": {"id": 885, "url": "https://apify.com/spidoosho/bing-search-scraper/api/client/nodejs", "title": "Apify API and Bing Search Result Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"queries\": [\n        \"Hotels in NYC\",\n        \"Restaurants in NYC\",\n        \"https://www.bing.com/search?q=restaurants+in+NYC\"\n    ],\n    \"resultsPerPage\": 10,\n    \"maxPagesPerQuery\": 1,\n    \"marketCode\": \"en-US\",\n    \"languageCode\": \"en\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"spidoosho/bing-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"queries": ["Hotels in NYC", "Restaurants in NYC", "https://www.bing.com/search?q=restaurants+in+NYC"], "marketCode": "en-US", "languageCode": "en", "resultsPerPage": 10, "maxPagesPerQuery": 1, "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "spidoosho/bing-search-scraper"}, "roundedge/facebook-pages-search": {"id": 886, "url": "https://apify.com/roundedge/facebook-pages-search/api/client/nodejs", "title": "Apify API and Facebook Pages Search interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"locations\": [\n        \"New York\"\n    ],\n    \"categories\": [\n        \"Real Estate\"\n    ],\n    \"maxResults\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"roundedge/facebook-pages-search\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"locations": ["New York"], "categories": ["Real Estate"], "maxResults": 100}, "actor_id": "roundedge/facebook-pages-search"}, "jancurn/find-broken-links": {"id": 887, "url": "https://apify.com/jancurn/find-broken-links/api/client/nodejs", "title": "Apify API and \ud83d\udd17 Broken Link Checker interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"baseUrl\": \"https://www.example.com/\",\n    \"maxPages\": 1000,\n    \"notificationEmails\": [\n        \"email@example.com\"\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jancurn/find-broken-links\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"baseUrl": "https://www.example.com/", "maxPages": 1000, "notificationEmails": ["email@example.com"], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "jancurn/find-broken-links"}, "microworlds/twitter-scraper": {"id": 888, "url": "https://apify.com/microworlds/twitter-scraper/api/client/nodejs", "title": "Apify API and Twitter Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchTerms\": [\n        \"apple\"\n    ],\n    \"urls\": [\n        \"https://twitter.com/search?q=wikipedia&f=live&src=typed_query\"\n    ],\n    \"maxTweets\": 50,\n    \"searchMode\": \"live\",\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"microworlds/twitter-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": ["https://twitter.com/search?q=wikipedia&f=live&src=typed_query"], "maxTweets": 50, "searchMode": "live", "searchTerms": ["apple"], "maxRequestRetries": 3}, "actor_id": "microworlds/twitter-scraper"}, "epctex/apartments-scraper": {"id": 889, "url": "https://apify.com/epctex/apartments-scraper/api/client/nodejs", "title": "Apify API and Apartments.com Property Data Extractor \ud83c\udfe1 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.apartments.com/sobe-apartment-rentals-miami-beach-fl/26xc7jb/\",\n        \"https://www.apartments.com/apartments/miami-fl/student-housing/\",\n        \"https://www.apartments.com/?sk=544b92fe1f766950d57299dc624e9ff9&bb=0nijpwq7qH0t1s0oB\"\n    ],\n    \"maxItems\": 50,\n    \"endPage\": 100,\n    \"extendOutputFunction\": ($) => { return {} },\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/apartments-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "endPage": 100, "maxItems": 50, "startUrls": ["https://www.apartments.com/sobe-apartment-rentals-miami-beach-fl/26xc7jb/", "https://www.apartments.com/apartments/miami-fl/student-housing/", "https://www.apartments.com/?sk=544b92fe1f766950d57299dc624e9ff9&bb=0nijpwq7qH0t1s0oB"]}, "actor_id": "epctex/apartments-scraper"}, "apify/facebook-page-posts-checker": {"id": 890, "url": "https://apify.com/apify/facebook-page-posts-checker/api/client/nodejs", "title": "Apify API and Facebook page posts checker interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"pageNames\": [\n        \"humansofnewyork\"\n    ],\n    \"maxPosts\": 3,\n    \"groupLimit\": 0,\n    \"commentsLimit\": 0,\n    \"resultsLimit\": 10,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/facebook-page-posts-checker\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "maxPosts": 3, "pageNames": ["humansofnewyork"], "groupLimit": 0, "resultsLimit": 10, "commentsLimit": 0}, "actor_id": "apify/facebook-page-posts-checker"}, "apify/quick-instagram-posts-checker": {"id": 891, "url": "https://apify.com/apify/quick-instagram-posts-checker/api/client/nodejs", "title": "Apify API and Quick Instagram Posts Checker interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.instagram.com/p/CeoM7sMsLIU/\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/quick-instagram-posts-checker\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "startUrls": [{"url": "https://www.instagram.com/p/CeoM7sMsLIU/"}]}, "actor_id": "apify/quick-instagram-posts-checker"}, "bebity/linkedin-premium-actor": {"id": 892, "url": "https://apify.com/bebity/linkedin-premium-actor/api/client/nodejs", "title": "Apify API and \ud83d\udd25 Linkedin Companies & Profiles Bulk Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bebity/linkedin-premium-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"action":"get-profiles","keywords":["web dev"],"isUrl":false,"isName":false,"limit":1}, "actor_id": "bebity/linkedin-premium-actor"}, "mcdowell/tiktok": {"id": 893, "url": "https://apify.com/mcdowell/tiktok/api/client/nodejs", "title": "Apify API and Tiktok Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"hashtags\": [\n        \"tiktokshortfilm\"\n    ],\n    \"maxVideosPerPage\": 20,\n    \"maxCommentsPerPost\": 20,\n    \"maxRequestRetries\": 3,\n    \"maxConcurrency\": 5,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mcdowell/tiktok\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"hashtags": ["tiktokshortfilm"], "maxConcurrency": 5, "maxVideosPerPage": 20, "maxRequestRetries": 3, "maxCommentsPerPost": 20, "proxyConfiguration": {"useApifyProxy": true, "apifyProxyCountry": "US"}}, "actor_id": "mcdowell/tiktok"}, "krish_patel/yellow-pages-scraper-withemail": {"id": 894, "url": "https://apify.com/krish_patel/yellow-pages-scraper-withemail/api/client/nodejs", "title": "Apify API and yellow-pages-scraper-withEmail interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"Dentist\",\n    \"location\": \"Los Angeles\",\n    \"maxItems\": 200,\n    \"extendOutputFunction\": ($, record) => {\n        return {};\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"krish_patel/yellow-pages-scraper-withemail\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"search": "Dentist", "location": "Los Angeles", "maxItems": 200, "proxyConfiguration": {"useApifyProxy": false}}, "actor_id": "krish_patel/yellow-pages-scraper-withemail"}, "casper11515/trustpilot-reviews-scraper": {"id": 895, "url": "https://apify.com/casper11515/trustpilot-reviews-scraper/api/client/nodejs", "title": "Apify API and Trustpilot reviews scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"companyWebsite\": \"shopwagandtail.com\",\n    \"startFromPageNumber\": \"\",\n    \"endAtPageNumber\": \"\",\n    \"Proxy configuration\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"casper11515/trustpilot-reviews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"companyWebsite": "shopwagandtail.com", "endAtPageNumber": "", "Proxy configuration": {"useApifyProxy": false}, "startFromPageNumber": ""}, "actor_id": "casper11515/trustpilot-reviews-scraper"}, "dan.scraper/google-jobs-scraper": {"id": 896, "url": "https://apify.com/dan.scraper/google-jobs-scraper/api/client/nodejs", "title": "Apify API and Google Jobs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"queries\": \"Teacher\\nhttps://www.google.com/search?q=doctor&ibp=htl;jobs\",\n    \"maxPagesPerQuery\": 1,\n    \"countryCode\": \"\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dan.scraper/google-jobs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"queries": "Teacher\nhttps://www.google.com/search?q=doctor&ibp=htl;jobs", "countryCode": "", "maxPagesPerQuery": 1}, "actor_id": "dan.scraper/google-jobs-scraper"}, "jupri/airbnb": {"id": 897, "url": "https://apify.com/jupri/airbnb/api/client/nodejs", "title": "Apify API and AirBnb Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"limit\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/airbnb\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"limit": 5}, "actor_id": "jupri/airbnb"}, "jancurn/screenshot-taker": {"id": 898, "url": "https://apify.com/jancurn/screenshot-taker/api/client/nodejs", "title": "Apify API and Screenshot Taker interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urls\": [\n        {\n            \"url\": \"https://www.example.com\"\n        },\n        {\n            \"url\": \"https://sdk.apify.com\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jancurn/screenshot-taker\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": [{"url": "https://www.example.com"}, {"url": "https://sdk.apify.com"}]}, "actor_id": "jancurn/screenshot-taker"}, "apify/proxy-test": {"id": 899, "url": "https://apify.com/apify/proxy-test/api/client/nodejs", "title": "Apify API and Proxy Test interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"testUrls\": [\n        {\n            \"url\": \"https://example.com\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/proxy-test\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "testUrls": [{"url": "https://example.com"}]}, "actor_id": "apify/proxy-test"}, "jancurn/pdf-to-html": {"id": 900, "url": "https://apify.com/jancurn/pdf-to-html/api/client/nodejs", "title": "Apify API and PDF to HTML Converter interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://apify.com/ext/ycf_application.pdf\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jancurn/pdf-to-html\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://apify.com/ext/ycf_application.pdf"}, "actor_id": "jancurn/pdf-to-html"}, "curious_coder/onlyfans-scraper": {"id": 901, "url": "https://apify.com/curious_coder/onlyfans-scraper/api/client/nodejs", "title": "Apify API and Onlyfans profile scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"profileUrls\": [\n        \"https://onlyfans.com/onlyfans\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/onlyfans-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"profileUrls": ["https://onlyfans.com/onlyfans"]}, "actor_id": "curious_coder/onlyfans-scraper"}, "svpetrenko/quora-scraper": {"id": 902, "url": "https://apify.com/svpetrenko/quora-scraper/api/client/nodejs", "title": "Apify API and Quora Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"queries\": [\n        \"how to drink yogurt\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"svpetrenko/quora-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "queries": ["how to drink yogurt"]}, "actor_id": "svpetrenko/quora-scraper"}, "dtrungtin/allrecipes-scraper": {"id": 903, "url": "https://apify.com/dtrungtin/allrecipes-scraper/api/client/nodejs", "title": "Apify API and Allrecipes Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.allrecipes.com/recipes/276/desserts/cakes/\"\n        }\n    ],\n    \"maxItems\": 50,\n    \"extendOutputFunction\": ($, pageResult) => {\n      return {}\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/allrecipes-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 50, "startUrls": [{"url": "https://www.allrecipes.com/recipes/276/desserts/cakes/"}], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "dtrungtin/allrecipes-scraper"}, "selcukitmis/binance-top-traders-positions": {"id": 904, "url": "https://apify.com/selcukitmis/binance-top-traders-positions/api/client/nodejs", "title": "Apify API and Binance Top Traders Future Positions, Binance Top Traders interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"selcukitmis/binance-top-traders-positions\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "selcukitmis/binance-top-traders-positions"}, "apify/example-puppeteer": {"id": 905, "url": "https://apify.com/apify/example-puppeteer/api/client/nodejs", "title": "Apify API and Example Puppeteer interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/example-puppeteer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/example-puppeteer"}, "jaroslavhejlek/kickstarter-location-to-ids": {"id": 906, "url": "https://apify.com/jaroslavhejlek/kickstarter-location-to-ids/api/client/nodejs", "title": "Apify API and Kickstarter Location To Ids interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jaroslavhejlek/kickstarter-location-to-ids\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jaroslavhejlek/kickstarter-location-to-ids"}, "epctex/osint-scraper": {"id": 907, "url": "https://apify.com/epctex/osint-scraper/api/client/nodejs", "title": "Apify API and OSINT Data Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchKeywords\": [\n        \"a\"\n    ],\n    \"maxItems\": 20,\n    \"extendOutputFunction\": ($) => { return {} },\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/osint-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "maxItems": 20, "searchKeywords": ["a"]}, "actor_id": "epctex/osint-scraper"}, "drobnikj/js-code-2-flowchart": {"id": 908, "url": "https://apify.com/drobnikj/js-code-2-flowchart/api/client/nodejs", "title": "Apify API and JavaScript Code to Flowchart interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"code\": // The function accepts a single argument: the \"context\" object.\n    // For a complete list of its properties and functions,\n    // see https://apify.com/apify/web-scraper#page-function \n    async function pageFunction(context) {\n        // This statement works as a breakpoint when you're trying to debug your code. Works only with Run mode: DEVELOPMENT!\n        // debugger; \n    \n        // jQuery is handy for finding DOM elements and extracting data from them.\n        // To use it, make sure to enable the \"Inject jQuery\" option.\n        const $ = context.jQuery;\n        const pageTitle = $('title').first().text();\n    \n        // Print some information to actor log\n        context.log.info(`URL: ${context.request.url}, TITLE: ${pageTitle}`);\n    \n        // Manually add a new page to the queue for scraping.\n        context.enqueueRequest({ url: 'http://www.example.com' });\n    \n        // Return an object with the data extracted from the page.\n        // It will be stored to the resulting dataset.\n        return {\n            url: context.request.url,\n            pageTitle\n        };\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"drobnikj/js-code-2-flowchart\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "drobnikj/js-code-2-flowchart"}, "jancurn/url-to-pdf": {"id": 909, "url": "https://apify.com/jancurn/url-to-pdf/api/client/nodejs", "title": "Apify API and HTML to PDF Converter interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jancurn/url-to-pdf\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jancurn/url-to-pdf"}, "lhotanok/google-news-scraper": {"id": 910, "url": "https://apify.com/lhotanok/google-news-scraper/api/client/nodejs", "title": "Apify API and Google News Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"Tesla\",\n    \"language\": \"US:en\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lhotanok/google-news-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "Tesla", "language": "US:en", "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "lhotanok/google-news-scraper"}, "dhrumil/opensea-collection-scraper": {"id": 911, "url": "https://apify.com/dhrumil/opensea-collection-scraper/api/client/nodejs", "title": "Apify API and Opensea Collection Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://opensea.io/collection/10ktf-stockroom\"\n        }\n    ],\n    \"maxItemsPerCollection\": 20,\n    \"extendOutputFunction\": (asset) => {\n        const result = {};\n        // Uncomment to add a asset id to the output\n        // result.id = asset.id;\n    \n        return result;\n    },\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dhrumil/opensea-collection-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://opensea.io/collection/10ktf-stockroom"}], "maxItemsPerCollection": 20}, "actor_id": "dhrumil/opensea-collection-scraper"}, "yir/pdf-scraper": {"id": 912, "url": "https://apify.com/yir/pdf-scraper/api/client/nodejs", "title": "Apify API and Example PDF Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"http://www.pdf995.com/samples/pdf.pdf\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"yir/pdf-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "http://www.pdf995.com/samples/pdf.pdf"}, "actor_id": "yir/pdf-scraper"}, "epctex/twitter-search-scraper": {"id": 913, "url": "https://apify.com/epctex/twitter-search-scraper/api/client/nodejs", "title": "Apify API and Twitter Search Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"keywords\": [\n        \"#binance\",\n        \"ukraine\"\n    ],\n    \"maxItems\": 20,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/twitter-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "keywords": ["#binance", "ukraine"], "maxItems": 20}, "actor_id": "epctex/twitter-search-scraper"}, "shmlkv/facebook-marketplace": {"id": 914, "url": "https://apify.com/shmlkv/facebook-marketplace/api/client/nodejs", "title": "Apify API and Facebook Marketplace interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.facebook.com/marketplace/telaviv/apartments-for-rent\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"shmlkv/facebook-marketplace\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.facebook.com/marketplace/telaviv/apartments-for-rent"}]}, "actor_id": "shmlkv/facebook-marketplace"}, "autofacts/shopify": {"id": 915, "url": "https://apify.com/autofacts/shopify/api/client/nodejs", "title": "Apify API and Shopify Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.gymshark.com/collections/crop-tops\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"autofacts/shopify\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://www.gymshark.com/collections/crop-tops"}]}, "actor_id": "autofacts/shopify"}, "jancurn/analyze-domains": {"id": 916, "url": "https://apify.com/jancurn/analyze-domains/api/client/nodejs", "title": "Apify API and Naked Domains Analyzer interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"domains\": `example.com\n        iana.org\n        apify.com`,\n    \"maxRequestRetries\": 2,\n    \"crawlLinkCount\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jancurn/analyze-domains\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"domains": "example.com\n        iana.org\n        apify.com", "crawlLinkCount": 5, "maxRequestRetries": 2}, "actor_id": "jancurn/analyze-domains"}, "jupri/zillow-scraper": {"id": 917, "url": "https://apify.com/jupri/zillow-scraper/api/client/nodejs", "title": "Apify API and Zillow Property Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": [\n        \"New York\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/zillow-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"location": ["New York"]}, "actor_id": "jupri/zillow-scraper"}, "natasha.lekh/gas-prices-scraper": {"id": 918, "url": "https://apify.com/natasha.lekh/gas-prices-scraper/api/client/nodejs", "title": "Apify API and Free Gas Prices Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"New York\",\n    \"maxCrawledPlacesPerSearch\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"natasha.lekh/gas-prices-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"location": "New York", "maxCrawledPlacesPerSearch": 10}, "actor_id": "natasha.lekh/gas-prices-scraper"}, "jupri/shopify-scraper": {"id": 919, "url": "https://apify.com/jupri/shopify-scraper/api/client/nodejs", "title": "Apify API and Shopify Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://watchesofamerica.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/shopify-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://watchesofamerica.com"}, "actor_id": "jupri/shopify-scraper"}, "jupri/onlyfans-scraper": {"id": 920, "url": "https://apify.com/jupri/onlyfans-scraper/api/client/nodejs", "title": "Apify API and OnlyFans.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/onlyfans-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jupri/onlyfans-scraper"}, "junglee/amazon-asins-scraper": {"id": 921, "url": "https://apify.com/junglee/amazon-asins-scraper/api/client/nodejs", "title": "Apify API and Amazon ASINs scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"asins\": [\n        \"B08BHHSB6M\"\n    ],\n    \"amazonDomain\": \"amazon.com\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"junglee/amazon-asins-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"asins": ["B08BHHSB6M"], "amazonDomain": "amazon.com", "proxyConfiguration": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}}, "actor_id": "junglee/amazon-asins-scraper"}, "alexey/youtube-and-all-videos": {"id": 922, "url": "https://apify.com/alexey/youtube-and-all-videos/api/client/nodejs", "title": "Apify API and YouTube and All Videos PRO Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"alexey/youtube-and-all-videos\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "alexey/youtube-and-all-videos"}, "newpo/tiktok-30x-scraper-free-experience": {"id": 923, "url": "https://apify.com/newpo/tiktok-30x-scraper-free-experience/api/client/nodejs", "title": "Apify API and Tiktok 30x Scraper Free Experience interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"hashtag\": \"petsoftiktok\",\n    \"user\": \"gordonramsayofficial\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"newpo/tiktok-30x-scraper-free-experience\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"user": "gordonramsayofficial", "proxy": {"useApifyProxy": true}, "hashtag": "petsoftiktok"}, "actor_id": "newpo/tiktok-30x-scraper-free-experience"}, "danielmilevski9/telegram-channel-scraper": {"id": 924, "url": "https://apify.com/danielmilevski9/telegram-channel-scraper/api/client/nodejs", "title": "Apify API and Telegram Channel Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"channels\": [\n        \"mediumcom\"\n    ],\n    \"postsFrom\": 10,\n    \"postsTo\": 20,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"danielmilevski9/telegram-channel-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "postsTo": 20, "channels": ["mediumcom"], "postsFrom": 10}, "actor_id": "danielmilevski9/telegram-channel-scraper"}, "lukass/shopee-scraper": {"id": 925, "url": "https://apify.com/lukass/shopee-scraper/api/client/nodejs", "title": "Apify API and Shopee Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"keyword\": \"jeans\",\n    \"maxItems\": 3,\n    \"startUrl\": [],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukass/shopee-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "keyword": "jeans", "maxItems": 3, "startUrl": []}, "actor_id": "lukass/shopee-scraper"}, "iglu/e-commerce-scraper": {"id": 926, "url": "https://apify.com/iglu/e-commerce-scraper/api/client/nodejs", "title": "Apify API and E-Commerce Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"uri\": \"https://www.amazon.it/amazon-echo-dot-3-generazione-altoparlante-intelligente-con-integrazione-alexa-tessuto-antracite/dp/B07PHPXHQS\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"iglu/e-commerce-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"uri": "https://www.amazon.it/amazon-echo-dot-3-generazione-altoparlante-intelligente-con-integrazione-alexa-tessuto-antracite/dp/B07PHPXHQS"}, "actor_id": "iglu/e-commerce-scraper"}, "dtrungtin/covid-vi": {"id": 927, "url": "https://apify.com/dtrungtin/covid-vi/api/client/nodejs", "title": "Apify API and Coronavirus stats in Vietnam interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/covid-vi\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "dtrungtin/covid-vi"}, "glosterr/soccerstats-scraper": {"id": 928, "url": "https://apify.com/glosterr/soccerstats-scraper/api/client/nodejs", "title": "Apify API and Soccerstats football scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"informationType\": \"TABLES\",\n    \"selectedLeagues\": [\n        \"England\"\n    ],\n    \"startWeek\": 1,\n    \"endWeek\": 4,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"glosterr/soccerstats-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"endWeek": 4, "startWeek": 1, "proxyConfig": {"useApifyProxy": true}, "informationType": "TABLES", "selectedLeagues": ["England"]}, "actor_id": "glosterr/soccerstats-scraper"}, "lukaskrivka/images-download-upload": {"id": 929, "url": "https://apify.com/lukaskrivka/images-download-upload/api/client/nodejs", "title": "Apify API and Dataset Image Downloader & Uploader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"pathToImageUrls\": \"images\",\n    \"fileNameFunction\": ({url, md5}) => md5(url),\n    \"uploadTo\": \"zip-file\",\n    \"preDownloadFunction\": `/* Example: We get rid of the items with price 0\n        ({ data }) => data.filter((item) => item.price > 0)\n        */`,\n    \"postDownloadFunction\": `/* Example: We remove items without any successfully uploaded images.\n         We also remove any image URLs that were not uploaded\n         \n         ({ data, state }) => {\n            return data.reduce((newData, item) => {\n                const downloadedImages = item.images.filter((imageUrl) => {\n                    return state[imageUrl] && state[imageUrl].imageUploaded;\n                });\n                \n                if (downloadedImages.length === 0) {\n                    return newData;\n                }\n                \n                return newData.concat({ ...item, images: downloadedImages });\n            }, []);\n        }\n        */`,\n    \"imageCheckType\": \"content-type\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/images-download-upload\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"uploadTo": "zip-file", "imageCheckType": "content-type", "pathToImageUrls": "images", "proxyConfiguration": {"useApifyProxy": true}, "preDownloadFunction": "/* Example: We get rid of the items with price 0\n        ({ data }) => data.filter((item) => item.price > 0)\n        */", "postDownloadFunction": "/* Example: We remove items without any successfully uploaded images.\n         We also remove any image URLs that were not uploaded\n         \n         ({ data, state }) => {\n            return data.reduce((newData, item) => {\n                const downloadedImages = item.images.filter((imageUrl) => {\n                    return state[imageUrl] && state[imageUrl].imageUploaded;\n                });\n                \n                if (downloadedImages.length === 0) {\n                    return newData;\n                }\n                \n                return newData.concat({ ...item, images: downloadedImages });\n            }, []);\n        }\n        */"}, "actor_id": "lukaskrivka/images-download-upload"}, "pocesar/json-downloader": {"id": 930, "url": "https://apify.com/pocesar/json-downloader/api/client/nodejs", "title": "Apify API and API / JSON scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://blockchain.info/latestblock\"\n        }\n    ],\n    \"filterMap\": async ({ request, response, addRequest, flattenObjectKeys, context, Apify, _, moment, data, input, customData }) => {\n      return { url: request.url, data };\n    },\n    \"handleError\": async ({ error, request, response, addRequest, _, moment, Apify, input, context }) => {\n      \n    },\n    \"customData\": {},\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    },\n    \"maxRequestRetries\": 4\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/json-downloader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://blockchain.info/latestblock"}], "customData": {}, "proxyConfig": {"useApifyProxy": true}, "maxRequestRetries": 4}, "actor_id": "pocesar/json-downloader"}, "jupri/booking-hotels": {"id": 966, "url": "https://apify.com/jupri/booking-hotels/api/client/nodejs", "title": "Apify API and Booking.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"United States of America\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/booking-hotels\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"location": "United States of America"}, "actor_id": "jupri/booking-hotels"}, "bebity/web-traffic-generator": {"id": 931, "url": "https://apify.com/bebity/web-traffic-generator/api/client/nodejs", "title": "Apify API and \ud83d\udd25 Web Traffic Generator | \ud83d\ude80 WebRocket \ud83d\ude80 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://bebity.io\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bebity/web-traffic-generator\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": ["https://bebity.io"]}, "actor_id": "bebity/web-traffic-generator"}, "valek.josef/weather-scraper": {"id": 932, "url": "https://apify.com/valek.josef/weather-scraper/api/client/nodejs", "title": "Apify API and Weather Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://weather.com/weather/today/l/f892433d7660da170347398eb8e3d722d8d362fe7dd15af16ce88324e1b96e70\"\n        }\n    ],\n    \"timeFrame\": \"today\",\n    \"units\": \"imperial\",\n    \"maxItems\": 10,\n    \"locations\": [],\n    \"locationIds\": [],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"extendOutputFunction\": () => ({})\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"valek.josef/weather-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"units": "imperial", "maxItems": 10, "locations": [], "startUrls": [{"url": "https://weather.com/weather/today/l/f892433d7660da170347398eb8e3d722d8d362fe7dd15af16ce88324e1b96e70"}], "timeFrame": "today", "locationIds": [], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "valek.josef/weather-scraper"}, "pocesar/login-session": {"id": 933, "url": "https://apify.com/pocesar/login-session/api/client/nodejs", "title": "Apify API and Login Session interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"userAgent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36\",\n    \"maxRequestRetries\": 0,\n    \"sessionConfig\": {\n        \"storageName\": \"login-sessions\",\n        \"maxAgeSecs\": 3600,\n        \"maxUsageCount\": 100,\n        \"maxPoolSize\": 100\n    },\n    \"steps\": [\n        {\n            \"username\": {\n                \"selector\": \"input#email\"\n            },\n            \"password\": {\n                \"selector\": \"input#pass\"\n            },\n            \"submit\": {\n                \"selector\": \"input[type=\\\"submit\\\"]\"\n            },\n            \"failed\": {\n                \"selector\": \"[role=\\\"alert\\\"]\"\n            },\n            \"waitForMillis\": 5000\n        }\n    ],\n    \"cookieDomains\": [],\n    \"gotoTimeout\": 30,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"extraUrlPatterns\": []\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/login-session\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"steps": [{"failed": {"selector": "[role=\"alert\"]"}, "submit": {"selector": "input[type=\"submit\"]"}, "password": {"selector": "input#pass"}, "username": {"selector": "input#email"}, "waitForMillis": 5000}], "userAgent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36", "gotoTimeout": 30, "cookieDomains": [], "sessionConfig": {"maxAgeSecs": 3600, "maxPoolSize": 100, "storageName": "login-sessions", "maxUsageCount": 100}, "extraUrlPatterns": [], "maxRequestRetries": 0, "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "pocesar/login-session"}, "timwhite/twitch-channel-email-finder": {"id": 934, "url": "https://apify.com/timwhite/twitch-channel-email-finder/api/client/nodejs", "title": "Apify API and Twitch Email Finder interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.twitch.tv/katerino\"\n        },\n        {\n            \"url\": \"https://www.twitch.tv/monstercat\"\n        },\n        {\n            \"url\": \"https://www.twitch.tv/riotgames\"\n        }\n    ],\n    \"handlePageTimeoutSecs\": 30,\n    \"maxRequestRetries\": 1,\n    \"minConcurrency\": 1,\n    \"maxConcurrency\": 100,\n    \"maxRequestsPerCrawl\": 10,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"timwhite/twitch-channel-email-finder\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.twitch.tv/katerino"}, {"url": "https://www.twitch.tv/monstercat"}, {"url": "https://www.twitch.tv/riotgames"}], "maxConcurrency": 100, "minConcurrency": 1, "maxRequestRetries": 1, "proxyConfiguration": {"useApifyProxy": true}, "maxRequestsPerCrawl": 10, "handlePageTimeoutSecs": 30}, "actor_id": "timwhite/twitch-channel-email-finder"}, "shanes/tweet-flash-plus": {"id": 935, "url": "https://apify.com/shanes/tweet-flash-plus/api/client/nodejs", "title": "Apify API and Tweet Flash Plus (working as of 10/13) interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"shanes/tweet-flash-plus\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"max_tweets":500,"language":"any","collect_user_info":false,"use_experimental_scraper":false,"user_info":"user info and replying info","max_attempts":5}, "actor_id": "shanes/tweet-flash-plus"}, "junglee/amazon-seller-scraper": {"id": 936, "url": "https://apify.com/junglee/amazon-seller-scraper/api/client/nodejs", "title": "Apify API and Amazon Seller Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.amazon.com/s?k=keyboard\"\n        }\n    ],\n    \"maxProducts\": 10,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"junglee/amazon-seller-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.amazon.com/s?k=keyboard"}], "maxProducts": 10, "proxyConfiguration": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}}, "actor_id": "junglee/amazon-seller-scraper"}, "argusapi/snapchat-profile-scraper": {"id": 937, "url": "https://apify.com/argusapi/snapchat-profile-scraper/api/client/nodejs", "title": "Apify API and Snapchat Profile Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"username\": [\n        \"loganpaul\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"argusapi/snapchat-profile-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"username": ["loganpaul"]}, "actor_id": "argusapi/snapchat-profile-scraper"}, "lhotanok/ticketmaster-scraper": {"id": 938, "url": "https://apify.com/lhotanok/ticketmaster-scraper/api/client/nodejs", "title": "Apify API and Scrapes events by categories, genres, location and date. interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxItems\": 200,\n    \"countryCode\": \"US\",\n    \"geoHash\": \"dr5regw3pg6ft\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lhotanok/ticketmaster-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"geoHash": "dr5regw3pg6ft", "maxItems": 200, "countryCode": "US"}, "actor_id": "lhotanok/ticketmaster-scraper"}, "vaclavrut/sitemap-sniffer": {"id": 939, "url": "https://apify.com/vaclavrut/sitemap-sniffer/api/client/nodejs", "title": "Apify API and Sitemap Sniffer interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://mall.cz\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"vaclavrut/sitemap-sniffer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://mall.cz", "proxy": {"useApifyProxy": true}}, "actor_id": "vaclavrut/sitemap-sniffer"}, "junglee/amazon-bestsellers": {"id": 940, "url": "https://apify.com/junglee/amazon-bestsellers/api/client/nodejs", "title": "Apify API and \u2b50\ufe0f Amazon Bestsellers Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"categoryUrls\": [\n        \"https://www.amazon.com/Best-Sellers-Electronics/zgbs/electronics/\"\n    ],\n    \"depthOfCrawl\": 1,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"junglee/amazon-bestsellers\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "categoryUrls": ["https://www.amazon.com/Best-Sellers-Electronics/zgbs/electronics/"], "depthOfCrawl": 1}, "actor_id": "junglee/amazon-bestsellers"}, "tomas_jindra/flashscore-scraper": {"id": 941, "url": "https://apify.com/tomas_jindra/flashscore-scraper/api/client/nodejs", "title": "Apify API and Flashscore Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"inputURL\": [\n        {\n            \"url\": \"https://www.flashscore.com/floorball/hungary/ob1/results/\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"tomas_jindra/flashscore-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"inputURL": [{"url": "https://www.flashscore.com/floorball/hungary/ob1/results/"}], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "tomas_jindra/flashscore-scraper"}, "flood/linkedin-company-search-scraper": {"id": 942, "url": "https://apify.com/flood/linkedin-company-search-scraper/api/client/nodejs", "title": "Apify API and LinkedIn Company Search Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"keywords\": \"apify\",\n    \"location\": [\n        \"Czechia\"\n    ],\n    \"industry\": [\n        \"Tech\"\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"flood/linkedin-company-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"industry": ["Tech"], "keywords": "apify", "location": ["Czechia"], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "flood/linkedin-company-search-scraper"}, "jahmadkhan/yelp-b2b-email-lead-generation": {"id": 943, "url": "https://apify.com/jahmadkhan/yelp-b2b-email-lead-generation/api/client/nodejs", "title": "Apify API and Yelp || B2B Email Lead Generation interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchKeyowords\": \"gift shops\",\n    \"locationsZipCodes\": \"05141\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jahmadkhan/yelp-b2b-email-lead-generation\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "searchKeyowords": "gift shops", "locationsZipCodes": "05141"}, "actor_id": "jahmadkhan/yelp-b2b-email-lead-generation"}, "mhamas/website-backup": {"id": 944, "url": "https://apify.com/mhamas/website-backup/api/client/nodejs", "title": "Apify API and Website Backup interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startURLs\": [\n        {\n            \"url\": \"https://blog.apify.com\"\n        }\n    ],\n    \"linkSelector\": \"a\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mhamas/website-backup\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startURLs": [{"url": "https://blog.apify.com"}], "linkSelector": "a", "proxyConfiguration": {"useApifyProxy": false}}, "actor_id": "mhamas/website-backup"}, "mscraper/similarweb-quick-scraper": {"id": 945, "url": "https://apify.com/mscraper/similarweb-quick-scraper/api/client/nodejs", "title": "Apify API and Similarweb Quick Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"websites\": [\n        \"apple.com\",\n        \"microsoft.com\",\n        \"google.com\",\n        \"twitter.com\",\n        \"samsung.com\",\n        \"hp.com\",\n        \"dell.com\",\n        \"mi.com\",\n        \"lg.com\",\n        \"sony.com\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/similarweb-quick-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyCountry": "US"}, "websites": ["apple.com", "microsoft.com", "google.com", "twitter.com", "samsung.com", "hp.com", "dell.com", "mi.com", "lg.com", "sony.com"]}, "actor_id": "mscraper/similarweb-quick-scraper"}, "arcatdmz/twitter-ff": {"id": 946, "url": "https://apify.com/arcatdmz/twitter-ff/api/client/nodejs", "title": "Apify API and Twitter Info Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"arcatdmz/twitter-ff\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "arcatdmz/twitter-ff"}, "stefanie-rink/youtube-scraper": {"id": 947, "url": "https://apify.com/stefanie-rink/youtube-scraper/api/client/nodejs", "title": "Apify API and Advanced Youtube Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"queries\": [\n        \"music\",\n        \"https://www.youtube.com/@Google/videos\",\n        \"https://www.youtube.com/watch?v=0EqSXDwTq6U\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"stefanie-rink/youtube-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"queries": ["music", "https://www.youtube.com/@Google/videos", "https://www.youtube.com/watch?v=0EqSXDwTq6U"]}, "actor_id": "stefanie-rink/youtube-scraper"}, "jupri/realtor-agents": {"id": 948, "url": "https://apify.com/jupri/realtor-agents/api/client/nodejs", "title": "Apify API and Realtor Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"Washington, DC\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/realtor-agents\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"location": "Washington, DC"}, "actor_id": "jupri/realtor-agents"}, "epctex/bbb-scraper": {"id": 949, "url": "https://apify.com/epctex/bbb-scraper/api/client/nodejs", "title": "Apify API and BBB Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.bbb.org/us/ny/new-york/category/home-improvement\",\n        \"https://www.bbb.org/us/ny/new-york/categories\",\n        \"https://www.bbb.org/us/ny/new-york/profile/home-builders/jf-hughes-builders-inc-0121-87134180\",\n        \"https://www.bbb.org/search?find_country=USA&find_entity=66005-000&find_id=1099_2800-400-1100&find_latlng=40.762801%2C-73.977818&find_loc=New%20York%2C%20NY&find_text=Clean%20Up%20Services&find_type=Category&page=1&sort=Relevance\"\n    ],\n    \"maxItems\": 50,\n    \"endPage\": 1,\n    \"extendOutputFunction\": ($) => { return {} },\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/bbb-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "endPage": 1, "maxItems": 50, "startUrls": ["https://www.bbb.org/us/ny/new-york/category/home-improvement", "https://www.bbb.org/us/ny/new-york/categories", "https://www.bbb.org/us/ny/new-york/profile/home-builders/jf-hughes-builders-inc-0121-87134180", "https://www.bbb.org/search?find_country=USA&find_entity=66005-000&find_id=1099_2800-400-1100&find_latlng=40.762801%2C-73.977818&find_loc=New%20York%2C%20NY&find_text=Clean%20Up%20Services&find_type=Category&page=1&sort=Relevance"]}, "actor_id": "epctex/bbb-scraper"}, "alexey/google-maps-pins-map-ocr": {"id": 950, "url": "https://apify.com/alexey/google-maps-pins-map-ocr/api/client/nodejs", "title": "Apify API and OCR for Google Maps pins interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"alexey/google-maps-pins-map-ocr\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "alexey/google-maps-pins-map-ocr"}, "alexey/google-lens": {"id": 951, "url": "https://apify.com/alexey/google-lens/api/client/nodejs", "title": "Apify API and \ud83d\udc41 Google Lens API interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://hips.hearstapps.com/hmg-prod/images/two-glasses-of-wine-and-summer-fruits-on-the-beach-royalty-free-image-1635272439.jpg\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"alexey/google-lens\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://hips.hearstapps.com/hmg-prod/images/two-glasses-of-wine-and-summer-fruits-on-the-beach-royalty-free-image-1635272439.jpg"}]}, "actor_id": "alexey/google-lens"}, "apify/example-selenium": {"id": 952, "url": "https://apify.com/apify/example-selenium/api/client/nodejs", "title": "Apify API and Example Selenium interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.example.com\",\n    \"userAgent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/example-selenium\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.example.com", "proxy": {"useApifyProxy": true}, "userAgent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36"}, "actor_id": "apify/example-selenium"}, "curious_coder/instagram-scraper": {"id": 953, "url": "https://apify.com/curious_coder/instagram-scraper/api/client/nodejs", "title": "Apify API and Instagram Scraper - All in one interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"scrapeProfiles\",\n    \"scrapeProfiles.profileList\": [\n        \"zuck\"\n    ],\n    \"scrapeFriendships.profile\": \"zuck\",\n    \"scrapeFriendships.friendshipType\": \"followers\",\n    \"scrapeCommentsOfPost.url\": \"https://www.instagram.com/p/CwsgvUlLtEn/\",\n    \"scrapeLikesOfPost.url\": \"https://www.instagram.com/p/CwsgvUlLtEn/\",\n    \"scrapePostsOfUser.profile\": \"https://www.instagram.com/p/CwsgvUlLtEn/\",\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [],\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/instagram-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": [], "apifyProxyCountry": "US"}, "action": "scrapeProfiles", "maxDelay": 3, "minDelay": 1, "scrapeLikesOfPost.url": "https://www.instagram.com/p/CwsgvUlLtEn/", "scrapeCommentsOfPost.url": "https://www.instagram.com/p/CwsgvUlLtEn/", "scrapeFriendships.profile": "zuck", "scrapePostsOfUser.profile": "https://www.instagram.com/p/CwsgvUlLtEn/", "scrapeProfiles.profileList": ["zuck"], "scrapeFriendships.friendshipType": "followers"}, "actor_id": "curious_coder/instagram-scraper"}, "jupri/youtube-explorer": {"id": 954, "url": "https://apify.com/jupri/youtube-explorer/api/client/nodejs", "title": "Apify API and Youtube Scraper, Video, Channel, Shorts, Playlists, Comments interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"@Apify\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/youtube-explorer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "@Apify"}, "actor_id": "jupri/youtube-explorer"}, "apify/instagram-hashtag-stats": {"id": 955, "url": "https://apify.com/apify/instagram-hashtag-stats/api/client/nodejs", "title": "Apify API and Instagram Hashtag Stats interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"hashtags\": [\n        \"webscraping\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/instagram-hashtag-stats\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"hashtags": ["webscraping"]}, "actor_id": "apify/instagram-hashtag-stats"}, "alexey/bestbuy-products-actor": {"id": 956, "url": "https://apify.com/alexey/bestbuy-products-actor/api/client/nodejs", "title": "Apify API and BestBuy products interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.bestbuy.com/site/promo/tv-deals\"\n        }\n    ],\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    },\n    \"maxProductsCnt\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"alexey/bestbuy-products-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.bestbuy.com/site/promo/tv-deals"}], "proxyConfig": {"useApifyProxy": true}, "maxProductsCnt": 100}, "actor_id": "alexey/bestbuy-products-actor"}, "parse/google-search": {"id": 957, "url": "https://apify.com/parse/google-search/api/client/nodejs", "title": "Apify API and google-search interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"trump\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"parse/google-search\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "trump"}, "actor_id": "parse/google-search"}, "andrewtaylor/craigslist-scraper": {"id": 958, "url": "https://apify.com/andrewtaylor/craigslist-scraper/api/client/nodejs", "title": "Apify API and Craigslist alerts for emails or SMS, notified for new posts interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search_url\": \"https://sfbay.craigslist.org/search/apa?min_bathrooms=2&min_bedrooms=3&postal=94040#search=1~gallery~0~0\",\n    \"email_addr\": \"example@example.com\",\n    \"email_subj\": \"New Apartment Post!\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"andrewtaylor/craigslist-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"email_addr": "example@example.com", "email_subj": "New Apartment Post!", "search_url": "https://sfbay.craigslist.org/search/apa?min_bathrooms=2&min_bedrooms=3&postal=94040#search=1~gallery~0~0"}, "actor_id": "andrewtaylor/craigslist-scraper"}, "epctex/gutenberg-scraper": {"id": 959, "url": "https://apify.com/epctex/gutenberg-scraper/api/client/nodejs", "title": "Apify API and Free eBook Data Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"book\",\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.gutenberg.org/browse/recent/last7\"\n        },\n        {\n            \"url\": \"https://www.gutenberg.org/browse/titles/h\"\n        }\n    ],\n    \"maxItems\": 5,\n    \"extendOutputFunction\": ($) => { return {} },\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/gutenberg-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"search": "book", "maxItems": 5, "startUrls": [{"url": "https://www.gutenberg.org/browse/recent/last7"}, {"url": "https://www.gutenberg.org/browse/titles/h"}], "proxyConfig": {"useApifyProxy": true}}, "actor_id": "epctex/gutenberg-scraper"}, "manishrc/url-redirect": {"id": 960, "url": "https://apify.com/manishrc/url-redirect/api/client/nodejs", "title": "Apify API and URL Redirects interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"manishrc/url-redirect\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "manishrc/url-redirect"}, "pocesar/json-ld-schema": {"id": 961, "url": "https://apify.com/pocesar/json-ld-schema/api/client/nodejs", "title": "Apify API and LD+JSON Schema scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://blog.apify.com/\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"customData\": {}\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/json-ld-schema\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://blog.apify.com/"}], "customData": {}, "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "pocesar/json-ld-schema"}, "jupri/twitter-scraper": {"id": 962, "url": "https://apify.com/jupri/twitter-scraper/api/client/nodejs", "title": "Apify API and Twitter.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"@apify\",\n    \"limit\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/twitter-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"limit": 5, "query": "@apify"}, "actor_id": "jupri/twitter-scraper"}, "kristoferlund/apify-metascraper": {"id": 963, "url": "https://apify.com/kristoferlund/apify-metascraper/api/client/nodejs", "title": "Apify API and Metascraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"kristoferlund/apify-metascraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "kristoferlund/apify-metascraper"}, "m0uka/steam-store-scraper": {"id": 964, "url": "https://apify.com/m0uka/steam-store-scraper/api/client/nodejs", "title": "Apify API and Steam Store Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"m0uka/steam-store-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "m0uka/steam-store-scraper"}, "wandersonsousa/cnpj-brasil-extrator": {"id": 965, "url": "https://apify.com/wandersonsousa/cnpj-brasil-extrator/api/client/nodejs", "title": "Apify API and Extrator de CNPJ Brasil interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"wandersonsousa/cnpj-brasil-extrator\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "wandersonsousa/cnpj-brasil-extrator"}, "pocesar/download-instagram-video": {"id": 967, "url": "https://apify.com/pocesar/download-instagram-video/api/client/nodejs", "title": "Apify API and Instagram video downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.instagram.com/p/CeosHiNjXwT/\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    },\n    \"maxRequestRetries\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/download-instagram-video\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://www.instagram.com/p/CeosHiNjXwT/"}], "maxRequestRetries": 10}, "actor_id": "pocesar/download-instagram-video"}, "trudax/actor-nordstrom-scraper": {"id": 968, "url": "https://apify.com/trudax/actor-nordstrom-scraper/api/client/nodejs", "title": "Apify API and Nordstrom Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"Jeans\",\n    \"country\": \"United States\",\n    \"maxItems\": 2,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"trudax/actor-nordstrom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "search": "Jeans", "country": "United States", "maxItems": 2}, "actor_id": "trudax/actor-nordstrom-scraper"}, "curious_coder/apollo-scraper": {"id": 969, "url": "https://apify.com/curious_coder/apollo-scraper/api/client/nodejs", "title": "Apify API and Apollo Scraper - Pay per leads interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startPage\": 1,\n    \"minDelay\": 2,\n    \"maxDelay\": 7\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/apollo-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDelay": 7, "minDelay": 2, "startPage": 1}, "actor_id": "curious_coder/apollo-scraper"}, "epctex/ryanair-scraper": {"id": 970, "url": "https://apify.com/epctex/ryanair-scraper/api/client/nodejs", "title": "Apify API and Ryanair Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"ROUND\",\n    \"origin\": \"BER\",\n    \"destination\": \"BCN\",\n    \"departureDate\": \"2023-10-10\",\n    \"returnDate\": \"2023-10-14\",\n    \"flexDays\": 2,\n    \"adults\": 1,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/ryanair-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "ROUND", "proxy": {"useApifyProxy": true}, "adults": 1, "origin": "BER", "flexDays": 2, "returnDate": "2023-10-14", "destination": "BCN", "departureDate": "2023-10-10"}, "actor_id": "epctex/ryanair-scraper"}, "anchor/email-check-verify-validate": {"id": 971, "url": "https://apify.com/anchor/email-check-verify-validate/api/client/nodejs", "title": "Apify API and Email check, verify, validate interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"emails\": [\n        \"elon.musk@tesla.com\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"anchor/email-check-verify-validate\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"emails": ["elon.musk@tesla.com"]}, "actor_id": "anchor/email-check-verify-validate"}, "andrey_bykov/sreality-monitor": {"id": 972, "url": "https://apify.com/andrey_bykov/sreality-monitor/api/client/nodejs", "title": "Apify API and sReality Listings Monitor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"\u0158\u00ed\u010dany u Prahy, Czechia\",\n    \"offerType\": \"sale\",\n    \"type\": \"apartment\",\n    \"maxPages\": 1,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"andrey_bykov/sreality-monitor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"type": "apartment", "location": "\u0158\u00ed\u010dany u Prahy, Czechia", "maxPages": 1, "offerType": "sale", "proxyConfiguration": {"useApifyProxy": false}}, "actor_id": "andrey_bykov/sreality-monitor"}, "jupri/expedia-hotels": {"id": 973, "url": "https://apify.com/jupri/expedia-hotels/api/client/nodejs", "title": "Apify API and Expedia Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"Europe\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/expedia-hotels\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"location": "Europe"}, "actor_id": "jupri/expedia-hotels"}, "jupri/tesco-grocery": {"id": 981, "url": "https://apify.com/jupri/tesco-grocery/api/client/nodejs", "title": "Apify API and Tesco Grocery Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"cake\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/tesco-grocery\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "cake"}, "actor_id": "jupri/tesco-grocery"}, "anchor/leboncoin": {"id": 974, "url": "https://apify.com/anchor/leboncoin/api/client/nodejs", "title": "Apify API and Leboncoin Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.leboncoin.fr/recherche?category=21&text=mer&price=17-50\"\n        }\n    ],\n    \"pageFunction\": async function pageFunction(context) {\n        let data = {}\n        let userData = context.request.userData\n        data.url = context.request.url\n        data.label = userData.label\n    \n        let items = await context.page.evaluate(() => {\n            const item = $('[data-qa-id=aditem_container]')\n            const itemInfo = item.map(function(i,elem) {\n                let obj = {}\n                obj.title = $(this).find('[data-qa-id=aditem_title]').text()\n                obj.price = $(this).find('[data-test-id=price]').text()\n                obj.location = $(this).find('span').filter(function() { return this.title.match(/[0-9]{5}/);}).text()\n                obj.date = $(this).find('span').filter(function() { return this.title.match(/:/);}).text()\n                obj.img = $(this).find('[data-test-id=adcard-consumer-goods-list] img').attr('src')\n                obj.rank = i+1\n                return obj\n            }).get()\n            return itemInfo\n        })\n        let itemsWithDataProp = items.map(obj => { \n            for(const key of Object.keys(data) ){\n                obj[key] = data[key]\n            }\n            return obj\n        })\n        return itemsWithDataProp;\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"FR\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"anchor/leboncoin\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.leboncoin.fr/recherche?category=21&text=mer&price=17-50"}], "proxyConfiguration": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "FR"}}, "actor_id": "anchor/leboncoin"}, "zhangzhenhu/best-twitter-scraper": {"id": 975, "url": "https://apify.com/zhangzhenhu/best-twitter-scraper/api/client/nodejs", "title": "Apify API and Best Twitter Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"login_user\": \"HuZhang18851\",\n    \"login_password\": \"r7pkkp22\",\n    \"names\": [\n        \"elonmusk\"\n    ],\n    \"tweet_urls\": [],\n    \"search_urls\": []\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zhangzhenhu/best-twitter-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"names": ["elonmusk"], "login_user": "HuZhang18851", "tweet_urls": [], "search_urls": [], "login_password": "r7pkkp22"}, "actor_id": "zhangzhenhu/best-twitter-scraper"}, "lukaskrivka/duplications-checker": {"id": 976, "url": "https://apify.com/lukaskrivka/duplications-checker/api/client/nodejs", "title": "Apify API and Duplications Checker interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/duplications-checker\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukaskrivka/duplications-checker"}, "emastra/anti-captcha-geetest": {"id": 977, "url": "https://apify.com/emastra/anti-captcha-geetest/api/client/nodejs", "title": "Apify API and Anti Captcha Geetest interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"emastra/anti-captcha-geetest\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "emastra/anti-captcha-geetest"}, "pocesar/download-tiktok-video": {"id": 978, "url": "https://apify.com/pocesar/download-tiktok-video/api/client/nodejs", "title": "Apify API and Tiktok video downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.tiktok.com/@khaby.lame/video/7211630755884043525\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    },\n    \"maxRequestRetries\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/download-tiktok-video\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://www.tiktok.com/@khaby.lame/video/7211630755884043525"}], "maxRequestRetries": 10}, "actor_id": "pocesar/download-tiktok-video"}, "abe/tiktok-profile-scraper": {"id": 979, "url": "https://apify.com/abe/tiktok-profile-scraper/api/client/nodejs", "title": "Apify API and TikTok Profile Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"usernames\": [\n        \"khaby.lame\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"abe/tiktok-profile-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"usernames": ["khaby.lame"]}, "actor_id": "abe/tiktok-profile-scraper"}, "curious_coder/twitter-scraper": {"id": 980, "url": "https://apify.com/curious_coder/twitter-scraper/api/client/nodejs", "title": "Apify API and Twitter followers scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"profileUrl\": \"https://twitter.com/elonmusk\",\n    \"friendshipType\": \"followers\",\n    \"count\": 100,\n    \"minDelay\": 3,\n    \"maxDelay\": 15\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/twitter-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"count": 100, "maxDelay": 15, "minDelay": 3, "profileUrl": "https://twitter.com/elonmusk", "friendshipType": "followers"}, "actor_id": "curious_coder/twitter-scraper"}, "mcdowell/amazon-reviews-scraper": {"id": 982, "url": "https://apify.com/mcdowell/amazon-reviews-scraper/api/client/nodejs", "title": "Apify API and Amazon Reviews Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"productURLs\": [\n        {\n            \"url\": \"https://www.amazon.com/iPhone-Pro-128GB-Sierra-Blue/dp/B0BGYG5GSJ\"\n        }\n    ],\n    \"maxReviews\": 20,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mcdowell/amazon-reviews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxReviews": 20, "productURLs": [{"url": "https://www.amazon.com/iPhone-Pro-128GB-Sierra-Blue/dp/B0BGYG5GSJ"}], "proxyConfiguration": {"useApifyProxy": true, "apifyProxyCountry": "US"}}, "actor_id": "mcdowell/amazon-reviews-scraper"}, "katerinahronik/covid-philippines": {"id": 983, "url": "https://apify.com/katerinahronik/covid-philippines/api/client/nodejs", "title": "Apify API and Coronavirus stats in Philippines interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"katerinahronik/covid-philippines\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "katerinahronik/covid-philippines"}, "novi/fast-tiktok-api": {"id": 984, "url": "https://apify.com/novi/fast-tiktok-api/api/client/nodejs", "title": "Apify API and Fast TikTok API, Unmatched speed and stability scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"region\": \"US\",\n    \"urls\": [\n        \"https://www.tiktok.com/@seuamigopitbull/video/7137451371405561093\"\n    ],\n    \"limit\": 20,\n    \"sortType\": 0,\n    \"publishTime\": \"ALL_TIME\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"novi/fast-tiktok-api\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": ["https://www.tiktok.com/@seuamigopitbull/video/7137451371405561093"], "limit": 20, "region": "US", "sortType": 0, "publishTime": "ALL_TIME", "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "novi/fast-tiktok-api"}, "molly/amazon-reviews": {"id": 985, "url": "https://apify.com/molly/amazon-reviews/api/client/nodejs", "title": "Apify API and Amazon Reviews Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"asin\": \"\",\n    \"limitPages\": 2,\n    \"URL\": \"\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"molly/amazon-reviews\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"URL": "", "asin": "", "limitPages": 2}, "actor_id": "molly/amazon-reviews"}, "jupri/skyscanner-flight": {"id": 986, "url": "https://apify.com/jupri/skyscanner-flight/api/client/nodejs", "title": "Apify API and Skyscanner Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"origin.0\": \"Jakarta\",\n    \"target.0\": \"Bandung\",\n    \"depart.0\": \"TOMORROW\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/skyscanner-flight\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"depart.0": "TOMORROW", "origin.0": "Jakarta", "target.0": "Bandung"}, "actor_id": "jupri/skyscanner-flight"}, "curious_coder/linkedin-company-scraper": {"id": 987, "url": "https://apify.com/curious_coder/linkedin-company-scraper/api/client/nodejs", "title": "Apify API and Linkedin company scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urls\": [\n        \"https://www.linkedin.com/company/microsoft\",\n        \"https://www.linkedin.com/company/76987811\"\n    ],\n    \"minDelay\": 2,\n    \"maxDelay\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/linkedin-company-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": ["https://www.linkedin.com/company/microsoft", "https://www.linkedin.com/company/76987811"], "maxDelay": 5, "minDelay": 2}, "actor_id": "curious_coder/linkedin-company-scraper"}, "lukass/redfin-scraper": {"id": 988, "url": "https://apify.com/lukass/redfin-scraper/api/client/nodejs", "title": "Apify API and Redfin crawler interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"Philadelphia\",\n    \"maxItems\": 20,\n    \"category\": \"forSale\",\n    \"startUrls\": [],\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukass/redfin-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"category": "forSale", "location": "Philadelphia", "maxItems": 20, "startUrls": [], "proxyConfig": {"useApifyProxy": true}}, "actor_id": "lukass/redfin-scraper"}, "pocesar/covid-brazil": {"id": 989, "url": "https://apify.com/pocesar/covid-brazil/api/client/nodejs", "title": "Apify API and Coronavirus stats in Brazil interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/covid-brazil\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "pocesar/covid-brazil"}, "jupri/amazon-explorer": {"id": 990, "url": "https://apify.com/jupri/amazon-explorer/api/client/nodejs", "title": "Apify API and Amazon Product ASIN Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"B013OVBYZG B00ZSI7Y3U B09S8G8RPK B08G8TDCJ2\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/amazon-explorer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "B013OVBYZG B00ZSI7Y3U B09S8G8RPK B08G8TDCJ2"}, "actor_id": "jupri/amazon-explorer"}, "curious_coder/facebook-profile-scraper": {"id": 991, "url": "https://apify.com/curious_coder/facebook-profile-scraper/api/client/nodejs", "title": "Apify API and Facebook profile and page scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/facebook-profile-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDelay": 3, "minDelay": 1}, "actor_id": "curious_coder/facebook-profile-scraper"}, "hamza.alwan/contact-bundle": {"id": 992, "url": "https://apify.com/hamza.alwan/contact-bundle/api/client/nodejs", "title": "Apify API and Contact Details Scraper Bundler interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"pathToUrl\": \"url\",\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/contact-bundle\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"pathToUrl": "url", "proxyConfig": {"useApifyProxy": true}}, "actor_id": "hamza.alwan/contact-bundle"}, "kilerhg/wikipedia": {"id": 993, "url": "https://apify.com/kilerhg/wikipedia/api/client/nodejs", "title": "Apify API and Wikipedia Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"input_value\": [\n        \"example value\",\n        \"https://en.wikipedia.org/wiki/Example\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"kilerhg/wikipedia\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"input_value": ["example value", "https://en.wikipedia.org/wiki/Example"]}, "actor_id": "kilerhg/wikipedia"}, "vaclavrut/bigjson2csv": {"id": 994, "url": "https://apify.com/vaclavrut/bigjson2csv/api/client/nodejs", "title": "Apify API and JSON to CSV Converter interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"vaclavrut/bigjson2csv\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "vaclavrut/bigjson2csv"}, "curious_coder/linkedin-post-search-scraper": {"id": 995, "url": "https://apify.com/curious_coder/linkedin-post-search-scraper/api/client/nodejs", "title": "Apify API and Linkedin post scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchUrl\": \"\",\n    \"startPage\": 1,\n    \"minDelay\": 2,\n    \"maxDelay\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/linkedin-post-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDelay": 5, "minDelay": 2, "searchUrl": "", "startPage": 1}, "actor_id": "curious_coder/linkedin-post-search-scraper"}, "lhotanok/allegro-scraper": {"id": 996, "url": "https://apify.com/lhotanok/allegro-scraper/api/client/nodejs", "title": "Apify API and Allegro Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://allegro.pl/oferta/procesor-intel-i9-12900k-16-x-3-2-ghz-12375564256\",\n        \"https://allegro.pl/dzial/moda\",\n        \"https://allegro.pl/kategoria/komputery\",\n        \"https://allegro.pl/magazyn-allegro\",\n        \"https://allegro.pl/uzytkownik/www_comtrade_pl\"\n    ],\n    \"maxProducts\": 30,\n    \"maxReviews\": 15,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lhotanok/allegro-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": ["https://allegro.pl/oferta/procesor-intel-i9-12900k-16-x-3-2-ghz-12375564256", "https://allegro.pl/dzial/moda", "https://allegro.pl/kategoria/komputery", "https://allegro.pl/magazyn-allegro", "https://allegro.pl/uzytkownik/www_comtrade_pl"], "maxReviews": 15, "maxProducts": 30, "proxyConfiguration": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}}, "actor_id": "lhotanok/allegro-scraper"}, "newbs/youtube-channel": {"id": 997, "url": "https://apify.com/newbs/youtube-channel/api/client/nodejs", "title": "Apify API and Advanced YouTube Data Scraper Tool \u00b7 Apify interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"channel\": \"netflix\",\n    \"numberOfResults\": 2\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"newbs/youtube-channel\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"channel": "netflix", "numberOfResults": 2}, "actor_id": "newbs/youtube-channel"}, "apify/monitoring-runner": {"id": 998, "url": "https://apify.com/apify/monitoring-runner/api/client/nodejs", "title": "Apify API and Monitoring Runner interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"limit\": 1000,\n    \"checkersInput\": {\n        \"RESOURCE_STATS_CHECKER\": {},\n        \"DEDUPLICATION_CHECKER\": {\n            \"uniqueKey\": \"email\"\n        },\n        \"SCHEMA_VALIDATOR_CHECKER\": {\n            \"options\": [\n                {\n                    \"resourceList\": [\n                        \"id\",\n                        \"id\"\n                    ],\n                    \"resourceRegex\": \"\",\n                    \"minItemCount\": 100,\n                    \"maxItemCount\": 5000,\n                    \"validationSchema\": \"{ address: String, open: Boolean }\"\n                }\n            ]\n        }\n    },\n    \"reportersInput\": {\n        \"EMAIL_REPORTER\": {\n            \"sendMailInput\": {\n                \"to\": \"info@apify.com\",\n                \"subject\": \"My monitoring task report\"\n            }\n        },\n        \"SLACK_REPORTER\": {\n            \"slackInput\": {\n                \"token\": \"Your token\",\n                \"message\": \"Hey, look what I have done!\",\n                \"channel\": \"#monitoring\"\n            }\n        },\n        \"DASHBOARD_REPORTER\": {\n            \"notifyOnUpdate\": true\n        }\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/monitoring-runner\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"limit": 1000, "checkersInput": {"DEDUPLICATION_CHECKER": {"uniqueKey": "email"}, "RESOURCE_STATS_CHECKER": {}, "SCHEMA_VALIDATOR_CHECKER": {"options": [{"maxItemCount": 5000, "minItemCount": 100, "resourceList": ["id", "id"], "resourceRegex": "", "validationSchema": "{ address: String, open: Boolean }"}]}}, "reportersInput": {"EMAIL_REPORTER": {"sendMailInput": {"to": "info@apify.com", "subject": "My monitoring task report"}}, "SLACK_REPORTER": {"slackInput": {"token": "Your token", "channel": "#monitoring", "message": "Hey, look what I have done!"}}, "DASHBOARD_REPORTER": {"notifyOnUpdate": true}}}, "actor_id": "apify/monitoring-runner"}, "dtrungtin/github-users-scraper": {"id": 999, "url": "https://apify.com/dtrungtin/github-users-scraper/api/client/nodejs", "title": "Apify API and github users interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://github.com/dtrungtin/actor-allrecipes-scraper\"\n        }\n    ],\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/github-users-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://github.com/dtrungtin/actor-allrecipes-scraper"}], "proxyConfig": {"useApifyProxy": true}}, "actor_id": "dtrungtin/github-users-scraper"}, "curious_coder/facebook-group-member-scraper": {"id": 1000, "url": "https://apify.com/curious_coder/facebook-group-member-scraper/api/client/nodejs", "title": "Apify API and Facebook group member scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/facebook-group-member-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDelay": 3, "minDelay": 1}, "actor_id": "curious_coder/facebook-group-member-scraper"}, "lukaskrivka/website-checker-cheerio": {"id": 1001, "url": "https://apify.com/lukaskrivka/website-checker-cheerio/api/client/nodejs", "title": "Apify API and Website Checker Runner Cheerio interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urlsToCheck\": [\n        {\n            \"url\": \"https://www.amazon.com/b?ie=UTF8&node=11392907011\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": false\n    },\n    \"linkSelector\": \"a[href]\",\n    \"pseudoUrls\": [\n        {\n            \"purl\": \"https://www.amazon.com[.*]/dp/[.*]\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/website-checker-cheerio\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"pseudoUrls": [{"purl": "https://www.amazon.com[.*]/dp/[.*]"}], "urlsToCheck": [{"url": "https://www.amazon.com/b?ie=UTF8&node=11392907011"}], "linkSelector": "a[href]", "proxyConfiguration": {"useApifyProxy": false}}, "actor_id": "lukaskrivka/website-checker-cheerio"}, "lexis-solutions/meta-threads-replies-scraper": {"id": 1002, "url": "https://apify.com/lexis-solutions/meta-threads-replies-scraper/api/client/nodejs", "title": "Apify API and Meta Threads & Replies Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"usernames\": [\n        \"zuck\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lexis-solutions/meta-threads-replies-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"usernames": ["zuck"]}, "actor_id": "lexis-solutions/meta-threads-replies-scraper"}, "zuzka/covid-my": {"id": 1003, "url": "https://apify.com/zuzka/covid-my/api/client/nodejs", "title": "Apify API and Coronavirus stats in Malaysia interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"email\": \"zuzka@apify.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/covid-my\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"email": "zuzka@apify.com"}, "actor_id": "zuzka/covid-my"}, "juansgaitan/link-extractor": {"id": 1004, "url": "https://apify.com/juansgaitan/link-extractor/api/client/nodejs", "title": "Apify API and Link Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"juansgaitan/link-extractor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "juansgaitan/link-extractor"}, "mcdowell/yellow-pages": {"id": 1005, "url": "https://apify.com/mcdowell/yellow-pages/api/client/nodejs", "title": "Apify API and Yellow Pages interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"Mechanic\",\n    \"location\": \"New York\",\n    \"maxItems\": 100,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mcdowell/yellow-pages\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"search": "Mechanic", "location": "New York", "maxItems": 100, "proxyConfiguration": {"useApifyProxy": false}}, "actor_id": "mcdowell/yellow-pages"}, "jupri/asda-scraper": {"id": 1006, "url": "https://apify.com/jupri/asda-scraper/api/client/nodejs", "title": "Apify API and Asda.com Grocery Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"sausage\",\n    \"limit\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/asda-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"limit": 5, "query": "sausage"}, "actor_id": "jupri/asda-scraper"}, "lukaskrivka/download-image": {"id": 1007, "url": "https://apify.com/lukaskrivka/download-image/api/client/nodejs", "title": "Apify API and Example Image Download interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/download-image\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukaskrivka/download-image"}, "eneiromatos/google-jobs-scraper": {"id": 1008, "url": "https://apify.com/eneiromatos/google-jobs-scraper/api/client/nodejs", "title": "Apify API and Google Jobs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"keyword\": \"Cloud Data Engineer\",\n    \"location\": \"California\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"eneiromatos/google-jobs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"keyword": "Cloud Data Engineer", "location": "California"}, "actor_id": "eneiromatos/google-jobs-scraper"}, "ivanvs/craigslist-scraper": {"id": 1009, "url": "https://apify.com/ivanvs/craigslist-scraper/api/client/nodejs", "title": "Apify API and Craigslist Ad Scraper, Craigslist API, Craigslist crawler interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://newyork.craigslist.org/search/fct/rrr#search=1~list~0~0\",\n    \"urls\": [],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"maxConcurrency\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"ivanvs/craigslist-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://newyork.craigslist.org/search/fct/rrr#search=1~list~0~0", "urls": [], "maxConcurrency": 1, "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "ivanvs/craigslist-scraper"}, "tcz/youtube-search-term-channel-crawler": {"id": 1010, "url": "https://apify.com/tcz/youtube-search-term-channel-crawler/api/client/nodejs", "title": "Apify API and YouTube Search Term Channel Crawler interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"keywords\": [\n        \"dog grooming\"\n    ],\n    \"minVideosPerKeywordLimit\": 25\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"tcz/youtube-search-term-channel-crawler\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"keywords": ["dog grooming"], "minVideosPerKeywordLimit": 25}, "actor_id": "tcz/youtube-search-term-channel-crawler"}, "novi/tiktok-trend-api": {"id": 1018, "url": "https://apify.com/novi/tiktok-trend-api/api/client/nodejs", "title": "Apify API and Tiktok Trend API, Harness the power of TikTok trends interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"novi/tiktok-trend-api\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "novi/tiktok-trend-api"}, "sandboxcmd/lazada-scraper": {"id": 1011, "url": "https://apify.com/sandboxcmd/lazada-scraper/api/client/nodejs", "title": "Apify API and Lazada Product Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"categoryUrls\": [\n        {\n            \"url\": \"https://www.lazada.co.id/gaun-wanita/?spm=a2o4j.home.categories.1.57997838Vw6eh0&up_id=2690052796&clickTrackInfo=378a8db0-4583-4567-ac9c-293c1b07eb2c__4962__2690052796__static__0.09986028971706916__158075__7253&from=hp_categories&item_id=2690052796&version=v2\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"sandboxcmd/lazada-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"categoryUrls": [{"url": "https://www.lazada.co.id/gaun-wanita/?spm=a2o4j.home.categories.1.57997838Vw6eh0&up_id=2690052796&clickTrackInfo=378a8db0-4583-4567-ac9c-293c1b07eb2c__4962__2690052796__static__0.09986028971706916__158075__7253&from=hp_categories&item_id=2690052796&version=v2"}], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "sandboxcmd/lazada-scraper"}, "mpj/asos": {"id": 1012, "url": "https://apify.com/mpj/asos/api/client/nodejs", "title": "Apify API and ASOS Product Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mpj/asos\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "mpj/asos"}, "adrian_horning/best-reddit-scraper": {"id": 1013, "url": "https://apify.com/adrian_horning/best-reddit-scraper/api/client/nodejs", "title": "Apify API and Best Reddit Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"redditPostURL\": \"https://www.reddit.com/r/webscraping/comments/13ni1fq/noob_question_about_reddit_webscraping/\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"adrian_horning/best-reddit-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"redditPostURL": "https://www.reddit.com/r/webscraping/comments/13ni1fq/noob_question_about_reddit_webscraping/"}, "actor_id": "adrian_horning/best-reddit-scraper"}, "curious_coder/linkedin-people-search-scraper": {"id": 1014, "url": "https://apify.com/curious_coder/linkedin-people-search-scraper/api/client/nodejs", "title": "Apify API and Linkedin people search scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startPage\": 1,\n    \"minDelay\": 2,\n    \"maxDelay\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/linkedin-people-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDelay": 5, "minDelay": 2, "startPage": 1}, "actor_id": "curious_coder/linkedin-people-search-scraper"}, "jupri/vrbo-property": {"id": 1015, "url": "https://apify.com/jupri/vrbo-property/api/client/nodejs", "title": "Apify API and VRBO Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"New York\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/vrbo-property\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"location": "New York"}, "actor_id": "jupri/vrbo-property"}, "lukaskrivka/website-checker-puppeteer": {"id": 1016, "url": "https://apify.com/lukaskrivka/website-checker-puppeteer/api/client/nodejs", "title": "Apify API and Website Checker Runner Puppeteer interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urlsToCheck\": [\n        {\n            \"url\": \"https://www.amazon.com/b?ie=UTF8&node=11392907011\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": false\n    },\n    \"linkSelector\": \"a[href]\",\n    \"pseudoUrls\": [\n        {\n            \"purl\": \"https://www.amazon.com[.*]/dp/[.*]\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/website-checker-puppeteer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"pseudoUrls": [{"purl": "https://www.amazon.com[.*]/dp/[.*]"}], "urlsToCheck": [{"url": "https://www.amazon.com/b?ie=UTF8&node=11392907011"}], "linkSelector": "a[href]", "proxyConfiguration": {"useApifyProxy": false}}, "actor_id": "lukaskrivka/website-checker-puppeteer"}, "yeyo/trendyol-scraper": {"id": 1017, "url": "https://apify.com/yeyo/trendyol-scraper/api/client/nodejs", "title": "Apify API and Trendyol Product Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"input\": [\n        {\n            \"url\": \"https://www.trendyol.com/apple/iphone-11-128-gb-beyaz-cep-telefonu-aksesuarsiz-kutu-apple-turkiye-garantili-p-64074794\"\n        },\n        {\n            \"url\": \"https://www.trendyol.com/sr?q=iphone\"\n        }\n    ],\n    \"maxItems\": 10,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"yeyo/trendyol-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"input": [{"url": "https://www.trendyol.com/apple/iphone-11-128-gb-beyaz-cep-telefonu-aksesuarsiz-kutu-apple-turkiye-garantili-p-64074794"}, {"url": "https://www.trendyol.com/sr?q=iphone"}], "maxItems": 10, "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "yeyo/trendyol-scraper"}, "katerinahronik/heureka-scraper": {"id": 1019, "url": "https://apify.com/katerinahronik/heureka-scraper/api/client/nodejs", "title": "Apify API and Heureka Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": \"https://pletova-kosmetika.heureka.cz/, https://telova-kosmetika.heureka.cz/\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"katerinahronik/heureka-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": "https://pletova-kosmetika.heureka.cz/, https://telova-kosmetika.heureka.cz/"}, "actor_id": "katerinahronik/heureka-scraper"}, "epctex/edx-scraper": {"id": 1020, "url": "https://apify.com/epctex/edx-scraper/api/client/nodejs", "title": "Apify API and edX Online Course Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxItems\": 10,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/edx-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "maxItems": 10}, "actor_id": "epctex/edx-scraper"}, "runtime_terror/google-search-reviews-scraper": {"id": 1021, "url": "https://apify.com/runtime_terror/google-search-reviews-scraper/api/client/nodejs", "title": "Apify API and Google Search Reviews Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search_queries\": [\n        \"hale education carby st westwood\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"runtime_terror/google-search-reviews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"search_queries": ["hale education carby st westwood"]}, "actor_id": "runtime_terror/google-search-reviews-scraper"}, "big-brain.io/upwork-application": {"id": 1022, "url": "https://apify.com/big-brain.io/upwork-application/api/client/nodejs", "title": "Apify API and Upwork Job Auto Apply interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"defaultAnswer\": \"Let's get on a call\",\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    },\n    \"attachments\": []\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"big-brain.io/upwork-application\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"attachments": [], "proxyConfig": {"useApifyProxy": true}, "defaultAnswer": "Let's get on a call"}, "actor_id": "big-brain.io/upwork-application"}, "mvolfik/expedia-hotels-com-reviews-scraper": {"id": 1023, "url": "https://apify.com/mvolfik/expedia-hotels-com-reviews-scraper/api/client/nodejs", "title": "Apify API and Expedia & Hotels.com reviews scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.expedia.com/Prague-Hotels-Hotel-Krystal.h10966026.Hotel-Information\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mvolfik/expedia-hotels-com-reviews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.expedia.com/Prague-Hotels-Hotel-Krystal.h10966026.Hotel-Information"}]}, "actor_id": "mvolfik/expedia-hotels-com-reviews-scraper"}, "newpo/tiktok-super-efficient-scraper": {"id": 1024, "url": "https://apify.com/newpo/tiktok-super-efficient-scraper/api/client/nodejs", "title": "Apify API and Tiktok Scraper: 30x Performance Improvement interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"hashtags\": [\n        \"petsoftiktok\"\n    ],\n    \"users\": [\n        \"@gordonramsayofficial\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"newpo/tiktok-super-efficient-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "users": ["@gordonramsayofficial"], "hashtags": ["petsoftiktok"]}, "actor_id": "newpo/tiktok-super-efficient-scraper"}, "apify/image-diff": {"id": 1025, "url": "https://apify.com/apify/image-diff/api/client/nodejs", "title": "Apify API and Image Difference Generator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"imageUrl1\": \"https://raw.githubusercontent.com/mapbox/pixelmatch/master/test/fixtures/4a.png\",\n    \"imageUrl2\": \"https://raw.githubusercontent.com/mapbox/pixelmatch/master/test/fixtures/4b.png\",\n    \"pixelmatchOptions\": {\n        \"threshold\": 0.1\n    },\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/image-diff\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "imageUrl1": "https://raw.githubusercontent.com/mapbox/pixelmatch/master/test/fixtures/4a.png", "imageUrl2": "https://raw.githubusercontent.com/mapbox/pixelmatch/master/test/fixtures/4b.png", "pixelmatchOptions": {"threshold": 0.1}}, "actor_id": "apify/image-diff"}, "autofacts/sephora": {"id": 1027, "url": "https://apify.com/autofacts/sephora/api/client/nodejs", "title": "Apify API and Sephora Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.sephora.com/shop/eyeshadow-palettes\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    },\n    \"maxConcurrency\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"autofacts/sephora\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://www.sephora.com/shop/eyeshadow-palettes"}], "maxConcurrency": 5}, "actor_id": "autofacts/sephora"}, "jupri/yellowpages-au": {"id": 1028, "url": "https://apify.com/jupri/yellowpages-au/api/client/nodejs", "title": "Apify API and YellowPages AU Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"keywords\": \"restaurants\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/yellowpages-au\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"keywords": "restaurants"}, "actor_id": "jupri/yellowpages-au"}, "alexey/google-maps-radar-search": {"id": 1029, "url": "https://apify.com/alexey/google-maps-radar-search/api/client/nodejs", "title": "Apify API and Google Places API Radar Search interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"radiusMeters\": 1000,\n    \"minRadiusMeters\": 50,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"alexey/google-maps-radar-search\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "radiusMeters": 1000, "minRadiusMeters": 50}, "actor_id": "alexey/google-maps-radar-search"}, "johndon/generic-lectio-scraper": {"id": 1030, "url": "https://apify.com/johndon/generic-lectio-scraper/api/client/nodejs", "title": "Apify API and Lectio skema til kalender interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"johndon/generic-lectio-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "johndon/generic-lectio-scraper"}, "adrian_horning/best-zillow-scraper": {"id": 1031, "url": "https://apify.com/adrian_horning/best-zillow-scraper/api/client/nodejs", "title": "Apify API and Best Zillow Search Results Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"Austin\",\n    \"status\": \"sale\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"adrian_horning/best-zillow-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"search": "Austin", "status": "sale"}, "actor_id": "adrian_horning/best-zillow-scraper"}, "danielmilevski9/linkedin-company-jobs-finder": {"id": 1032, "url": "https://apify.com/danielmilevski9/linkedin-company-jobs-finder/api/client/nodejs", "title": "Apify API and LinkedIn companies and jobs finder interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchList\": [\n        \"Data\"\n    ],\n    \"resultsLimit\": 10,\n    \"location\": \"United Kingdom\",\n    \"resultsType\": \"job\",\n    \"f_E\": \"company\",\n    \"f_JT\": \"company\",\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"danielmilevski9/linkedin-company-jobs-finder\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"f_E": "company", "f_JT": "company", "proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "location": "United Kingdom", "searchList": ["Data"], "resultsType": "job", "resultsLimit": 10}, "actor_id": "danielmilevski9/linkedin-company-jobs-finder"}, "adrian_horning/best-linkedin-jobs-scraper": {"id": 1033, "url": "https://apify.com/adrian_horning/best-linkedin-jobs-scraper/api/client/nodejs", "title": "Apify API and Best Linkedin Jobs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"Data Engineer\",\n    \"location\": \"Chicago, Illinois\",\n    \"datePosted\": \"today\",\n    \"distance\": \"25\",\n    \"salary\": \"40\",\n    \"remote\": \"remote\",\n    \"jobType\": \"full_time\",\n    \"experienceLevel\": \"associate\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"adrian_horning/best-linkedin-jobs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "Data Engineer", "remote": "remote", "salary": "40", "jobType": "full_time", "distance": "25", "location": "Chicago, Illinois", "datePosted": "today", "experienceLevel": "associate"}, "actor_id": "adrian_horning/best-linkedin-jobs-scraper"}, "jupri/indiegogo": {"id": 1180, "url": "https://apify.com/jupri/indiegogo/api/client/nodejs", "title": "Apify API and Indiegogo.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/indiegogo\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jupri/indiegogo"}, "apify/monitoring-checker-run-status": {"id": 1034, "url": "https://apify.com/apify/monitoring-checker-run-status/api/client/nodejs", "title": "Apify API and Monitoring Checker Run Status interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/monitoring-checker-run-status\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/monitoring-checker-run-status"}, "ivanvs/remoteok-job-scraper": {"id": 1035, "url": "https://apify.com/ivanvs/remoteok-job-scraper/api/client/nodejs", "title": "Apify API and RemoteOK Job Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"tag\": \"\",\n    \"maxNumberOfListings\": 50,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"maxConcurrency\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"ivanvs/remoteok-job-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"tag": "", "maxConcurrency": 1, "proxyConfiguration": {"useApifyProxy": true}, "maxNumberOfListings": 50}, "actor_id": "ivanvs/remoteok-job-scraper"}, "sergeylukin/steam-puppeteer": {"id": 1036, "url": "https://apify.com/sergeylukin/steam-puppeteer/api/client/nodejs", "title": "Apify API and Steam Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"sergeylukin/steam-puppeteer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "sergeylukin/steam-puppeteer"}, "dtrungtin/dnb-companies-scraper": {"id": 1037, "url": "https://apify.com/dtrungtin/dnb-companies-scraper/api/client/nodejs", "title": "Apify API and DNB Companies Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchTerm\": \"Apple\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/dnb-companies-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"searchTerm": "Apple", "proxyConfiguration": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}}, "actor_id": "dtrungtin/dnb-companies-scraper"}, "newpo/fiverr-scraper": {"id": 1038, "url": "https://apify.com/newpo/fiverr-scraper/api/client/nodejs", "title": "Apify API and Fiverr Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.fiverr.com/categories/graphics-design/website-design?source=hplo_search_tag&pos=1&name=website-design\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"newpo/fiverr-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://www.fiverr.com/categories/graphics-design/website-design?source=hplo_search_tag&pos=1&name=website-design"}]}, "actor_id": "newpo/fiverr-scraper"}, "curious_coder/indeed-scraper": {"id": 1039, "url": "https://apify.com/curious_coder/indeed-scraper/api/client/nodejs", "title": "Apify API and Indeed Jobs Scraper \u2705 Updated: 17 Oct \u2705 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"scrapeJobs\",\n    \"scrapeJobs.searchUrl\": \"https://www.indeed.com/jobs?q=sales+associate+part+time&l=New+York%2C+NY&vjk=b28e7b80d0399215\",\n    \"startPage\": 1,\n    \"count\": 10,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/indeed-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"count": 10, "proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "action": "scrapeJobs", "startPage": 1, "scrapeJobs.searchUrl": "https://www.indeed.com/jobs?q=sales+associate+part+time&l=New+York%2C+NY&vjk=b28e7b80d0399215"}, "actor_id": "curious_coder/indeed-scraper"}, "autofacts/farfetch": {"id": 1040, "url": "https://apify.com/autofacts/farfetch/api/client/nodejs", "title": "Apify API and Farfetch Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.farfetch.com/cz/shopping/women/mules-1/items.aspx?page=1&view=90&sort=3&scale=274&colour=13\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    },\n    \"maxConcurrency\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"autofacts/farfetch\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://www.farfetch.com/cz/shopping/women/mules-1/items.aspx?page=1&view=90&sort=3&scale=274&colour=13"}], "maxConcurrency": 5}, "actor_id": "autofacts/farfetch"}, "logie/shopify-products-scraper": {"id": 1041, "url": "https://apify.com/logie/shopify-products-scraper/api/client/nodejs", "title": "Apify API and Shopify Products Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.gymshark.com\",\n    \"Proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"logie/shopify-products-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.gymshark.com", "Proxy": {"useApifyProxy": true}}, "actor_id": "logie/shopify-products-scraper"}, "theo/new-york-times-scraper": {"id": 1042, "url": "https://apify.com/theo/new-york-times-scraper/api/client/nodejs", "title": "Apify API and New York Times Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.nytimes.com/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"theo/new-york-times-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.nytimes.com/"}], "maxArticlesPerCrawl": 100}, "actor_id": "theo/new-york-times-scraper"}, "jupri/patreon": {"id": 1043, "url": "https://apify.com/jupri/patreon/api/client/nodejs", "title": "Apify API and Patreon.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"creator\": \"The Best Show\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/patreon\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"creator": "The Best Show"}, "actor_id": "jupri/patreon"}, "valehprague/image-to-text": {"id": 1044, "url": "https://apify.com/valehprague/image-to-text/api/client/nodejs", "title": "Apify API and OCR Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"input_image\": \"https://images4.programmersought.com/934/e8/e89758ae0ed991f1c8aba947addec9e6.png\",\n    \"lang\": \"en\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"valehprague/image-to-text\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"lang": "en", "input_image": "https://images4.programmersought.com/934/e8/e89758ae0ed991f1c8aba947addec9e6.png"}, "actor_id": "valehprague/image-to-text"}, "curious_coder/crunchbase-scraper": {"id": 1045, "url": "https://apify.com/curious_coder/crunchbase-scraper/api/client/nodejs", "title": "Apify API and Crunchbase pro search scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"search\",\n    \"search.url\": \"https://www.crunchbase.com/discover/organization.companies/e2b9ad6d513ac5d0e7965bd16bd02327\",\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/crunchbase-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"action": "search", "maxDelay": 3, "minDelay": 1, "search.url": "https://www.crunchbase.com/discover/organization.companies/e2b9ad6d513ac5d0e7965bd16bd02327"}, "actor_id": "curious_coder/crunchbase-scraper"}, "mtnfranke/daft-ie-scraper": {"id": 1046, "url": "https://apify.com/mtnfranke/daft-ie-scraper/api/client/nodejs", "title": "Apify API and Daft.ie Scraper \ud83c\udfe1 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"proxy\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mtnfranke/daft-ie-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": false}}, "actor_id": "mtnfranke/daft-ie-scraper"}, "lukass/idealista-scraper": {"id": 1047, "url": "https://apify.com/lukass/idealista-scraper/api/client/nodejs", "title": "Apify API and Idealist.com interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"district\": \"Madrid\",\n    \"maxItems\": 100,\n    \"endPage\": 50,\n    \"startUrl\": [],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukass/idealista-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "endPage": 50, "district": "Madrid", "maxItems": 100, "startUrl": []}, "actor_id": "lukass/idealista-scraper"}, "apify/monitoring-teardown": {"id": 1055, "url": "https://apify.com/apify/monitoring-teardown/api/client/nodejs", "title": "Apify API and Monitoring Teardown interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/monitoring-teardown\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/monitoring-teardown"}, "onidivo/trulia-scraper": {"id": 1048, "url": "https://apify.com/onidivo/trulia-scraper/api/client/nodejs", "title": "Apify API and Trulia Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.trulia.com/for_sale/San_Francisco,CA/3p_beds/300000p_price/MULTI-FAMILY,SINGLE-FAMILY_HOME,TOWNHOUSE_type/7_nl/\"\n        },\n        {\n            \"url\": \"https://www.trulia.com/property/7095741518-37901-Ocean-Ridge-Dr-Gualala-CA-95445\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"onidivo/trulia-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.trulia.com/for_sale/San_Francisco,CA/3p_beds/300000p_price/MULTI-FAMILY,SINGLE-FAMILY_HOME,TOWNHOUSE_type/7_nl/"}, {"url": "https://www.trulia.com/property/7095741518-37901-Ocean-Ridge-Dr-Gualala-CA-95445"}], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "onidivo/trulia-scraper"}, "ensembledata/tiktok-api": {"id": 1049, "url": "https://apify.com/ensembledata/tiktok-api/api/client/nodejs", "title": "Apify API and Tiktok API - EnsembleData interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"post_info\": [\n        \"https://www.tiktok.com/@animalrescue243/video/7206488344535125290\",\n        \"https://www.tiktok.com/@evitastork/video/7197406477001198854\"\n    ],\n    \"post_comments\": [\n        \"https://www.tiktok.com/@animalrescue243/video/7206488344535125290\",\n        \"https://www.tiktok.com/@evitastork/video/7197406477001198854\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"ensembledata/tiktok-api\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"post_info": ["https://www.tiktok.com/@animalrescue243/video/7206488344535125290", "https://www.tiktok.com/@evitastork/video/7197406477001198854"], "post_comments": ["https://www.tiktok.com/@animalrescue243/video/7206488344535125290", "https://www.tiktok.com/@evitastork/video/7197406477001198854"]}, "actor_id": "ensembledata/tiktok-api"}, "zuzka/covid-wm": {"id": 1050, "url": "https://apify.com/zuzka/covid-wm/api/client/nodejs", "title": "Apify API and Coronavirus stats from Worldometer interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"email\": \"zuzka@apify.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/covid-wm\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"email": "zuzka@apify.com"}, "actor_id": "zuzka/covid-wm"}, "petr_cermak/anti-captcha-image": {"id": 1051, "url": "https://apify.com/petr_cermak/anti-captcha-image/api/client/nodejs", "title": "Apify API and Anti Captcha Image interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/anti-captcha-image\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petr_cermak/anti-captcha-image"}, "curious_coder/twitter-replies-scraper": {"id": 1052, "url": "https://apify.com/curious_coder/twitter-replies-scraper/api/client/nodejs", "title": "Apify API and Twitter comments scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"tweetUrl\": \"https://twitter.com/elonmusk/status/1692435700275404853\",\n    \"minDelay\": 2,\n    \"maxDelay\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/twitter-replies-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDelay": 5, "minDelay": 2, "tweetUrl": "https://twitter.com/elonmusk/status/1692435700275404853"}, "actor_id": "curious_coder/twitter-replies-scraper"}, "juansgaitan/counter": {"id": 1053, "url": "https://apify.com/juansgaitan/counter/api/client/nodejs", "title": "Apify API and Page Visits Counter interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"juansgaitan/counter\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "juansgaitan/counter"}, "rikunk/youtube-channel-contact-information-finder": {"id": 1054, "url": "https://apify.com/rikunk/youtube-channel-contact-information-finder/api/client/nodejs", "title": "Apify API and YouTube Channel Contact Information Finder (up-to-date) interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchTitles\": [\n        \"MrBeast\",\n        \"Kids Diana Show\"\n    ],\n    \"searchURLs\": [\n        \"https://www.youtube.com/@PewDiePie\",\n        \"https://www.youtube.com/@MOVIECLIPS/featured\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"rikunk/youtube-channel-contact-information-finder\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"searchURLs": ["https://www.youtube.com/@PewDiePie", "https://www.youtube.com/@MOVIECLIPS/featured"], "searchTitles": ["MrBeast", "Kids Diana Show"]}, "actor_id": "rikunk/youtube-channel-contact-information-finder"}, "natanielsantos/shein-scraper": {"id": 1056, "url": "https://apify.com/natanielsantos/shein-scraper/api/client/nodejs", "title": "Apify API and Shein Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": [\n        {\n            \"url\": \"https://us.shein.com/Women-Beachwear-c-2039.html\"\n        }\n    ],\n    \"max_items_count\": 30,\n    \"proxySettings\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"natanielsantos/shein-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"start_urls": [{"url": "https://us.shein.com/Women-Beachwear-c-2039.html"}], "proxySettings": {"useApifyProxy": true}, "max_items_count": 30}, "actor_id": "natanielsantos/shein-scraper"}, "mtrunkat/xmls-to-dataset": {"id": 1057, "url": "https://apify.com/mtrunkat/xmls-to-dataset/api/client/nodejs", "title": "Apify API and XMLs To Dataset interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"sources\": [\n        {\n            \"url\": \"https://www.w3schools.com/xml/plant_catalog.xml\"\n        },\n        {\n            \"url\": \"https://www.w3schools.com/xml/cd_catalog.xml\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mtrunkat/xmls-to-dataset\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "sources": [{"url": "https://www.w3schools.com/xml/plant_catalog.xml"}, {"url": "https://www.w3schools.com/xml/cd_catalog.xml"}]}, "actor_id": "mtrunkat/xmls-to-dataset"}, "forward_dinosaur/shopify-app-store-scraper": {"id": 1058, "url": "https://apify.com/forward_dinosaur/shopify-app-store-scraper/api/client/nodejs", "title": "Apify API and Shopify App Store Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxItems\": 10,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"forward_dinosaur/shopify-app-store-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 10, "proxyConfiguration": {"useApifyProxy": true, "apifyProxyCountry": "US"}}, "actor_id": "forward_dinosaur/shopify-app-store-scraper"}, "mshopik/dania-furniture-scraper": {"id": 1059, "url": "https://apify.com/mshopik/dania-furniture-scraper/api/client/nodejs", "title": "Apify API and Dania Furniture Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/dania-furniture-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/dania-furniture-scraper"}, "apify/monitoring-checker-stats": {"id": 1060, "url": "https://apify.com/apify/monitoring-checker-stats/api/client/nodejs", "title": "Apify API and Monitoring Checker Stats interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/monitoring-checker-stats\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/monitoring-checker-stats"}, "jupri/freelancer-scraper": {"id": 1061, "url": "https://apify.com/jupri/freelancer-scraper/api/client/nodejs", "title": "Apify API and Freelancer interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/freelancer-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jupri/freelancer-scraper"}, "jupri/zomato": {"id": 1062, "url": "https://apify.com/jupri/zomato/api/client/nodejs", "title": "Apify API and Zomato Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"London\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/zomato\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"location": "London"}, "actor_id": "jupri/zomato"}, "katerinahronik/slack-message": {"id": 1063, "url": "https://apify.com/katerinahronik/slack-message/api/client/nodejs", "title": "Apify API and Slack Message Generator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"text\": \"Hello from Apify actor!\",\n    \"channel\": \"#general\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"katerinahronik/slack-message\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"text": "Hello from Apify actor!", "channel": "#general"}, "actor_id": "katerinahronik/slack-message"}, "novi/tiktok-user-api": {"id": 1064, "url": "https://apify.com/novi/tiktok-user-api/api/client/nodejs", "title": "Apify API and Tiktok User API (with no-watermark download link) interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"username\": \"nickiminai\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"novi/tiktok-user-api\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"username": "nickiminai", "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "novi/tiktok-user-api"}, "medh/asins-extractor": {"id": 1065, "url": "https://apify.com/medh/asins-extractor/api/client/nodejs", "title": "Apify API and ASINs Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://apify.com\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"medh/asins-extractor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://apify.com"}]}, "actor_id": "medh/asins-extractor"}, "jirimoravcik/gpt2-text-generation": {"id": 1066, "url": "https://apify.com/jirimoravcik/gpt2-text-generation/api/client/nodejs", "title": "Apify API and GPT-2 text generation interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jirimoravcik/gpt2-text-generation\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jirimoravcik/gpt2-text-generation"}, "jancurn/algolia-webcrawler": {"id": 1067, "url": "https://apify.com/jancurn/algolia-webcrawler/api/client/nodejs", "title": "Apify API and Algolia Webcrawler interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jancurn/algolia-webcrawler\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jancurn/algolia-webcrawler"}, "pocesar/download-youtube-video": {"id": 1068, "url": "https://apify.com/pocesar/download-youtube-video/api/client/nodejs", "title": "Apify API and Youtube video downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.youtube.com/watch?v=nn-bCRvhNUM\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    },\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/download-youtube-video\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://www.youtube.com/watch?v=nn-bCRvhNUM"}], "maxRequestRetries": 3}, "actor_id": "pocesar/download-youtube-video"}, "gololobov/unsplash-scraper": {"id": 1069, "url": "https://apify.com/gololobov/unsplash-scraper/api/client/nodejs", "title": "Apify API and Unsplash Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"gololobov/unsplash-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "gololobov/unsplash-scraper"}, "trudax/actor-macys-scraper": {"id": 1070, "url": "https://apify.com/trudax/actor-macys-scraper/api/client/nodejs", "title": "Apify API and Macy's Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxItems\": 5,\n    \"search\": \"Jeans\",\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"trudax/actor-macys-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyCountry": "US"}, "search": "Jeans", "maxItems": 5}, "actor_id": "trudax/actor-macys-scraper"}, "lucier/wsj-scraper": {"id": 1071, "url": "https://apify.com/lucier/wsj-scraper/api/client/nodejs", "title": "Apify API and WSJ Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.wsj.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lucier/wsj-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.wsj.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "lucier/wsj-scraper"}, "adrian_horning/best-indeed-jobs-scraper": {"id": 1072, "url": "https://apify.com/adrian_horning/best-indeed-jobs-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udd25 Best Indeed Jobs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"Data Engineer\",\n    \"location\": \"New York, NY\",\n    \"experienceLevel\": \"entry_level\",\n    \"sort\": \"date\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"adrian_horning/best-indeed-jobs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"sort": "date", "query": "Data Engineer", "location": "New York, NY", "experienceLevel": "entry_level"}, "actor_id": "adrian_horning/best-indeed-jobs-scraper"}, "apify/example-php": {"id": 1073, "url": "https://apify.com/apify/example-php/api/client/nodejs", "title": "Apify API and Example Php interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/example-php\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/example-php"}, "epctex/torrent-downloader": {"id": 1074, "url": "https://apify.com/epctex/torrent-downloader/api/client/nodejs", "title": "Apify API and Torrent Downloader Wizard interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"magnet:?xt=urn:btih:0D4AF725CC7CDF3CE933E5BFF8C0B3A6ED6E0D93&dn=Python+Programming+And+Maching+Learning+Understanding+How+To+Cod&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.dler.org%3A6969%2Fannounce&tr=udp%3A%2F%2Fopentracker.i2p.rocks%3A6969%2Fannounce&tr=udp%3A%2F%2F47.ip-51-68-199.eu%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.internetwarriors.net%3A1337%2Fannounce&tr=udp%3A%2F%2F9.rarbg.to%3A2920%2Fannounce&tr=udp%3A%2F%2Ftracker.pirateparty.gr%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.cyberia.is%3A6969%2Fannounce\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/torrent-downloader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": ["magnet:?xt=urn:btih:0D4AF725CC7CDF3CE933E5BFF8C0B3A6ED6E0D93&dn=Python+Programming+And+Maching+Learning+Understanding+How+To+Cod&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.dler.org%3A6969%2Fannounce&tr=udp%3A%2F%2Fopentracker.i2p.rocks%3A6969%2Fannounce&tr=udp%3A%2F%2F47.ip-51-68-199.eu%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.internetwarriors.net%3A1337%2Fannounce&tr=udp%3A%2F%2F9.rarbg.to%3A2920%2Fannounce&tr=udp%3A%2F%2Ftracker.pirateparty.gr%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.cyberia.is%3A6969%2Fannounce"]}, "actor_id": "epctex/torrent-downloader"}, "novi/tiktok-search-api": {"id": 1075, "url": "https://apify.com/novi/tiktok-search-api/api/client/nodejs", "title": "Apify API and Tiktok Search API, explore, discover TikTok content with ease interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"novi/tiktok-search-api\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "novi/tiktok-search-api"}, "zhangzhenhu/tiktok-scraper": {"id": 1076, "url": "https://apify.com/zhangzhenhu/tiktok-scraper/api/client/nodejs", "title": "Apify API and Tiktok Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"profiles\": [\n        \"filmesdotom888\"\n    ],\n    \"hashtags\": [\n        \"rich\"\n    ],\n    \"search_urls\": [\n        \"https://www.tiktok.com/search/user?q=elonmusk\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zhangzhenhu/tiktok-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"hashtags": ["rich"], "profiles": ["filmesdotom888"], "search_urls": ["https://www.tiktok.com/search/user?q=elonmusk"]}, "actor_id": "zhangzhenhu/tiktok-scraper"}, "apify/monitoring-reporter-dashboard": {"id": 1077, "url": "https://apify.com/apify/monitoring-reporter-dashboard/api/client/nodejs", "title": "Apify API and Monitoring Reporter Dashboard interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"name\": \"My scraping project\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/monitoring-reporter-dashboard\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"name": "My scraping project"}, "actor_id": "apify/monitoring-reporter-dashboard"}, "drinksight/save-to-s3": {"id": 1078, "url": "https://apify.com/drinksight/save-to-s3/api/client/nodejs", "title": "Apify API and Save To S3 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"accessKeyId\": \"\",\n    \"secretAccessKey\": \"\",\n    \"region\": \"\",\n    \"bucket\": \"\",\n    \"objectKeyFormat\": \"${resource.id}_${resource.startedAt}.${format}\",\n    \"datasetOptions\": {}\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"drinksight/save-to-s3\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"bucket": "", "region": "", "accessKeyId": "", "datasetOptions": {}, "objectKeyFormat": "${resource.id}_${resource.startedAt}.${format}", "secretAccessKey": ""}, "actor_id": "drinksight/save-to-s3"}, "mcdowell/yell-scraper": {"id": 1079, "url": "https://apify.com/mcdowell/yell-scraper/api/client/nodejs", "title": "Apify API and Yell Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.yell.com/ucs/UcsSearchAction.do?keywords=Pizza&location=chelsea\"\n        }\n    ],\n    \"sortBy\": \"relevance\",\n    \"maxResults\": 50,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mcdowell/yell-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"sortBy": "relevance", "startUrls": [{"url": "https://www.yell.com/ucs/UcsSearchAction.do?keywords=Pizza&location=chelsea"}], "maxResults": 50, "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "mcdowell/yell-scraper"}, "petrpatek/covid-usa-cdc": {"id": 1080, "url": "https://apify.com/petrpatek/covid-usa-cdc/api/client/nodejs", "title": "Apify API and Coronavirus USA CDC Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petrpatek/covid-usa-cdc\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petrpatek/covid-usa-cdc"}, "mscraper/temu-scraper": {"id": 1081, "url": "https://apify.com/mscraper/temu-scraper/api/client/nodejs", "title": "Apify API and Temu Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.temu.com/de/100pcs-colored-acrylic-crystal-beads-round-crackle-glass-beads-for-faux-jewelry-making-bracelets-earring-necklace-keychains-adults-beading-diy-art-craft-projects-christmas-ornament-birthday-gifts-8mm-0-31in-g-601099512321944.html\"\n        },\n        {\n            \"url\": \"https://www.temu.com/de/jewelry-making-accessories-o3-1485.html\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"DE\",\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    },\n    \"resultsLimit\": 20\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/temu-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "DE"}, "startUrls": [{"url": "https://www.temu.com/de/100pcs-colored-acrylic-crystal-beads-round-crackle-glass-beads-for-faux-jewelry-making-bracelets-earring-necklace-keychains-adults-beading-diy-art-craft-projects-christmas-ornament-birthday-gifts-8mm-0-31in-g-601099512321944.html"}, {"url": "https://www.temu.com/de/jewelry-making-accessories-o3-1485.html"}], "resultsLimit": 20}, "actor_id": "mscraper/temu-scraper"}, "juansgaitan/dns-extractor": {"id": 1082, "url": "https://apify.com/juansgaitan/dns-extractor/api/client/nodejs", "title": "Apify API and DNS Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"juansgaitan/dns-extractor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "juansgaitan/dns-extractor"}, "pocesar/crypto-watcher": {"id": 1083, "url": "https://apify.com/pocesar/crypto-watcher/api/client/nodejs", "title": "Apify API and Bitcoin / Ethereum Address Watcher interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxTransactions\": 10,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    },\n    \"extendOutputFunction\": async ({ data, item, page, request, _, customData }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ page, request, customData, Apify }) => {\n     \n    },\n    \"customData\": {}\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/crypto-watcher\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "customData": {}, "maxTransactions": 10}, "actor_id": "pocesar/crypto-watcher"}, "dtrungtin/gettyimages-scraper": {"id": 1084, "url": "https://apify.com/dtrungtin/gettyimages-scraper/api/client/nodejs", "title": "Apify API and Getty Images Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.gettyimages.com/photos/ukraine-war?family=editorial&assettype=image&phrase=ukraine%20war\"\n        }\n    ],\n    \"maxItems\": 10,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/gettyimages-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 10, "startUrls": [{"url": "https://www.gettyimages.com/photos/ukraine-war?family=editorial&assettype=image&phrase=ukraine%20war"}], "proxyConfig": {"useApifyProxy": true}}, "actor_id": "dtrungtin/gettyimages-scraper"}, "curious_coder/linkedin-profile-scraper": {"id": 1101, "url": "https://apify.com/curious_coder/linkedin-profile-scraper/api/client/nodejs", "title": "Apify API and Linkedin profile scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"minDelay\": 15,\n    \"maxDelay\": 60\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/linkedin-profile-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDelay": 60, "minDelay": 15}, "actor_id": "curious_coder/linkedin-profile-scraper"}, "zuzka/download-images-from-dataset": {"id": 1085, "url": "https://apify.com/zuzka/download-images-from-dataset/api/client/nodejs", "title": "Apify API and Download Images From Dataset interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"datasetId\": \"fm7rVjKXGOQnUZohL\",\n    \"pathToImageUrls\": \"image\",\n    \"limit\": 10,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/download-images-from-dataset\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"limit": 10, "datasetId": "fm7rVjKXGOQnUZohL", "pathToImageUrls": "image", "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "zuzka/download-images-from-dataset"}, "lukaskrivka/foursquare-reviews": {"id": 1086, "url": "https://apify.com/lukaskrivka/foursquare-reviews/api/client/nodejs", "title": "Apify API and Foursquare Reviews Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"queries\": [\n        {\n            \"location\": \"Prague\",\n            \"category\": \"food\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/foursquare-reviews\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "queries": [{"category": "food", "location": "Prague"}]}, "actor_id": "lukaskrivka/foursquare-reviews"}, "apify/example-live-view": {"id": 1087, "url": "https://apify.com/apify/example-live-view/api/client/nodejs", "title": "Apify API and Example Live View interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/example-live-view\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/example-live-view"}, "jahmadkhan/yelp-super-fast-data-scraper": {"id": 1088, "url": "https://apify.com/jahmadkhan/yelp-super-fast-data-scraper/api/client/nodejs", "title": "Apify API and Yelp || Super Fast Data Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchKeyowords\": \"gift shops\",\n    \"locationsZipCodes\": \"05141\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jahmadkhan/yelp-super-fast-data-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "searchKeyowords": "gift shops", "locationsZipCodes": "05141"}, "actor_id": "jahmadkhan/yelp-super-fast-data-scraper"}, "apify/example-code-runner-playwright": {"id": 1089, "url": "https://apify.com/apify/example-code-runner-playwright/api/client/nodejs", "title": "Apify API and Example Code Runner (Playwright) interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/example-code-runner-playwright\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/example-code-runner-playwright"}, "petr_cermak/json-to-xlsx": {"id": 1090, "url": "https://apify.com/petr_cermak/json-to-xlsx/api/client/nodejs", "title": "Apify API and Json To Xlsx interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/json-to-xlsx\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petr_cermak/json-to-xlsx"}, "fdg43jkg33455/abc-test-deri-458": {"id": 1091, "url": "https://apify.com/fdg43jkg33455/abc-test-deri-458/api/client/nodejs", "title": "Apify API and Abc Test Deri 458 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"fdg43jkg33455/abc-test-deri-458\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "fdg43jkg33455/abc-test-deri-458"}, "newpo/super-tiktok-scraper": {"id": 1092, "url": "https://apify.com/newpo/super-tiktok-scraper/api/client/nodejs", "title": "Apify API and Super Tiktok Scraper 20x Efficiency with user videos + hashtags interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"hashtags\": [\n        \"petsoftiktok\"\n    ],\n    \"users\": [\n        \"@gordonramsayofficial\"\n    ],\n    \"startUrls\": [],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"newpo/super-tiktok-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "users": ["@gordonramsayofficial"], "hashtags": ["petsoftiktok"], "startUrls": []}, "actor_id": "newpo/super-tiktok-scraper"}, "qbie/kickstarter-scraper": {"id": 1093, "url": "https://apify.com/qbie/kickstarter-scraper/api/client/nodejs", "title": "Apify API and Kickstarter Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxResults\": 50,\n    \"category\": \"All\",\n    \"sort\": \"newest\",\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"qbie/kickstarter-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"sort": "newest", "category": "All", "maxResults": 50, "proxyConfig": {"useApifyProxy": true}}, "actor_id": "qbie/kickstarter-scraper"}, "bebity/amazon-reviews-scraper": {"id": 1094, "url": "https://apify.com/bebity/amazon-reviews-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udd25 Amazon reviews scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bebity/amazon-reviews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "bebity/amazon-reviews-scraper"}, "runtime/youtube-channel": {"id": 1095, "url": "https://apify.com/runtime/youtube-channel/api/client/nodejs", "title": "Apify API and Youtube scrape : get the videos id from a Youtube channel interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urls\": [\n        \"https://www.youtube.com/@yvessaintlaurent\",\n        \"https://www.youtube.com/@chanel\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"runtime/youtube-channel\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": ["https://www.youtube.com/@yvessaintlaurent", "https://www.youtube.com/@chanel"]}, "actor_id": "runtime/youtube-channel"}, "dan.scraper/google-shopping": {"id": 1096, "url": "https://apify.com/dan.scraper/google-shopping/api/client/nodejs", "title": "Apify API and Google Shopping interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dan.scraper/google-shopping\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "dan.scraper/google-shopping"}, "fdg43jkg33455/abc-web-test-deri": {"id": 1097, "url": "https://apify.com/fdg43jkg33455/abc-web-test-deri/api/client/nodejs", "title": "Apify API and Abc Web Test Deri interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"runMode\": \"PRODUCTION\",\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.example.com\"\n        }\n    ],\n    \"pageFunction\": // The function accepts a single argument: the \"context\" object.\n    // For a complete list of its properties and functions,\n    // see https://apify.com/apify/web-scraper#page-function \n    async function pageFunction(context) {\n        // This statement works as a breakpoint when you're trying to debug your code. Works only with Run mode: DEVELOPMENT!\n        // debugger; \n    \n        // jQuery is handy for finding DOM elements and extracting data from them.\n        // To use it, make sure to enable the \"Inject jQuery\" option.\n        const $ = context.jQuery;\n        const pageTitle = $('title').first().text();\n        const h1 = $('h1').first().text();\n        const first_h2 = $('h2').first().text();\n        const random_text_from_the_page = $('p').first().text();\n    \n    \n        // Print some information to actor log\n        context.log.info(`URL: ${context.request.url}, TITLE: ${pageTitle}`);\n    \n        // Manually add a new page to the queue for scraping.\n       await context.enqueueRequest({ url: 'http://www.example.com' });\n    \n        // Return an object with the data extracted from the page.\n        // It will be stored to the resulting dataset.\n        return {\n            url: context.request.url,\n            pageTitle,\n            h1,\n            first_h2,\n            random_text_from_the_page\n        };\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"proxyRotation\": \"UNTIL_FAILURE\",\n    \"initialCookies\": [],\n    \"waitUntil\": [\n        \"networkidle2\"\n    ],\n    \"preNavigationHooks\": `// We need to return array of (possibly async) functions here.\n        // The functions accept two arguments: the \"crawlingContext\" object\n        // and \"gotoOptions\".\n        [\n            async (crawlingContext, gotoOptions) => {\n                // ...\n            },\n        ]`,\n    \"postNavigationHooks\": `// We need to return array of (possibly async) functions here.\n        // The functions accept a single argument: the \"crawlingContext\" object.\n        [\n            async (crawlingContext) => {\n                // ...\n            },\n        ]`,\n    \"breakpointLocation\": \"NONE\",\n    \"customData\": {}\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"fdg43jkg33455/abc-web-test-deri\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"runMode": "PRODUCTION", "startUrls": [{"url": "https://www.example.com"}], "waitUntil": ["networkidle2"], "customData": {}, "proxyRotation": "UNTIL_FAILURE", "initialCookies": [], "breakpointLocation": "NONE", "preNavigationHooks": "// We need to return array of (possibly async) functions here.\n        // The functions accept two arguments: the \"crawlingContext\" object\n        // and \"gotoOptions\".\n        [\n            async (crawlingContext, gotoOptions) => {\n                // ...\n            },\n        ]", "proxyConfiguration": {"useApifyProxy": true}, "postNavigationHooks": "// We need to return array of (possibly async) functions here.\n        // The functions accept a single argument: the \"crawlingContext\" object.\n        [\n            async (crawlingContext) => {\n                // ...\n            },\n        ]"}, "actor_id": "fdg43jkg33455/abc-web-test-deri"}, "lhotanok/product-hunt-profile-scraper": {"id": 1098, "url": "https://apify.com/lhotanok/product-hunt-profile-scraper/api/client/nodejs", "title": "Apify API and Product Hunt Profile Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"profileUrls\": [\n        \"https://www.producthunt.com/@chrismessina\"\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lhotanok/product-hunt-profile-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"profileUrls": ["https://www.producthunt.com/@chrismessina"], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "lhotanok/product-hunt-profile-scraper"}, "epctex/hackernews-scraper": {"id": 1099, "url": "https://apify.com/epctex/hackernews-scraper/api/client/nodejs", "title": "Apify API and Hacker News Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://news.ycombinator.com/front\"\n        },\n        {\n            \"url\": \"https://news.ycombinator.com/show\"\n        },\n        {\n            \"url\": \"https://news.ycombinator.com/jobs\"\n        },\n        {\n            \"url\": \"https://news.ycombinator.com\"\n        },\n        {\n            \"url\": \"https://news.ycombinator.com/item?id=26566373\"\n        }\n    ],\n    \"maxItems\": 50,\n    \"endPage\": 1,\n    \"extendOutputFunction\": ($) => { return {} },\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/hackernews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "endPage": 1, "maxItems": 50, "startUrls": [{"url": "https://news.ycombinator.com/front"}, {"url": "https://news.ycombinator.com/show"}, {"url": "https://news.ycombinator.com/jobs"}, {"url": "https://news.ycombinator.com"}, {"url": "https://news.ycombinator.com/item?id=26566373"}]}, "actor_id": "epctex/hackernews-scraper"}, "apify/email-signature-generator": {"id": 1100, "url": "https://apify.com/apify/email-signature-generator/api/client/nodejs", "title": "Apify API and Apify Email Signature Generator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/email-signature-generator\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/email-signature-generator"}, "geneea-analytics/reviews-text-nlp-analyzer": {"id": 1102, "url": "https://apify.com/geneea-analytics/reviews-text-nlp-analyzer/api/client/nodejs", "title": "Apify API and AI Text Analyzer for Google Reviews interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"geneea-analytics/reviews-text-nlp-analyzer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "geneea-analytics/reviews-text-nlp-analyzer"}, "epctex/twitter-spaces-scraper": {"id": 1103, "url": "https://apify.com/epctex/twitter-spaces-scraper/api/client/nodejs", "title": "Apify API and Twitter Spaces Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://twitter.com/i/spaces/1BdGYyVbyRLGX\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/twitter-spaces-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "startUrls": ["https://twitter.com/i/spaces/1BdGYyVbyRLGX"]}, "actor_id": "epctex/twitter-spaces-scraper"}, "jupri/wordpress-scraper": {"id": 1104, "url": "https://apify.com/jupri/wordpress-scraper/api/client/nodejs", "title": "Apify API and WordPress Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.sonymusic.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/wordpress-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.sonymusic.com"}, "actor_id": "jupri/wordpress-scraper"}, "pocesar/download-facebook-video": {"id": 1105, "url": "https://apify.com/pocesar/download-facebook-video/api/client/nodejs", "title": "Apify API and Facebook video downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.facebook.com/humansofnewyork/videos/394538142629575\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    },\n    \"maxRequestRetries\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/download-facebook-video\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://www.facebook.com/humansofnewyork/videos/394538142629575"}], "maxRequestRetries": 10}, "actor_id": "pocesar/download-facebook-video"}, "apify/monitoring-reporter-mail": {"id": 1106, "url": "https://apify.com/apify/monitoring-reporter-mail/api/client/nodejs", "title": "Apify API and Monitoring Reporter Mail interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"transformedResult\": []\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/monitoring-reporter-mail\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"transformedResult": []}, "actor_id": "apify/monitoring-reporter-mail"}, "monkey/realtorcomlight": {"id": 1107, "url": "https://apify.com/monkey/realtorcomlight/api/client/nodejs", "title": "Apify API and Realtor.com Scraper Light interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.realtor.com/realestateandhomes-search/Montana/baths-4/lot-sqft-653400\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"monkey/realtorcomlight\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.realtor.com/realestateandhomes-search/Montana/baths-4/lot-sqft-653400"}]}, "actor_id": "monkey/realtorcomlight"}, "curious_coder/linkedin-jobs-search-scraper": {"id": 1108, "url": "https://apify.com/curious_coder/linkedin-jobs-search-scraper/api/client/nodejs", "title": "Apify API and Linkedin job scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchUrl\": \"https://www.linkedin.com/jobs/search/?keywords=&location=United%20States&locationId=&geoId=103644278&f_TPR=&f_C=1035&f_PP=104145663&f_JT=F&f_WT=3%2C2&f_SB2=1&position=1&pageNum=0\",\n    \"startPage\": 1,\n    \"count\": 25,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"BUYPROXIES94952\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"minDelay\": 2,\n    \"maxDelay\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/linkedin-jobs-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"count": 25, "proxy": {"useApifyProxy": true, "apifyProxyGroups": ["BUYPROXIES94952"], "apifyProxyCountry": "US"}, "maxDelay": 5, "minDelay": 2, "searchUrl": "https://www.linkedin.com/jobs/search/?keywords=&location=United%20States&locationId=&geoId=103644278&f_TPR=&f_C=1035&f_PP=104145663&f_JT=F&f_WT=3%2C2&f_SB2=1&position=1&pageNum=0", "startPage": 1}, "actor_id": "curious_coder/linkedin-jobs-search-scraper"}, "krakorj/covid-russia": {"id": 1109, "url": "https://apify.com/krakorj/covid-russia/api/client/nodejs", "title": "Apify API and Coronavirus stats in Russia interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"krakorj/covid-russia\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "krakorj/covid-russia"}, "pertosh/chatgpt": {"id": 1110, "url": "https://apify.com/pertosh/chatgpt/api/client/nodejs", "title": "Apify API and ChatGPT interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pertosh/chatgpt\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "pertosh/chatgpt"}, "completed_ocarina/twitch-search-scraper": {"id": 1111, "url": "https://apify.com/completed_ocarina/twitch-search-scraper/api/client/nodejs", "title": "Apify API and Twitch Video Search Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxResults\": 100,\n    \"searchTerms\": \"Video Games\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"completed_ocarina/twitch-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxResults": 100, "searchTerms": "Video Games"}, "actor_id": "completed_ocarina/twitch-search-scraper"}, "jupri/magento-scraper": {"id": 1112, "url": "https://apify.com/jupri/magento-scraper/api/client/nodejs", "title": "Apify API and Magento E-Commerce Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.reebok.id\",\n    \"limit\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/magento-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.reebok.id", "limit": 5}, "actor_id": "jupri/magento-scraper"}, "lukaskrivka/find-ips-from-proxy-groups": {"id": 1113, "url": "https://apify.com/lukaskrivka/find-ips-from-proxy-groups/api/client/nodejs", "title": "Apify API and Find IPs from Proxy Groups interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/find-ips-from-proxy-groups\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukaskrivka/find-ips-from-proxy-groups"}, "hamza.alwan/craiyon-crawler": {"id": 1114, "url": "https://apify.com/hamza.alwan/craiyon-crawler/api/client/nodejs", "title": "Apify API and Craiyon Crawler interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchStrings\": [\n        \"Wood Sword\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/craiyon-crawler\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"searchStrings": ["Wood Sword"]}, "actor_id": "hamza.alwan/craiyon-crawler"}, "curious_coder/whatsapp-scraper": {"id": 1115, "url": "https://apify.com/curious_coder/whatsapp-scraper/api/client/nodejs", "title": "Apify API and Whatsapp scraper \u2705 FREE \u2705 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"numbers\": [\n        \"8976859807\",\n        \"8655907795\",\n        \"8860033772\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/whatsapp-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"numbers": ["8976859807", "8655907795", "8860033772"]}, "actor_id": "curious_coder/whatsapp-scraper"}, "adrian_horning/zillow-property-details": {"id": 1116, "url": "https://apify.com/adrian_horning/zillow-property-details/api/client/nodejs", "title": "Apify API and Zillow Property Details interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.zillow.com/homedetails/2704-Pither-Ln-Austin-TX-78741/124839823_zpid/\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"adrian_horning/zillow-property-details\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.zillow.com/homedetails/2704-Pither-Ln-Austin-TX-78741/124839823_zpid/"}, "actor_id": "adrian_horning/zillow-property-details"}, "bebity/linkedin-bulk-email-lookup": {"id": 1117, "url": "https://apify.com/bebity/linkedin-bulk-email-lookup/api/client/nodejs", "title": "Apify API and \ud83d\udd25 Linkedin Bulk Email Lookup interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bebity/linkedin-bulk-email-lookup\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "bebity/linkedin-bulk-email-lookup"}, "curious_coder/facebook-ads-library-scraper": {"id": 1118, "url": "https://apify.com/curious_coder/facebook-ads-library-scraper/api/client/nodejs", "title": "Apify API and Facebook ads library scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"scrapeSearchResults\",\n    \"searchUrl\": \"https://www.facebook.com/ads/library/?active_status=all&ad_type=all&country=IN&q=linkedin&search_type=keyword_unordered&media_type=all\",\n    \"urls\": [\n        \"https://www.facebook.com/ZapierApp\"\n    ],\n    \"count\": 30,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/facebook-ads-library-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": ["https://www.facebook.com/ZapierApp"], "count": 30, "proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "action": "scrapeSearchResults", "searchUrl": "https://www.facebook.com/ads/library/?active_status=all&ad_type=all&country=IN&q=linkedin&search_type=keyword_unordered&media_type=all"}, "actor_id": "curious_coder/facebook-ads-library-scraper"}, "mshopik/mapscom-scraper": {"id": 1119, "url": "https://apify.com/mshopik/mapscom-scraper/api/client/nodejs", "title": "Apify API and Maps.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/mapscom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/mapscom-scraper"}, "jaroslavhejlek/zip-key-value-store": {"id": 1120, "url": "https://apify.com/jaroslavhejlek/zip-key-value-store/api/client/nodejs", "title": "Apify API and Zip Key-value Store interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jaroslavhejlek/zip-key-value-store\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jaroslavhejlek/zip-key-value-store"}, "novi/tiktok-hashtag-api": {"id": 1121, "url": "https://apify.com/novi/tiktok-hashtag-api/api/client/nodejs", "title": "Apify API and Tiktok Hashtag API (with no-watermark download link) interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"name\": \"viral\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"novi/tiktok-hashtag-api\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"name": "viral", "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "novi/tiktok-hashtag-api"}, "curious_coder/amazon-scraper": {"id": 1122, "url": "https://apify.com/curious_coder/amazon-scraper/api/client/nodejs", "title": "Apify API and Amazon Scraper - Products and bestseller ranks interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchUrl\": \"https://www.amazon.in/s?k=redmi&rh=n%3A1389401031&ref=nb_sb_noss\",\n    \"startPage\": 1,\n    \"count\": 50,\n    \"threadCount\": 10,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/amazon-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"count": 50, "proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "searchUrl": "https://www.amazon.in/s?k=redmi&rh=n%3A1389401031&ref=nb_sb_noss", "startPage": 1, "threadCount": 10}, "actor_id": "curious_coder/amazon-scraper"}, "alexey/apple-maps-scraper": {"id": 1123, "url": "https://apify.com/alexey/apple-maps-scraper/api/client/nodejs", "title": "Apify API and Apple Maps Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"address\": \"Old Town Prague\",\n    \"searchTerm\": \"hotel\",\n    \"proxyConfig\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"alexey/apple-maps-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"address": "Old Town Prague", "searchTerm": "hotel", "proxyConfig": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}}, "actor_id": "alexey/apple-maps-scraper"}, "vaclavrut/mall-cz": {"id": 1136, "url": "https://apify.com/vaclavrut/mall-cz/api/client/nodejs", "title": "Apify API and Mall Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"vaclavrut/mall-cz\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "vaclavrut/mall-cz"}, "hamza.alwan/article-data-extractor": {"id": 1124, "url": "https://apify.com/hamza.alwan/article-data-extractor/api/client/nodejs", "title": "Apify API and Articles Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.cnbc.com/2022/09/21/what-another-major-rate-hike-by-the-federal-reserve-means-to-you.html\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/article-data-extractor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.cnbc.com/2022/09/21/what-another-major-rate-hike-by-the-federal-reserve-means-to-you.html"}], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "hamza.alwan/article-data-extractor"}, "anchor/doctolib": {"id": 1125, "url": "https://apify.com/anchor/doctolib/api/client/nodejs", "title": "Apify API and Doctolib interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.doctolib.fr/infectiologue/75001-paris\"\n        }\n    ],\n    \"pageFunction\": async function pageFunction(context) {\n        let data = {}\n        let userData = context.request.userData\n        data.url = context.request.url\n        data.label = userData.label\n        // product here is a reference to a doctor page. Naming is mislieading, my apologies. \n        // it's here because usually I work with marketplaces.\n        if(userData && userData.label === 'product'){   \n            context.log.info('label product.');     \n            // data.img = await context.page.locator('[data-qa-id=adview_spotlight_container] img >> nth=0').getAttribute('src')\n            data.nom = await context.page.locator('#main-content h1').innerText({timeout:6000})\n            try{\n                data.tarif = await context.page.locator('#payment_means').first().innerText({timeout:3000})\n                data.horaire_contact = await context.page.locator('#openings_and_contact').first().innerText({timeout:3000})\n                data.description = await context.page.locator('.dl-profile-bio').first().innerText({timeout:3000})\n                data.specialite = await context.page.locator('.dl-profile-header-speciality').first().innerText({timeout:3000})\n                data.expertise = await context.page.locator('#skills').first().innerText({timeout:3000})        \n                data.phones = await context.getPhones(data.horaire_contact)\n            }catch(e){\n                context.log.info('not found',e);     \n            }        \n            \n        }else{\n            context.log.info('we are not on a doctor page, so a search or pagination page.');\n            // we are looking for \"doctors\" (called \"product\" here) to be queued, let's write it down\n            userData.label = 'product';\n            const elements = context.page.locator('.dl-search-result-presentation a[href]');\n            const links = await elements.evaluateAll(elems => elems.map(elem => elem.getAttribute('href')));\n            links.forEach(async link => {\n                if(link.startsWith('/')){ link = 'https://www.doctolib.fr' + link }\n                await context.enqueueRequest(link, userData , false);\n            })\n        }\n        context.log.info(`function ended`);\n        delete data.label\n        return data;\n    },\n    \"pseudoUrls\": [\n        {\n            \"purl\": \"https://www.doctolib.fr/[.*]\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"anchor/doctolib\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.doctolib.fr/infectiologue/75001-paris"}], "pseudoUrls": [{"purl": "https://www.doctolib.fr/[.*]"}]}, "actor_id": "anchor/doctolib"}, "inutil_labs/whatsapp-groups-scraper": {"id": 1126, "url": "https://apify.com/inutil_labs/whatsapp-groups-scraper/api/client/nodejs", "title": "Apify API and Whatsapp Groups Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"token\": \"ENTER_YOUR_TOKEN_HERE\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"inutil_labs/whatsapp-groups-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"token": "ENTER_YOUR_TOKEN_HERE"}, "actor_id": "inutil_labs/whatsapp-groups-scraper"}, "lukaskrivka/rust-scraper": {"id": 1127, "url": "https://apify.com/lukaskrivka/rust-scraper/api/client/nodejs", "title": "Apify API and Rust Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urls\": [\n        {\n            \"url\": \"http://example.com\"\n        }\n    ],\n    \"extract\": [\n        {\n            \"field_name\": \"title\",\n            \"selector\": \"h1\",\n            \"extract_type\": {\n                \"type\": \"Text\"\n            }\n        },\n        {\n            \"field_name\": \"description\",\n            \"selector\": \"p\",\n            \"extract_type\": {\n                \"type\": \"Text\"\n            }\n        }\n    ],\n    \"proxy_settings\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/rust-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": [{"url": "http://example.com"}], "extract": [{"selector": "h1", "field_name": "title", "extract_type": {"type": "Text"}}, {"selector": "p", "field_name": "description", "extract_type": {"type": "Text"}}], "proxy_settings": {"useApifyProxy": true}}, "actor_id": "lukaskrivka/rust-scraper"}, "curious_coder/linkedin-sales-navigator-search-scraper": {"id": 1128, "url": "https://apify.com/curious_coder/linkedin-sales-navigator-search-scraper/api/client/nodejs", "title": "Apify API and Linkedin Sales Navigator Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"minDelay\": 2,\n    \"maxDelay\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/linkedin-sales-navigator-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "maxDelay": 5, "minDelay": 2}, "actor_id": "curious_coder/linkedin-sales-navigator-search-scraper"}, "newpo/eventbrite-scraper": {"id": 1137, "url": "https://apify.com/newpo/eventbrite-scraper/api/client/nodejs", "title": "Apify API and EventBrite Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.eventbrite.com/d/fl--miami-beach/all-events/?page=1\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"newpo/eventbrite-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.eventbrite.com/d/fl--miami-beach/all-events/?page=1"}]}, "actor_id": "newpo/eventbrite-scraper"}, "epctex/forever21-scraper": {"id": 1129, "url": "https://apify.com/epctex/forever21-scraper/api/client/nodejs", "title": "Apify API and Forever21 Data Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"black\",\n    \"startUrls\": [\n        \"https://www.forever21.com/us/shop/catalog/category/f21/promo-now-trending-neutrals?cgid=promo_now_trending_neutrals&start=32&sz=32\",\n        \"https://www.forever21.com/us/2000474416.html\",\n        \"https://www.forever21.com/on/demandware.store/Sites-forever21-Site/en_US/Search-Show?q=red+skirt&lang=en_US\"\n    ],\n    \"maxItems\": 20,\n    \"endPage\": 1,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/forever21-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "search": "black", "endPage": 1, "maxItems": 20, "startUrls": ["https://www.forever21.com/us/shop/catalog/category/f21/promo-now-trending-neutrals?cgid=promo_now_trending_neutrals&start=32&sz=32", "https://www.forever21.com/us/2000474416.html", "https://www.forever21.com/on/demandware.store/Sites-forever21-Site/en_US/Search-Show?q=red+skirt&lang=en_US"]}, "actor_id": "epctex/forever21-scraper"}, "jan.turon/pinecone-integration": {"id": 1130, "url": "https://apify.com/jan.turon/pinecone-integration/api/client/nodejs", "title": "Apify API and Apify-Pinecone integration interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jan.turon/pinecone-integration\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jan.turon/pinecone-integration"}, "glosterr/whoscored-player-information-scraper": {"id": 1131, "url": "https://apify.com/glosterr/whoscored-player-information-scraper/api/client/nodejs", "title": "Apify API and Whoscored Player Information Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.whoscored.com/Teams/37/Show/Germany-Bayern-Munich\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"glosterr/whoscored-player-information-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.whoscored.com/Teams/37/Show/Germany-Bayern-Munich"}]}, "actor_id": "glosterr/whoscored-player-information-scraper"}, "petr_cermak/mysql-insert": {"id": 1132, "url": "https://apify.com/petr_cermak/mysql-insert/api/client/nodejs", "title": "Apify API and MySQL Insert interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"connection\": {},\n    \"proxyConfig\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"qfCaFFuCodXxAS59E\"\n        ]\n    },\n    \"rows\": [],\n    \"staticParam\": {}\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/mysql-insert\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"rows": [], "connection": {}, "proxyConfig": {"useApifyProxy": true, "apifyProxyGroups": ["qfCaFFuCodXxAS59E"]}, "staticParam": {}}, "actor_id": "petr_cermak/mysql-insert"}, "zyberg/rightmove": {"id": 1133, "url": "https://apify.com/zyberg/rightmove/api/client/nodejs", "title": "Apify API and RightMove interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zyberg/rightmove\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "zyberg/rightmove"}, "hanatsai/fox-news-scraper": {"id": 1134, "url": "https://apify.com/hanatsai/fox-news-scraper/api/client/nodejs", "title": "Apify API and Fox News Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"http://foxnews.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hanatsai/fox-news-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "http://foxnews.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "hanatsai/fox-news-scraper"}, "dhrumil/rightmove-scraper": {"id": 1135, "url": "https://apify.com/dhrumil/rightmove-scraper/api/client/nodejs", "title": "Apify API and Rightmove Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"listUrls\": [\n        {\n            \"url\": \"https://www.rightmove.co.uk/property-for-sale/find.html?locationIdentifier=OUTCODE%5E6036\"\n        }\n    ],\n    \"propertyUrls\": [],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dhrumil/rightmove-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "listUrls": [{"url": "https://www.rightmove.co.uk/property-for-sale/find.html?locationIdentifier=OUTCODE%5E6036"}], "propertyUrls": []}, "actor_id": "dhrumil/rightmove-scraper"}, "apify/example-code-runner-cheerio": {"id": 1138, "url": "https://apify.com/apify/example-code-runner-cheerio/api/client/nodejs", "title": "Apify API and Example Code Runner (Cheerio) interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/example-code-runner-cheerio\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/example-code-runner-cheerio"}, "eneiromatos/expired-article-hunter": {"id": 1139, "url": "https://apify.com/eneiromatos/expired-article-hunter/api/client/nodejs", "title": "Apify API and Expired Article Hunter interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"expiredDomainUrls\": [\n        \"destinomexico.mx\"\n    ],\n    \"startYear\": 1990,\n    \"endYear\": 2099,\n    \"minWords\": 500,\n    \"maxWords\": 5000\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"eneiromatos/expired-article-hunter\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"endYear": 2099, "maxWords": 5000, "minWords": 500, "startYear": 1990, "expiredDomainUrls": ["destinomexico.mx"]}, "actor_id": "eneiromatos/expired-article-hunter"}, "scrapingxpert/realtor-agents-scraper-pro": {"id": 1140, "url": "https://apify.com/scrapingxpert/realtor-agents-scraper-pro/api/client/nodejs", "title": "Apify API and Realtor Agents Scraper Pro interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": [\n        {\n            \"url\": \"https://www.realtor.com/realestateagents/california-city_ca/\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"scrapingxpert/realtor-agents-scraper-pro\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"start_urls": [{"url": "https://www.realtor.com/realestateagents/california-city_ca/"}]}, "actor_id": "scrapingxpert/realtor-agents-scraper-pro"}, "natasha.lekh/vegan-places-finder": {"id": 1141, "url": "https://apify.com/natasha.lekh/vegan-places-finder/api/client/nodejs", "title": "Apify API and \ud83c\udf31 Vegan Places Finder interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"countryCode\": \"us\",\n    \"city\": \"Pittsburg\",\n    \"maxCrawledPlacesPerSearch\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"natasha.lekh/vegan-places-finder\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"city": "Pittsburg", "countryCode": "us", "maxCrawledPlacesPerSearch": 10}, "actor_id": "natasha.lekh/vegan-places-finder"}, "mscraper/wellfound-jobs-scraper": {"id": 1142, "url": "https://apify.com/mscraper/wellfound-jobs-scraper/api/client/nodejs", "title": "Apify API and Wellfound Jobs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\",\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    },\n    \"listingStartUrls\": [\n        {\n            \"url\": \"https://wellfound.com/role/l/devops-engineer/seattle\"\n        }\n    ],\n    \"jobsFilterFunction\": async ({ jobs }) => jobs.filter(({ published }) => ['today','yesterday','2 days ago','3 days ago','4 days ago','5 days ago','6 days ago','7 days ago'].includes(published))\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/wellfound-jobs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "listingStartUrls": [{"url": "https://wellfound.com/role/l/devops-engineer/seattle"}]}, "actor_id": "mscraper/wellfound-jobs-scraper"}, "jupri/target-scraper": {"id": 1143, "url": "https://apify.com/jupri/target-scraper/api/client/nodejs", "title": "Apify API and Target.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"Minnesota\",\n    \"query\": \"T-Shirt\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/target-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "T-Shirt", "location": "Minnesota"}, "actor_id": "jupri/target-scraper"}, "curious_coder/similarweb-scraper": {"id": 1144, "url": "https://apify.com/curious_coder/similarweb-scraper/api/client/nodejs", "title": "Apify API and Similarweb scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"domains\": [\n        \"twitter.com\",\n        \"apify.com\"\n    ],\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/similarweb-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"domains": ["twitter.com", "apify.com"], "maxDelay": 3, "minDelay": 1}, "actor_id": "curious_coder/similarweb-scraper"}, "zyberg/zoopla": {"id": 1189, "url": "https://apify.com/zyberg/zoopla/api/client/nodejs", "title": "Apify API and Zoopla interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zyberg/zoopla\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "zyberg/zoopla"}, "mnmkng/email-notification-webhook": {"id": 1145, "url": "https://apify.com/mnmkng/email-notification-webhook/api/client/nodejs", "title": "Apify API and Email Notification Webhook interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mnmkng/email-notification-webhook\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "mnmkng/email-notification-webhook"}, "rado.ch/rotten-tomatoes-scraper": {"id": 1146, "url": "https://apify.com/rado.ch/rotten-tomatoes-scraper/api/client/nodejs", "title": "Apify API and Free Rotten Tomatoes Web Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.rottentomatoes.com/browse/movies_in_theaters\"\n        }\n    ],\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"rado.ch/rotten-tomatoes-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.rottentomatoes.com/browse/movies_in_theaters"}], "proxyConfig": {"useApifyProxy": true}}, "actor_id": "rado.ch/rotten-tomatoes-scraper"}, "pocesar/sitemap-to-request-queue": {"id": 1147, "url": "https://apify.com/pocesar/sitemap-to-request-queue/api/client/nodejs", "title": "Apify API and Sitemap To Request Queue interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"proxyConfig\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"SHADER\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/sitemap-to-request-queue\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxyConfig": {"useApifyProxy": true, "apifyProxyGroups": ["SHADER"]}}, "actor_id": "pocesar/sitemap-to-request-queue"}, "curious_coder/instant-web-scraper": {"id": 1148, "url": "https://apify.com/curious_coder/instant-web-scraper/api/client/nodejs", "title": "Apify API and Instant web data scraper - Scrape any website interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"scrapeData\",\n    \"url\": \"https://apify.com/store\",\n    \"columnMappings\": [\n        {\n            \"key\": \"name\",\n            \"value\": \"2\"\n        },\n        {\n            \"key\": \"description\",\n            \"value\": \"4\"\n        },\n        {\n            \"key\": \"userName\",\n            \"value\": \"5\"\n        },\n        {\n            \"key\": \"usersCount\",\n            \"value\": \"8\"\n        },\n        {\n            \"key\": \"price\",\n            \"value\": \"7\"\n        },\n        {\n            \"key\": \"iconUrl\",\n            \"value\": \"1\"\n        },\n        {\n            \"key\": \"authorPicture\",\n            \"value\": \"6\"\n        }\n    ],\n    \"tableNumber\": 1,\n    \"nextPageSelector\": \"button.data-tracking-actor-pagination-button-next-store-web\",\n    \"count\": 100,\n    \"minDelay\": 2,\n    \"maxDelay\": 2\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/instant-web-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://apify.com/store", "count": 100, "action": "scrapeData", "maxDelay": 2, "minDelay": 2, "tableNumber": 1, "columnMappings": [{"key": "name", "value": "2"}, {"key": "description", "value": "4"}, {"key": "userName", "value": "5"}, {"key": "usersCount", "value": "8"}, {"key": "price", "value": "7"}, {"key": "iconUrl", "value": "1"}, {"key": "authorPicture", "value": "6"}], "nextPageSelector": "button.data-tracking-actor-pagination-button-next-store-web"}, "actor_id": "curious_coder/instant-web-scraper"}, "dhrumil/zoopla-scraper": {"id": 1149, "url": "https://apify.com/dhrumil/zoopla-scraper/api/client/nodejs", "title": "Apify API and Zoopla.co.uk Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"listUrls\": [\n        {\n            \"url\": \"https://www.zoopla.co.uk/for-sale/property/london/ec4/fleet-street-temple-blackfriars-st-pauls/?beds_min=4&q=ec4&radius=0.25&results_sort=newest_listings&search_source=for-sale\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dhrumil/zoopla-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "listUrls": [{"url": "https://www.zoopla.co.uk/for-sale/property/london/ec4/fleet-street-temple-blackfriars-st-pauls/?beds_min=4&q=ec4&radius=0.25&results_sort=newest_listings&search_source=for-sale"}]}, "actor_id": "dhrumil/zoopla-scraper"}, "mscraper/instagram-threads-scraper": {"id": 1150, "url": "https://apify.com/mscraper/instagram-threads-scraper/api/client/nodejs", "title": "Apify API and Instagram Threads Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"usernames\": [\n        \"@alex_elle\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/instagram-threads-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyCountry": "US"}, "usernames": ["@alex_elle"]}, "actor_id": "mscraper/instagram-threads-scraper"}, "saswave/trustpilot-company-infos": {"id": 1151, "url": "https://apify.com/saswave/trustpilot-company-infos/api/client/nodejs", "title": "Apify API and Trustpilot company review scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"domains\": [\n        \"www.google.com\",\n        \"www.youtube.com\"\n    ],\n    \"str_domains\": \"www.google.com,www.youtube.com,apple.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"saswave/trustpilot-company-infos\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"domains": ["www.google.com", "www.youtube.com"], "str_domains": "www.google.com,www.youtube.com,apple.com"}, "actor_id": "saswave/trustpilot-company-infos"}, "curious_coder/dun-and-bradstreet-dnb-com-scraper": {"id": 1152, "url": "https://apify.com/curious_coder/dun-and-bradstreet-dnb-com-scraper/api/client/nodejs", "title": "Apify API and Dun and Bradstreet Business and Contact Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"scrapeSearch\",\n    \"searchTerm\": \"marketing\",\n    \"searchType\": \"company\",\n    \"startPage\": 1,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/dun-and-bradstreet-dnb-com-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "action": "scrapeSearch", "startPage": 1, "searchTerm": "marketing", "searchType": "company"}, "actor_id": "curious_coder/dun-and-bradstreet-dnb-com-scraper"}, "zyberg/spareroom": {"id": 1153, "url": "https://apify.com/zyberg/spareroom/api/client/nodejs", "title": "Apify API and Spareroom interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zyberg/spareroom\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "zyberg/spareroom"}, "dan.scraper/ebay-scraper": {"id": 1154, "url": "https://apify.com/dan.scraper/ebay-scraper/api/client/nodejs", "title": "Apify API and Ebay Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dan.scraper/ebay-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "dan.scraper/ebay-scraper"}, "vojtam/espn": {"id": 1155, "url": "https://apify.com/vojtam/espn/api/client/nodejs", "title": "Apify API and ESPN.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"matchListYears\": [\n        \"2022\"\n    ],\n    \"matchListSeasonTypes\": [\n        \"reg\",\n        \"post\"\n    ],\n    \"matchListLeagues\": [\n        \"mlb\"\n    ],\n    \"detailMatches\": [],\n    \"matchDetailsLeague\": \"mlb\",\n    \"newsLeague\": \"mlb\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"vojtam/espn\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"newsLeague": "mlb", "detailMatches": [], "matchListYears": ["2022"], "matchListLeagues": ["mlb"], "matchDetailsLeague": "mlb", "matchListSeasonTypes": ["reg", "post"]}, "actor_id": "vojtam/espn"}, "jupri/kayak-hotels": {"id": 1156, "url": "https://apify.com/jupri/kayak-hotels/api/client/nodejs", "title": "Apify API and Kayak.com Hotels Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"Jakarta\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/kayak-hotels\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"location": "Jakarta"}, "actor_id": "jupri/kayak-hotels"}, "erikhiggy96/on3-recruit-scraper": {"id": 1157, "url": "https://apify.com/erikhiggy96/on3-recruit-scraper/api/client/nodejs", "title": "Apify API and On3 Recruit Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"recruitingClass\": \"2022\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"erikhiggy96/on3-recruit-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"recruitingClass": "2022"}, "actor_id": "erikhiggy96/on3-recruit-scraper"}, "umairisrar/ecom-master-scraper": {"id": 1158, "url": "https://apify.com/umairisrar/ecom-master-scraper/api/client/nodejs", "title": "Apify API and Ecommerce Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"umairisrar/ecom-master-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "umairisrar/ecom-master-scraper"}, "theo/bbc-scraper": {"id": 1159, "url": "https://apify.com/theo/bbc-scraper/api/client/nodejs", "title": "Apify API and BBC Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.bbc.com/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"theo/bbc-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.bbc.com/"}], "maxArticlesPerCrawl": 100}, "actor_id": "theo/bbc-scraper"}, "zuzka/reuters-scraper": {"id": 1160, "url": "https://apify.com/zuzka/reuters-scraper/api/client/nodejs", "title": "Apify API and Reuters Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.reuters.com/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/reuters-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.reuters.com/"}], "maxArticlesPerCrawl": 100}, "actor_id": "zuzka/reuters-scraper"}, "mscraper/dun-bradstreet-business-directory-scraper": {"id": 1161, "url": "https://apify.com/mscraper/dun-bradstreet-business-directory-scraper/api/client/nodejs", "title": "Apify API and Dun & Bradstreet Business Directory Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"listingUrls\": [\n        {\n            \"url\": \"https://www.dnb.com/business-directory/company-information.manufacturing.eg.html\"\n        }\n    ],\n    \"maxPagesPerListing\": 1,\n    \"overviewCountryName\": \"\",\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"CA\",\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/dun-bradstreet-business-directory-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "CA"}, "listingUrls": [{"url": "https://www.dnb.com/business-directory/company-information.manufacturing.eg.html"}], "maxPagesPerListing": 1, "overviewCountryName": ""}, "actor_id": "mscraper/dun-bradstreet-business-directory-scraper"}, "jupri/yelp": {"id": 1162, "url": "https://apify.com/jupri/yelp/api/client/nodejs", "title": "Apify API and Yelp.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"scraper\",\n    \"location\": \"New York\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/yelp\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "scraper", "location": "New York"}, "actor_id": "jupri/yelp"}, "curious_coder/clutch-scraper": {"id": 1163, "url": "https://apify.com/curious_coder/clutch-scraper/api/client/nodejs", "title": "Apify API and Clutch Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"scrapeCompanies\",\n    \"scrapeCompanies.url\": \"https://clutch.co/web-developers\",\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/clutch-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "action": "scrapeCompanies", "maxDelay": 3, "minDelay": 1, "scrapeCompanies.url": "https://clutch.co/web-developers"}, "actor_id": "curious_coder/clutch-scraper"}, "newpo/etsy-scraper": {"id": 1164, "url": "https://apify.com/newpo/etsy-scraper/api/client/nodejs", "title": "Apify API and Etsy Product Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.etsy.com/search?q=painting&ref=pagination&page=1\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"newpo/etsy-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.etsy.com/search?q=painting&ref=pagination&page=1"}]}, "actor_id": "newpo/etsy-scraper"}, "bebity/indeed-jobs-scraper": {"id": 1165, "url": "https://apify.com/bebity/indeed-jobs-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udd25 Indeed Jobs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bebity/indeed-jobs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "bebity/indeed-jobs-scraper"}, "onidivo/pdf-scraper": {"id": 1166, "url": "https://apify.com/onidivo/pdf-scraper/api/client/nodejs", "title": "Apify API and PDF Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"pdfUrl\": \"http://www.pdf995.com/samples/pdf.pdf\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"onidivo/pdf-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"pdfUrl": "http://www.pdf995.com/samples/pdf.pdf", "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "onidivo/pdf-scraper"}, "jancurn/selenium-custom-firefox": {"id": 1167, "url": "https://apify.com/jancurn/selenium-custom-firefox/api/client/nodejs", "title": "Apify API and Selenium Custom Firefox POC interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jancurn/selenium-custom-firefox\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jancurn/selenium-custom-firefox"}, "jan.turon/baidu-scraper": {"id": 1168, "url": "https://apify.com/jan.turon/baidu-scraper/api/client/nodejs", "title": "Apify API and Baidu Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchPhrases\": [\n        \"Apify\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jan.turon/baidu-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"searchPhrases": ["Apify"]}, "actor_id": "jan.turon/baidu-scraper"}, "epctex/binance-futures-listener": {"id": 1169, "url": "https://apify.com/epctex/binance-futures-listener/api/client/nodejs", "title": "Apify API and Binance Futures Tracker interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"traderId\": \"FAD84AAFD6E43900BF15E06B21857715\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/binance-futures-listener\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "traderId": "FAD84AAFD6E43900BF15E06B21857715"}, "actor_id": "epctex/binance-futures-listener"}, "pocesar/google-search-by-image": {"id": 1170, "url": "https://apify.com/pocesar/google-search-by-image/api/client/nodejs", "title": "Apify API and Google Search By Image interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dc/Steve_Jobs_Headshot_2010-CROP_%28cropped_2%29.jpg/1024px-Steve_Jobs_Headshot_2010-CROP_%28cropped_2%29.jpg\",\n            \"userData\": {\n                \"name\": \"Steve Jobs\"\n            }\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/google-search-by-image\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dc/Steve_Jobs_Headshot_2010-CROP_%28cropped_2%29.jpg/1024px-Steve_Jobs_Headshot_2010-CROP_%28cropped_2%29.jpg", "userData": {"name": "Steve Jobs"}}]}, "actor_id": "pocesar/google-search-by-image"}, "natanielsantos/flipkart-scraper": {"id": 1171, "url": "https://apify.com/natanielsantos/flipkart-scraper/api/client/nodejs", "title": "Apify API and Flipkart Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": [\n        {\n            \"url\": \"https://www.flipkart.com/adidas-ampligy-m-running-shoes-men/p/itmab79cd4ce225d\"\n        }\n    ],\n    \"proxySettings\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"natanielsantos/flipkart-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"start_urls": [{"url": "https://www.flipkart.com/adidas-ampligy-m-running-shoes-men/p/itmab79cd4ce225d"}], "proxySettings": {"useApifyProxy": true}}, "actor_id": "natanielsantos/flipkart-scraper"}, "comchat/reddit-api-scraper": {"id": 1172, "url": "https://apify.com/comchat/reddit-api-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udd11 Reddit API Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchList\": [\n        \"webscraping\"\n    ],\n    \"resultsLimit\": 5,\n    \"sortBy\": \"new\",\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"comchat/reddit-api-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "sortBy": "new", "searchList": ["webscraping"], "resultsLimit": 5}, "actor_id": "comchat/reddit-api-scraper"}, "big-brain.io/linkedin-session-cookies": {"id": 1173, "url": "https://apify.com/big-brain.io/linkedin-session-cookies/api/client/nodejs", "title": "Apify API and Linkedin Session Cookies interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"imap\": {\n        \"user\": \"\",\n        \"password\": \"\",\n        \"host\": \"imap.gmail.com\",\n        \"port\": 993,\n        \"tls\": true,\n        \"mailbox\": \"INBOX\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"big-brain.io/linkedin-session-cookies\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"imap": {"tls": true, "host": "imap.gmail.com", "port": 993, "user": "", "mailbox": "INBOX", "password": ""}, "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "big-brain.io/linkedin-session-cookies"}, "danielmilevski9/facebook-business-hashtag": {"id": 1174, "url": "https://apify.com/danielmilevski9/facebook-business-hashtag/api/client/nodejs", "title": "Apify API and Facebook Business Hashtag interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"keywordList\": [\n        \"webscraping\"\n    ],\n    \"resultsLimit\": 50,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"danielmilevski9/facebook-business-hashtag\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "keywordList": ["webscraping"], "resultsLimit": 50}, "actor_id": "danielmilevski9/facebook-business-hashtag"}, "big-brain.io/linkedin-contacts-uploader": {"id": 1175, "url": "https://apify.com/big-brain.io/linkedin-contacts-uploader/api/client/nodejs", "title": "Apify API and Linkedin Contacts CSV Uploader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"big-brain.io/linkedin-contacts-uploader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "big-brain.io/linkedin-contacts-uploader"}, "novi/tiktok-comment-api": {"id": 1176, "url": "https://apify.com/novi/tiktok-comment-api/api/client/nodejs", "title": "Apify API and Blazing Fast TikTok Comment API interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.tiktok.com/@ladygaga/video/7211250685902359850\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"novi/tiktok-comment-api\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.tiktok.com/@ladygaga/video/7211250685902359850", "proxyConfiguration": {"useApifyProxy": false}}, "actor_id": "novi/tiktok-comment-api"}, "epctex/soundcloud-scraper": {"id": 1177, "url": "https://apify.com/epctex/soundcloud-scraper/api/client/nodejs", "title": "Apify API and SoundCloud Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://soundcloud.com/sibel-dar-c/ekin-ekinci-gel-art-k\",\n        \"https://soundcloud.com/sibel-dar-c\",\n        \"https://soundcloud.com/swedish-hiphop-rap-fm/sets/pistoler-poesi-och-sex\",\n        \"https://soundcloud.com/search/sets?q=dnb\",\n        \"https://soundcloud.com/search/people?q=dnb\",\n        \"https://soundcloud.com/search/albums?q=dnb\",\n        \"https://soundcloud.com/search/sounds?q=dnb\"\n    ],\n    \"maxItems\": 20,\n    \"endPage\": 1,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/soundcloud-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "endPage": 1, "maxItems": 20, "startUrls": ["https://soundcloud.com/sibel-dar-c/ekin-ekinci-gel-art-k", "https://soundcloud.com/sibel-dar-c", "https://soundcloud.com/swedish-hiphop-rap-fm/sets/pistoler-poesi-och-sex", "https://soundcloud.com/search/sets?q=dnb", "https://soundcloud.com/search/people?q=dnb", "https://soundcloud.com/search/albums?q=dnb", "https://soundcloud.com/search/sounds?q=dnb"]}, "actor_id": "epctex/soundcloud-scraper"}, "mshopik/ontario-cannabis-store-scraper": {"id": 1178, "url": "https://apify.com/mshopik/ontario-cannabis-store-scraper/api/client/nodejs", "title": "Apify API and Ontario Cannabis Store Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/ontario-cannabis-store-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/ontario-cannabis-store-scraper"}, "dtrungtin/saksfifthavenue-scraper": {"id": 1179, "url": "https://apify.com/dtrungtin/saksfifthavenue-scraper/api/client/nodejs", "title": "Apify API and Saksfifthavenue Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.saksfifthavenue.com/\"\n        }\n    ],\n    \"extendOutputFunction\": ($) => { return {} },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/saksfifthavenue-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.saksfifthavenue.com/"}], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "dtrungtin/saksfifthavenue-scraper"}, "jupri/argos-scraper": {"id": 1181, "url": "https://apify.com/jupri/argos-scraper/api/client/nodejs", "title": "Apify API and Argos Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/argos-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jupri/argos-scraper"}, "epctex/instagram-video-downloader": {"id": 1182, "url": "https://apify.com/epctex/instagram-video-downloader/api/client/nodejs", "title": "Apify API and Instagram Video Downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.instagram.com/p/CuKHeY_PLbZ/\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/instagram-video-downloader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": ["https://www.instagram.com/p/CuKHeY_PLbZ/"]}, "actor_id": "epctex/instagram-video-downloader"}, "curious_coder/threads-scraper": {"id": 1183, "url": "https://apify.com/curious_coder/threads-scraper/api/client/nodejs", "title": "Apify API and Meta threads scraper \u2705 FREE \u2705 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"scrapeThreads\",\n    \"scrapeThreads.profileUrl\": \"https://www.threads.net/@netflix\",\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/threads-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "action": "scrapeThreads", "maxDelay": 3, "minDelay": 1, "scrapeThreads.profileUrl": "https://www.threads.net/@netflix"}, "actor_id": "curious_coder/threads-scraper"}, "curious_coder/google-play-scraper": {"id": 1184, "url": "https://apify.com/curious_coder/google-play-scraper/api/client/nodejs", "title": "Apify API and Google Play Scraper \u2705 FREE \u2705 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"scrapeReviews\",\n    \"scrapeReviews.appId\": \"com.rockstargames.gtasa\",\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/google-play-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"action": "scrapeReviews", "maxDelay": 3, "minDelay": 1, "scrapeReviews.appId": "com.rockstargames.gtasa"}, "actor_id": "curious_coder/google-play-scraper"}, "petrpatek/simple-captcha-tesseract": {"id": 1185, "url": "https://apify.com/petrpatek/simple-captcha-tesseract/api/client/nodejs", "title": "Apify API and Simple Captcha Tesseract\ud83d\ude42 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petrpatek/simple-captcha-tesseract\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petrpatek/simple-captcha-tesseract"}, "mcdowell/leboncoin-scraper": {"id": 1186, "url": "https://apify.com/mcdowell/leboncoin-scraper/api/client/nodejs", "title": "Apify API and Leboncoin Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxResults\": 50\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mcdowell/leboncoin-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxResults": 50}, "actor_id": "mcdowell/leboncoin-scraper"}, "fdg43jkg33455/abc-web-test": {"id": 1187, "url": "https://apify.com/fdg43jkg33455/abc-web-test/api/client/nodejs", "title": "Apify API and Abc Web Test interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"runMode\": \"PRODUCTION\",\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.example.com\"\n        }\n    ],\n    \"pageFunction\": // The function accepts a single argument: the \"context\" object.\n    // For a complete list of its properties and functions,\n    // see https://apify.com/apify/web-scraper#page-function \n    async function pageFunction(context) {\n        // This statement works as a breakpoint when you're trying to debug your code. Works only with Run mode: DEVELOPMENT!\n        // debugger; \n    \n        // jQuery is handy for finding DOM elements and extracting data from them.\n        // To use it, make sure to enable the \"Inject jQuery\" option.\n        const $ = context.jQuery;\n        const pageTitle = $('title').first().text();\n        const h1 = $('h1').first().text();\n        const first_h2 = $('h2').first().text();\n        const random_text_from_the_page = $('p').first().text();\n    \n    \n        // Print some information to actor log\n        context.log.info(`URL: ${context.request.url}, TITLE: ${pageTitle}`);\n    \n        // Manually add a new page to the queue for scraping.\n       await context.enqueueRequest({ url: 'http://www.example.com' });\n    \n        // Return an object with the data extracted from the page.\n        // It will be stored to the resulting dataset.\n        return {\n            url: context.request.url,\n            pageTitle,\n            h1,\n            first_h2,\n            random_text_from_the_page\n        };\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"proxyRotation\": \"UNTIL_FAILURE\",\n    \"initialCookies\": [],\n    \"waitUntil\": [\n        \"networkidle2\"\n    ],\n    \"preNavigationHooks\": `// We need to return array of (possibly async) functions here.\n        // The functions accept two arguments: the \"crawlingContext\" object\n        // and \"gotoOptions\".\n        [\n            async (crawlingContext, gotoOptions) => {\n                // ...\n            },\n        ]`,\n    \"postNavigationHooks\": `// We need to return array of (possibly async) functions here.\n        // The functions accept a single argument: the \"crawlingContext\" object.\n        [\n            async (crawlingContext) => {\n                // ...\n            },\n        ]`,\n    \"breakpointLocation\": \"NONE\",\n    \"customData\": {}\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"fdg43jkg33455/abc-web-test\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"runMode": "PRODUCTION", "startUrls": [{"url": "https://www.example.com"}], "waitUntil": ["networkidle2"], "customData": {}, "proxyRotation": "UNTIL_FAILURE", "initialCookies": [], "breakpointLocation": "NONE", "preNavigationHooks": "// We need to return array of (possibly async) functions here.\n        // The functions accept two arguments: the \"crawlingContext\" object\n        // and \"gotoOptions\".\n        [\n            async (crawlingContext, gotoOptions) => {\n                // ...\n            },\n        ]", "proxyConfiguration": {"useApifyProxy": true}, "postNavigationHooks": "// We need to return array of (possibly async) functions here.\n        // The functions accept a single argument: the \"crawlingContext\" object.\n        [\n            async (crawlingContext) => {\n                // ...\n            },\n        ]"}, "actor_id": "fdg43jkg33455/abc-web-test"}, "apify/quick-start": {"id": 1188, "url": "https://apify.com/apify/quick-start/api/client/nodejs", "title": "Apify API and Quick Start for Actor Creation interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/quick-start\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/quick-start"}, "misceres/nike-scraper": {"id": 1190, "url": "https://apify.com/misceres/nike-scraper/api/client/nodejs", "title": "Apify API and Nike Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"misceres/nike-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "misceres/nike-scraper"}, "dtrungtin/canva-templates-scraper": {"id": 1191, "url": "https://apify.com/dtrungtin/canva-templates-scraper/api/client/nodejs", "title": "Apify API and canva templates interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.canva.com/banners/templates/diwali/\"\n        }\n    ],\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/canva-templates-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.canva.com/banners/templates/diwali/"}], "proxyConfig": {"useApifyProxy": true}}, "actor_id": "dtrungtin/canva-templates-scraper"}, "fdg43jkg33455/abc-test": {"id": 1192, "url": "https://apify.com/fdg43jkg33455/abc-test/api/client/nodejs", "title": "Apify API and ABC Test interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"fdg43jkg33455/abc-test\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "fdg43jkg33455/abc-test"}, "microworlds/jumia-scraper": {"id": 1193, "url": "https://apify.com/microworlds/jumia-scraper/api/client/nodejs", "title": "Apify API and Jumia Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchTerms\": [\n        \"laptop\"\n    ],\n    \"country\": \"nigeria\",\n    \"maxCrawledProducts\": 500,\n    \"handlePageTimeoutSecs\": 5000,\n    \"maxRequestRetries\": 3,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"microworlds/jumia-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"country": "nigeria", "proxyConfig": {"useApifyProxy": true}, "searchTerms": ["laptop"], "maxRequestRetries": 3, "maxCrawledProducts": 500, "handlePageTimeoutSecs": 5000}, "actor_id": "microworlds/jumia-scraper"}, "apify/example-web-server": {"id": 1194, "url": "https://apify.com/apify/example-web-server/api/client/nodejs", "title": "Apify API and Example Web Server interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/example-web-server\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/example-web-server"}, "petr_cermak/dropbox-upload": {"id": 1195, "url": "https://apify.com/petr_cermak/dropbox-upload/api/client/nodejs", "title": "Apify API and Dropbox Upload interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/dropbox-upload\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petr_cermak/dropbox-upload"}, "aurimas/workable-jobs-scraper": {"id": 1196, "url": "https://apify.com/aurimas/workable-jobs-scraper/api/client/nodejs", "title": "Apify API and Web Scraping Job Postings interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"aurimas/workable-jobs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "aurimas/workable-jobs-scraper"}, "mtrunkat/web-scraper-experimental-dbgr": {"id": 1203, "url": "https://apify.com/mtrunkat/web-scraper-experimental-dbgr/api/client/nodejs", "title": "Apify API and Web Scraper Experimental Debug interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://apify.com\"\n        }\n    ],\n    \"pseudoUrls\": [\n        {\n            \"purl\": \"https://apify.com[(/[\\\\w-]+)?]\"\n        }\n    ],\n    \"linkSelector\": \"a\",\n    \"pageFunction\": async function pageFunction(context) {\n        // See README for context properties. If the syntax is unfamiliar see the link\n        // https://javascript.info/destructuring-assignment#object-destructuring\n        const { request, log, jQuery } = context;\n    \n        // To be able to use jQuery as $, one needs save it into a variable\n        // and select the inject jQuery option. We've selected it for you.\n        const $ = jQuery;\n        const title = $('title').text();\n    \n        // This is yet another new feature of Javascript called template strings.\n        // https://javascript.info/string#quotes\n        log.info(`URL: ${request.url} TITLE: ${title}`);\n    \n        // To save data just return an object with the requested properties.\n        return {\n            url: request.url,\n            title\n        };\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": false\n    },\n    \"initialCookies\": [],\n    \"waitUntil\": [\n        \"networkidle2\"\n    ],\n    \"customData\": {}\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mtrunkat/web-scraper-experimental-dbgr\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://apify.com"}], "waitUntil": ["networkidle2"], "customData": {}, "pseudoUrls": [{"purl": "https://apify.com[(/[\\w-]+)?]"}], "linkSelector": "a", "initialCookies": [], "proxyConfiguration": {"useApifyProxy": false}}, "actor_id": "mtrunkat/web-scraper-experimental-dbgr"}, "eneiromatos/ultimate-walmart-scraper": {"id": 1197, "url": "https://apify.com/eneiromatos/ultimate-walmart-scraper/api/client/nodejs", "title": "Apify API and Ultimate Walmart Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"productUrls\": [\n        \"https://www.walmart.com/ip/The-Get-Women-s-Plus-Size-Ruffle-Trim-T-Shirt/1009843134\",\n        \"https://www.walmart.com/ip/Fantaslook-Womens-Tank-Tops-Summer-V-Neck-T-Shirts-Sleeveless-Tops-Side-Split-Tanks/2028341337\"\n    ],\n    \"listingUrls\": [\n        \"https://www.walmart.com/browse/clothing/womens-tops/5438_133162_2290732\",\n        \"https://www.walmart.com/search?q=sony\"\n    ],\n    \"keywords\": [\n        \"sony headphones\"\n    ],\n    \"startPageNumber\": 1,\n    \"finalPageNumber\": 1,\n    \"minPrice\": 0,\n    \"maxPrice\": 0\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"eneiromatos/ultimate-walmart-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"keywords": ["sony headphones"], "maxPrice": 0, "minPrice": 0, "listingUrls": ["https://www.walmart.com/browse/clothing/womens-tops/5438_133162_2290732", "https://www.walmart.com/search?q=sony"], "productUrls": ["https://www.walmart.com/ip/The-Get-Women-s-Plus-Size-Ruffle-Trim-T-Shirt/1009843134", "https://www.walmart.com/ip/Fantaslook-Womens-Tank-Tops-Summer-V-Neck-T-Shirts-Sleeveless-Tops-Side-Split-Tanks/2028341337"], "finalPageNumber": 1, "startPageNumber": 1}, "actor_id": "eneiromatos/ultimate-walmart-scraper"}, "mscraper/tradingview-stock-scraper": {"id": 1198, "url": "https://apify.com/mscraper/tradingview-stock-scraper/api/client/nodejs", "title": "Apify API and TradingView Stock Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"symbols\": [\n        \"NASDAQ:MSFT\",\n        \"NASDAQ:GOOGL\",\n        \"NASDAQ:AMZN\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/tradingview-stock-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyCountry": "US"}, "symbols": ["NASDAQ:MSFT", "NASDAQ:GOOGL", "NASDAQ:AMZN"]}, "actor_id": "mscraper/tradingview-stock-scraper"}, "marcuspapin/agoda-scraper": {"id": 1199, "url": "https://apify.com/marcuspapin/agoda-scraper/api/client/nodejs", "title": "Apify API and Agoda Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchText\": \"Toronto\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"marcuspapin/agoda-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"searchText": "Toronto"}, "actor_id": "marcuspapin/agoda-scraper"}, "hamza.alwan/google-videos-scraper": {"id": 1200, "url": "https://apify.com/hamza.alwan/google-videos-scraper/api/client/nodejs", "title": "Apify API and Google Search Videos Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"queries\": \"Cars\",\n    \"maxPagesPerQuery\": 1,\n    \"resultsPerPage\": 100,\n    \"countryCode\": \"us\",\n    \"languageCode\": \"en\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/google-videos-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"queries": "Cars", "countryCode": "us", "languageCode": "en", "resultsPerPage": 100, "maxPagesPerQuery": 1}, "actor_id": "hamza.alwan/google-videos-scraper"}, "kuaima/xiaohongshu": {"id": 1201, "url": "https://apify.com/kuaima/xiaohongshu/api/client/nodejs", "title": "Apify API and XiaoHongShu Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"kuaima/xiaohongshu\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "kuaima/xiaohongshu"}, "dan.scraper/apple-app-store-app-details-scraper": {"id": 1202, "url": "https://apify.com/dan.scraper/apple-app-store-app-details-scraper/api/client/nodejs", "title": "Apify API and Apple App Store App Details Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dan.scraper/apple-app-store-app-details-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "dan.scraper/apple-app-store-app-details-scraper"}, "petr_cermak/anti-captcha": {"id": 1204, "url": "https://apify.com/petr_cermak/anti-captcha/api/client/nodejs", "title": "Apify API and Anti Captcha interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/anti-captcha\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petr_cermak/anti-captcha"}, "matthewgall/url-to-png": {"id": 1205, "url": "https://apify.com/matthewgall/url-to-png/api/client/nodejs", "title": "Apify API and URL to PNG Convertor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"matthewgall/url-to-png\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "matthewgall/url-to-png"}, "newbs/glassdoor-job-scraper": {"id": 1206, "url": "https://apify.com/newbs/glassdoor-job-scraper/api/client/nodejs", "title": "Apify API and Glassdoor Scraper: Extract Company Reviews & Salaries ! interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"numberOfPages\": 1,\n    \"job\": \"software engineer\",\n    \"location\": \"paris\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"newbs/glassdoor-job-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"job": "software engineer", "location": "paris", "numberOfPages": 1}, "actor_id": "newbs/glassdoor-job-scraper"}, "advisable_rain/uber-eats-menu-scraper": {"id": 1207, "url": "https://apify.com/advisable_rain/uber-eats-menu-scraper/api/client/nodejs", "title": "Apify API and Uber Eats Menu Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.ubereats.com/nz/store/pizza-hut-newtown/8F10LUogSqWYO1ajzth5sQ\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"advisable_rain/uber-eats-menu-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.ubereats.com/nz/store/pizza-hut-newtown/8F10LUogSqWYO1ajzth5sQ"}, "actor_id": "advisable_rain/uber-eats-menu-scraper"}, "apify/monitoring-reporter-slack": {"id": 1208, "url": "https://apify.com/apify/monitoring-reporter-slack/api/client/nodejs", "title": "Apify API and Monitoring Reporter Slack interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"transformedResult\": []\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/monitoring-reporter-slack\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"transformedResult": []}, "actor_id": "apify/monitoring-reporter-slack"}, "lexis-solutions/alibaba-scraper": {"id": 1209, "url": "https://apify.com/lexis-solutions/alibaba-scraper/api/client/nodejs", "title": "Apify API and Alibaba Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.alibaba.com/trade/search?fsb=y&IndexArea=product_en&categoryId=201153401&keywords=Groom+Wear&knowledgeGraphId=100010232-10000166340&viewtype=L&&pricef=80&pricet\"\n        }\n    ],\n    \"maxItems\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lexis-solutions/alibaba-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 1, "startUrls": [{"url": "https://www.alibaba.com/trade/search?fsb=y&IndexArea=product_en&categoryId=201153401&keywords=Groom+Wear&knowledgeGraphId=100010232-10000166340&viewtype=L&&pricef=80&pricet"}]}, "actor_id": "lexis-solutions/alibaba-scraper"}, "lukaskrivka/website-checker-playwright": {"id": 1210, "url": "https://apify.com/lukaskrivka/website-checker-playwright/api/client/nodejs", "title": "Apify API and Website Checker Runner Playwright interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urlsToCheck\": [\n        {\n            \"url\": \"https://www.amazon.com/b?ie=UTF8&node=11392907011\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": false\n    },\n    \"linkSelector\": \"a[href]\",\n    \"pseudoUrls\": [\n        {\n            \"purl\": \"https://www.amazon.com[.*]/dp/[.*]\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/website-checker-playwright\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"pseudoUrls": [{"purl": "https://www.amazon.com[.*]/dp/[.*]"}], "urlsToCheck": [{"url": "https://www.amazon.com/b?ie=UTF8&node=11392907011"}], "linkSelector": "a[href]", "proxyConfiguration": {"useApifyProxy": false}}, "actor_id": "lukaskrivka/website-checker-playwright"}, "jupri/google-bard": {"id": 1211, "url": "https://apify.com/jupri/google-bard/api/client/nodejs", "title": "Apify API and Google Bard AI Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": [\n        \"Describe this image.\",\n        \"What is Apify ?\",\n        \"Is Web Scraping legal ?\",\n        \"Who is Chuck Norris ?\"\n    ],\n    \"image\": [\n        \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Apify-logo.svg/512px-Apify-logo.svg.png\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/google-bard\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"image": ["https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Apify-logo.svg/512px-Apify-logo.svg.png"], "query": ["Describe this image.", "What is Apify ?", "Is Web Scraping legal ?", "Who is Chuck Norris ?"]}, "actor_id": "jupri/google-bard"}, "mscraper/monster-job-search-scraper": {"id": 1212, "url": "https://apify.com/mscraper/monster-job-search-scraper/api/client/nodejs", "title": "Apify API and Monster Job Search Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"IT\",\n    \"address\": \"London, England\",\n    \"country\": \"GB\",\n    \"radius\": 20,\n    \"resultsLimit\": 20,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"groups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"GB\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/monster-job-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"groups": ["RESIDENTIAL"], "useApifyProxy": true, "apifyProxyCountry": "GB"}, "query": "IT", "radius": 20, "address": "London, England", "country": "GB", "resultsLimit": 20}, "actor_id": "mscraper/monster-job-search-scraper"}, "apify/monitoring-checker-schema": {"id": 1213, "url": "https://apify.com/apify/monitoring-checker-schema/api/client/nodejs", "title": "Apify API and Monitoring Checker Schema interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"options\": `/* global t */\n        // t variable definition because of type validation ... \n         [{\n            schema: {\n                url: t.string.url,\n                date: t.string.date,\n                messages: t.array,\n            },\n        }, \n        {\n            filter: 'eshop', // you can also use IDs definition as: Ids: [\"id\", \"id\"]\n            minItemCount: 100,  \n            maxItemCount: 1000\n        }]`\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/monitoring-checker-schema\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"options": "/* global t */\n        // t variable definition because of type validation ... \n         [{\n            schema: {\n                url: t.string.url,\n                date: t.string.date,\n                messages: t.array,\n            },\n        }, \n        {\n            filter: 'eshop', // you can also use IDs definition as: Ids: [\"id\", \"id\"]\n            minItemCount: 100,  \n            maxItemCount: 1000\n        }]"}, "actor_id": "apify/monitoring-checker-schema"}, "saswave/linkedin-company-ads": {"id": 1214, "url": "https://apify.com/saswave/linkedin-company-ads/api/client/nodejs", "title": "Apify API and Linkedin Company Ads interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"companies\": [\n        \"https://www.linkedin.com/company/financial-times/\",\n        \"https://www.linkedin.com/company/bloomberg-news/\"\n    ],\n    \"ad_limit\": 15,\n    \"ad_description\": \"yes\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"saswave/linkedin-company-ads\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"ad_limit": 15, "companies": ["https://www.linkedin.com/company/financial-times/", "https://www.linkedin.com/company/bloomberg-news/"], "ad_description": "yes"}, "actor_id": "saswave/linkedin-company-ads"}, "babak/angi-angie-s-list-scraper": {"id": 1215, "url": "https://apify.com/babak/angi-angie-s-list-scraper/api/client/nodejs", "title": "Apify API and Angi (Angie's List) Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"Portland\",\n    \"category\": \"asbestos-removal\",\n    \"maxListings\": 100,\n    \"maxReviews\": 10,\n    \"directLinks\": [],\n    \"categoryLinks\": [],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"maxConcurrency\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"babak/angi-angie-s-list-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"category": "asbestos-removal", "location": "Portland", "maxReviews": 10, "directLinks": [], "maxListings": 100, "categoryLinks": [], "maxConcurrency": 100, "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "babak/angi-angie-s-list-scraper"}, "jupri/google-speech": {"id": 1216, "url": "https://apify.com/jupri/google-speech/api/client/nodejs", "title": "Apify API and Google Free Text to Speech interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"The web is the largest and most important source of information ever created by humankind. And yet, most of that information is only available in a human-readable form that cannot be easily used by automated systems.\\n\\nOur mission is to let people automate mundane tasks on the web and spend their time on things that matter. We strive to keep the web open as a public good and a basic right for everyone, regardless of the way you want to use it, as its creators intended.\",\n    \"lang\": \"en-US\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/google-speech\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"lang": "en-US", "query": "The web is the largest and most important source of information ever created by humankind. And yet, most of that information is only available in a human-readable form that cannot be easily used by automated systems.\n\nOur mission is to let people automate mundane tasks on the web and spend their time on things that matter. We strive to keep the web open as a public good and a basic right for everyone, regardless of the way you want to use it, as its creators intended."}, "actor_id": "jupri/google-speech"}, "jirimoravcik/dalle-2-image-generation": {"id": 1225, "url": "https://apify.com/jirimoravcik/dalle-2-image-generation/api/client/nodejs", "title": "Apify API and DALL-E 2 Image Generation interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jirimoravcik/dalle-2-image-generation\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jirimoravcik/dalle-2-image-generation"}, "mscraper/tiktok-search-autocomplete": {"id": 1217, "url": "https://apify.com/mscraper/tiktok-search-autocomplete/api/client/nodejs", "title": "Apify API and TikTok Search Autocomplete interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": [\n        \"follow for\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\",\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/tiktok-search-autocomplete\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "query": ["follow for"]}, "actor_id": "mscraper/tiktok-search-autocomplete"}, "epctex/easyjet-scraper": {"id": 1218, "url": "https://apify.com/epctex/easyjet-scraper/api/client/nodejs", "title": "Apify API and easyJet Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"ROUND\",\n    \"arrival\": \"SAW\",\n    \"departure\": \"LPL\",\n    \"departureDate\": \"2023-10-10\",\n    \"returnDate\": \"2023-10-14\",\n    \"adults\": 1,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/easyjet-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "ROUND", "proxy": {"useApifyProxy": true}, "adults": 1, "arrival": "SAW", "departure": "LPL", "returnDate": "2023-10-14", "departureDate": "2023-10-10"}, "actor_id": "epctex/easyjet-scraper"}, "gavilar/ingatlancom-scraper": {"id": 1219, "url": "https://apify.com/gavilar/ingatlancom-scraper/api/client/nodejs", "title": "Apify API and Ingatlan.com RealEstateHungary Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"gavilar/ingatlancom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "gavilar/ingatlancom-scraper"}, "mcdowell/home-depot": {"id": 1220, "url": "https://apify.com/mcdowell/home-depot/api/client/nodejs", "title": "Apify API and Home Depot Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"zipcode\": \"10010\",\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.homedepot.com/p/317623149\"\n        }\n    ],\n    \"maxResults\": 5,\n    \"maxConcurrency\": 2,\n    \"reviewsCount\": 10,\n    \"faq\": 4\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mcdowell/home-depot\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"faq": 4, "zipcode": "10010", "startUrls": [{"url": "https://www.homedepot.com/p/317623149"}], "maxResults": 5, "reviewsCount": 10, "maxConcurrency": 2}, "actor_id": "mcdowell/home-depot"}, "voyn/gumtree-scraper": {"id": 1221, "url": "https://apify.com/voyn/gumtree-scraper/api/client/nodejs", "title": "Apify API and Gumtree Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"gumtree_link\": [\n        \"https://www.gumtree.com/cars/uk/page1\"\n    ],\n    \"proxy_config\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"voyn/gumtree-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"gumtree_link": ["https://www.gumtree.com/cars/uk/page1"], "proxy_config": {"useApifyProxy": true}}, "actor_id": "voyn/gumtree-scraper"}, "rado.ch/snapchat-scraper": {"id": 1222, "url": "https://apify.com/rado.ch/snapchat-scraper/api/client/nodejs", "title": "Apify API and Snapchat Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"profilesInput\": [\n        \"https://www.snapchat.com/add/fcbarcelona\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"rado.ch/snapchat-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"profilesInput": ["https://www.snapchat.com/add/fcbarcelona"]}, "actor_id": "rado.ch/snapchat-scraper"}, "curious_coder/facebook-comment-scraper": {"id": 1223, "url": "https://apify.com/curious_coder/facebook-comment-scraper/api/client/nodejs", "title": "Apify API and Facebook comment scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/facebook-comment-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDelay": 3, "minDelay": 1}, "actor_id": "curious_coder/facebook-comment-scraper"}, "newbs/youtube-shorts": {"id": 1224, "url": "https://apify.com/newbs/youtube-shorts/api/client/nodejs", "title": "Apify API and Shorts Scraper: Extract YouTube Shorts Data Efficiently interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"channel\": \"netflix\",\n    \"numberOfResults\": 2\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"newbs/youtube-shorts\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"channel": "netflix", "numberOfResults": 2}, "actor_id": "newbs/youtube-shorts"}, "lukaskrivka/audio-video-converter": {"id": 1226, "url": "https://apify.com/lukaskrivka/audio-video-converter/api/client/nodejs", "title": "Apify API and Free Large Video Converter interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"outputFormat\": \"mp4\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/audio-video-converter\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"outputFormat": "mp4"}, "actor_id": "lukaskrivka/audio-video-converter"}, "voyn/chatgpt": {"id": 1227, "url": "https://apify.com/voyn/chatgpt/api/client/nodejs", "title": "Apify API and Chatgpt interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"voyn/chatgpt\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "voyn/chatgpt"}, "lexis-solutions/google-ads-scraper": {"id": 1228, "url": "https://apify.com/lexis-solutions/google-ads-scraper/api/client/nodejs", "title": "Apify API and Google Ads Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://adstransparency.google.com/advertiser/AR18135649662495883265?region=anywhere\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lexis-solutions/google-ads-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://adstransparency.google.com/advertiser/AR18135649662495883265?region=anywhere"}]}, "actor_id": "lexis-solutions/google-ads-scraper"}, "jancurn/probe-page-resources": {"id": 1229, "url": "https://apify.com/jancurn/probe-page-resources/api/client/nodejs", "title": "Apify API and Probe Page Resources interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jancurn/probe-page-resources\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jancurn/probe-page-resources"}, "jupri/tumblr-scraper": {"id": 1230, "url": "https://apify.com/jupri/tumblr-scraper/api/client/nodejs", "title": "Apify API and Tumblr Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"python\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/tumblr-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "python"}, "actor_id": "jupri/tumblr-scraper"}, "lukaskrivka/results-checker": {"id": 1231, "url": "https://apify.com/lukaskrivka/results-checker/api/client/nodejs", "title": "Apify API and Results Checker interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"functionalChecker\": () => ({\n        url: (url, item) => typeof url === 'string' && url.startsWith('http') && url.length > 10,\n        myField: (field, item) => true // this means an optional fields (always passes)\n    })\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/results-checker\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukaskrivka/results-checker"}, "yeyo/hepsiburada-scraper": {"id": 1232, "url": "https://apify.com/yeyo/hepsiburada-scraper/api/client/nodejs", "title": "Apify API and Hepsiburada Product Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"input\": [\n        {\n            \"url\": \"https://www.hepsiburada.com/iphone-11-128-gb-p-HBV0000122JCR\"\n        },\n        {\n            \"url\": \"https://www.hepsiburada.com/ara?q=iphone\"\n        }\n    ],\n    \"maxItems\": 10,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"yeyo/hepsiburada-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"input": [{"url": "https://www.hepsiburada.com/iphone-11-128-gb-p-HBV0000122JCR"}, {"url": "https://www.hepsiburada.com/ara?q=iphone"}], "maxItems": 10, "proxyConfiguration": {"useApifyProxy": false}}, "actor_id": "yeyo/hepsiburada-scraper"}, "drobnikj/check-crawler-results": {"id": 1233, "url": "https://apify.com/drobnikj/check-crawler-results/api/client/nodejs", "title": "Apify API and Scraper Results Checker interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"drobnikj/check-crawler-results\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "drobnikj/check-crawler-results"}, "natanielsantos/udemy-courses-scraper": {"id": 1234, "url": "https://apify.com/natanielsantos/udemy-courses-scraper/api/client/nodejs", "title": "Apify API and Udemy Courses Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"course_urls\": [\n        {\n            \"url\": \"https://www.udemy.com/course/python-3-do-zero-ao-avancado/\"\n        }\n    ],\n    \"category_url\": [\n        {\n            \"url\": \"https://www.udemy.com/courses/development/\"\n        }\n    ],\n    \"proxySettings\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"natanielsantos/udemy-courses-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"course_urls": [{"url": "https://www.udemy.com/course/python-3-do-zero-ao-avancado/"}], "category_url": [{"url": "https://www.udemy.com/courses/development/"}], "proxySettings": {"useApifyProxy": true}}, "actor_id": "natanielsantos/udemy-courses-scraper"}, "petr_cermak/flatten-json": {"id": 1235, "url": "https://apify.com/petr_cermak/flatten-json/api/client/nodejs", "title": "Apify API and Flatten Json interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/flatten-json\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petr_cermak/flatten-json"}, "onidivo/covid-dz": {"id": 1236, "url": "https://apify.com/onidivo/covid-dz/api/client/nodejs", "title": "Apify API and Coronavirus stats in Algeria interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"onidivo/covid-dz\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "onidivo/covid-dz"}, "enco/carsandbids": {"id": 1237, "url": "https://apify.com/enco/carsandbids/api/client/nodejs", "title": "Apify API and Cars & Bids Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"enco/carsandbids\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "enco/carsandbids"}, "mhamas/html-string-to-pdf": {"id": 1238, "url": "https://apify.com/mhamas/html-string-to-pdf/api/client/nodejs", "title": "Apify API and HTML string to PDF interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mhamas/html-string-to-pdf\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "mhamas/html-string-to-pdf"}, "drobnikj/send-crawler-results": {"id": 1239, "url": "https://apify.com/drobnikj/send-crawler-results/api/client/nodejs", "title": "Apify API and Send Legacy PhantomJS Crawler Results interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"drobnikj/send-crawler-results\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "drobnikj/send-crawler-results"}, "jupri/kickstarter": {"id": 1240, "url": "https://apify.com/jupri/kickstarter/api/client/nodejs", "title": "Apify API and Kickstarter.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/kickstarter\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jupri/kickstarter"}, "voyager/fast-booking-scraper": {"id": 1241, "url": "https://apify.com/voyager/fast-booking-scraper/api/client/nodejs", "title": "Apify API and \ud83d\ude95 Scrape hotel data fast interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"New York\",\n    \"maxItems\": 10,\n    \"sortBy\": \"distance_from_search\",\n    \"starsCountFilter\": \"any\",\n    \"currency\": \"USD\",\n    \"language\": \"en-gb\",\n    \"minMaxPrice\": \"0-999999\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"voyager/fast-booking-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"search": "New York", "sortBy": "distance_from_search", "currency": "USD", "language": "en-gb", "maxItems": 10, "minMaxPrice": "0-999999", "starsCountFilter": "any"}, "actor_id": "voyager/fast-booking-scraper"}, "gamama/topparrain-boost": {"id": 1271, "url": "https://apify.com/gamama/topparrain-boost/api/client/nodejs", "title": "Apify API and TopParrain automatic update ranking interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"proxy\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"gamama/topparrain-boost\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": false}}, "actor_id": "gamama/topparrain-boost"}, "topaz_sharingan/youtube-shorts-actor": {"id": 1242, "url": "https://apify.com/topaz_sharingan/youtube-shorts-actor/api/client/nodejs", "title": "Apify API and YouTube Shorts Actor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.youtube.com/@Apify\"\n    ],\n    \"maxLinks\": 2,\n    \"maxRequest\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"topaz_sharingan/youtube-shorts-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxLinks": 2, "startUrls": ["https://www.youtube.com/@Apify"], "maxRequest": 5}, "actor_id": "topaz_sharingan/youtube-shorts-actor"}, "dtrungtin/bloomingdales-scraper": {"id": 1243, "url": "https://apify.com/dtrungtin/bloomingdales-scraper/api/client/nodejs", "title": "Apify API and Bloomingdales Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.bloomingdales.com/\"\n        }\n    ],\n    \"maxItems\": 50,\n    \"extendOutputFunction\": ($) => { return {} },\n    \"proxyConfig\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/bloomingdales-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 50, "startUrls": [{"url": "https://www.bloomingdales.com/"}], "proxyConfig": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}}, "actor_id": "dtrungtin/bloomingdales-scraper"}, "epctex/duckduckgo-scraper": {"id": 1244, "url": "https://apify.com/epctex/duckduckgo-scraper/api/client/nodejs", "title": "Apify API and DuckDuckGo Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"DuckDuckGo\",\n    \"maxItems\": 20,\n    \"endPage\": 1,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/duckduckgo-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "search": "DuckDuckGo", "endPage": 1, "maxItems": 20}, "actor_id": "epctex/duckduckgo-scraper"}, "vanadragos/covid-19-romania": {"id": 1245, "url": "https://apify.com/vanadragos/covid-19-romania/api/client/nodejs", "title": "Apify API and Coronavirus stats in Romania interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"vanadragos/covid-19-romania\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "vanadragos/covid-19-romania"}, "bebity/google-play-api": {"id": 1246, "url": "https://apify.com/bebity/google-play-api/api/client/nodejs", "title": "Apify API and \ud83d\udd25 Google Play Api interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"lang\": \"en\",\n    \"country\": \"us\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bebity/google-play-api\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"lang": "en", "country": "us"}, "actor_id": "bebity/google-play-api"}, "jupri/firmy": {"id": 1247, "url": "https://apify.com/jupri/firmy/api/client/nodejs", "title": "Apify API and Firmy Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"Car Service\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/firmy\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "Car Service"}, "actor_id": "jupri/firmy"}, "dhrumil/propertyfinder-scraper": {"id": 1248, "url": "https://apify.com/dhrumil/propertyfinder-scraper/api/client/nodejs", "title": "Apify API and Propertyfinder Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"listUrls\": [\n        {\n            \"url\": \"https://www.propertyfinder.ae/en/search?c=1&l=11-18-21-262-348-563&ob=nd&page=1\"\n        }\n    ],\n    \"propertyUrls\": [],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dhrumil/propertyfinder-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "listUrls": [{"url": "https://www.propertyfinder.ae/en/search?c=1&l=11-18-21-262-348-563&ob=nd&page=1"}], "propertyUrls": []}, "actor_id": "dhrumil/propertyfinder-scraper"}, "lukaskrivka/check-compute-per-actor": {"id": 1279, "url": "https://apify.com/lukaskrivka/check-compute-per-actor/api/client/nodejs", "title": "Apify API and Actor Compute Units Aggregator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/check-compute-per-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukaskrivka/check-compute-per-actor"}, "alexey/google-maps-itinerary": {"id": 1249, "url": "https://apify.com/alexey/google-maps-itinerary/api/client/nodejs", "title": "Apify API and Google Maps Itinerary interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.google.com/maps/dir/Supermercado+Suma+Can+Corona,+Pla\u00e7a+Espanya,+1,+07109+Fornalutx,+Balearic+Islands/07315+Escorca,+Balearic+Islands/@39.8140347,2.2493611,186969m/am=t/data=!3m1!1e3!4m14!4m13!1m5!1m1!1s0x1297e87f07000fad:0x8d6897747207d0bf!2m2!1d2.7412712!2d39.782408!1m5!1m1!1s0x1297dd95bfd7ce11:0xb8c0845ebf64f442!2m2!1d2.7954122!2d39.8409277!3e0?hl=en\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"alexey/google-maps-itinerary\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.google.com/maps/dir/Supermercado+Suma+Can+Corona,+Pla\u00e7a+Espanya,+1,+07109+Fornalutx,+Balearic+Islands/07315+Escorca,+Balearic+Islands/@39.8140347,2.2493611,186969m/am=t/data=!3m1!1e3!4m14!4m13!1m5!1m1!1s0x1297e87f07000fad:0x8d6897747207d0bf!2m2!1d2.7412712!2d39.782408!1m5!1m1!1s0x1297dd95bfd7ce11:0xb8c0845ebf64f442!2m2!1d2.7954122!2d39.8409277!3e0?hl=en"}]}, "actor_id": "alexey/google-maps-itinerary"}, "theo/ap-news-scraper": {"id": 1250, "url": "https://apify.com/theo/ap-news-scraper/api/client/nodejs", "title": "Apify API and AP News Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.theguardian.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"theo/ap-news-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.theguardian.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "theo/ap-news-scraper"}, "apify/example-code-runner-puppeteer": {"id": 1251, "url": "https://apify.com/apify/example-code-runner-puppeteer/api/client/nodejs", "title": "Apify API and Example Code Runner (Puppeteer) interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/example-code-runner-puppeteer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/example-code-runner-puppeteer"}, "jurooravec/apify-store-scraper": {"id": 1252, "url": "https://apify.com/jurooravec/apify-store-scraper/api/client/nodejs", "title": "Apify API and Apify Store Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://apify.com/store\"\n        }\n    ],\n    \"listingFilterMaxCount\": 100,\n    \"maxRequestRetries\": 3,\n    \"maxRequestsPerMinute\": 120,\n    \"minConcurrency\": 1,\n    \"requestHandlerTimeoutSecs\": 180,\n    \"logLevel\": \"info\",\n    \"errorReportingDatasetId\": \"REPORTING\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jurooravec/apify-store-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"logLevel": "info", "startUrls": [{"url": "https://apify.com/store"}], "minConcurrency": 1, "maxRequestRetries": 3, "maxRequestsPerMinute": 120, "listingFilterMaxCount": 100, "errorReportingDatasetId": "REPORTING", "requestHandlerTimeoutSecs": 180}, "actor_id": "jurooravec/apify-store-scraper"}, "curious_coder/temu-scraper": {"id": 1253, "url": "https://apify.com/curious_coder/temu-scraper/api/client/nodejs", "title": "Apify API and Temu scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.temu.com/search_result.html?search_key=shirt&search_method=user\",\n    \"count\": 120,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/temu-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.temu.com/search_result.html?search_key=shirt&search_method=user", "count": 120, "proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "maxDelay": 3, "minDelay": 1}, "actor_id": "curious_coder/temu-scraper"}, "undrtkr984/web-scraper-task": {"id": 1254, "url": "https://apify.com/undrtkr984/web-scraper-task/api/client/nodejs", "title": "Apify API and Web Scraper Task interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"runMode\": \"DEVELOPMENT\",\n    \"startUrls\": [\n        {\n            \"url\": \"https://crawlee.dev\"\n        }\n    ],\n    \"linkSelector\": \"a[href]\",\n    \"globs\": [\n        {\n            \"glob\": \"https://crawlee.dev/*/*\"\n        }\n    ],\n    \"pseudoUrls\": [],\n    \"pageFunction\": // The function accepts a single argument: the \"context\" object.\n    // For a complete list of its properties and functions,\n    // see https://apify.com/apify/web-scraper#page-function \n    async function pageFunction(context) {\n        // This statement works as a breakpoint when you're trying to debug your code. Works only with Run mode: DEVELOPMENT!\n        // debugger; \n    \n        // jQuery is handy for finding DOM elements and extracting data from them.\n        // To use it, make sure to enable the \"Inject jQuery\" option.\n        const $ = context.jQuery;\n        const pageTitle = $('title').first().text();\n        const h1 = $('h1').first().text();\n        const first_h2 = $('h2').first().text();\n        const random_text_from_the_page = $('p').first().text();\n    \n    \n        // Print some information to actor log\n        context.log.info(`URL: ${context.request.url}, TITLE: ${pageTitle}`);\n    \n        // Manually add a new page to the queue for scraping.\n       await context.enqueueRequest({ url: 'http://www.example.com' });\n    \n        // Return an object with the data extracted from the page.\n        // It will be stored to the resulting dataset.\n        return {\n            url: context.request.url,\n            pageTitle,\n            h1,\n            first_h2,\n            random_text_from_the_page\n        };\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"initialCookies\": [],\n    \"waitUntil\": [\n        \"networkidle2\"\n    ],\n    \"preNavigationHooks\": `// We need to return array of (possibly async) functions here.\n        // The functions accept two arguments: the \"crawlingContext\" object\n        // and \"gotoOptions\".\n        [\n            async (crawlingContext, gotoOptions) => {\n                // ...\n            },\n        ]`,\n    \"postNavigationHooks\": `// We need to return array of (possibly async) functions here.\n        // The functions accept a single argument: the \"crawlingContext\" object.\n        [\n            async (crawlingContext) => {\n                // ...\n            },\n        ]`,\n    \"breakpointLocation\": \"NONE\",\n    \"customData\": {}\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"undrtkr984/web-scraper-task\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"globs": [{"glob": "https://crawlee.dev/*/*"}], "runMode": "DEVELOPMENT", "startUrls": [{"url": "https://crawlee.dev"}], "waitUntil": ["networkidle2"], "customData": {}, "pseudoUrls": [], "linkSelector": "a[href]", "initialCookies": [], "breakpointLocation": "NONE", "preNavigationHooks": "// We need to return array of (possibly async) functions here.\n        // The functions accept two arguments: the \"crawlingContext\" object\n        // and \"gotoOptions\".\n        [\n            async (crawlingContext, gotoOptions) => {\n                // ...\n            },\n        ]", "proxyConfiguration": {"useApifyProxy": true}, "postNavigationHooks": "// We need to return array of (possibly async) functions here.\n        // The functions accept a single argument: the \"crawlingContext\" object.\n        [\n            async (crawlingContext) => {\n                // ...\n            },\n        ]"}, "actor_id": "undrtkr984/web-scraper-task"}, "curious_coder/twitter-likes-scraper": {"id": 1280, "url": "https://apify.com/curious_coder/twitter-likes-scraper/api/client/nodejs", "title": "Apify API and Twitter tweet likes scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"tweetUrl\": \"https://twitter.com/elonmusk\",\n    \"minDelay\": 2,\n    \"maxDelay\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/twitter-likes-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDelay": 5, "minDelay": 2, "tweetUrl": "https://twitter.com/elonmusk"}, "actor_id": "curious_coder/twitter-likes-scraper"}, "curious_coder/tripadvisor-scraper": {"id": 1255, "url": "https://apify.com/curious_coder/tripadvisor-scraper/api/client/nodejs", "title": "Apify API and Tripadvisor Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.tripadvisor.com/Hotels-g188082-Jungfrau_Region_Bernese_Oberland_Canton_of_Bern-Hotels.html\",\n    \"startPage\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/tripadvisor-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.tripadvisor.com/Hotels-g188082-Jungfrau_Region_Bernese_Oberland_Canton_of_Bern-Hotels.html", "startPage": 1}, "actor_id": "curious_coder/tripadvisor-scraper"}, "mvolfik/hacker-news-keyword-alert": {"id": 1256, "url": "https://apify.com/mvolfik/hacker-news-keyword-alert/api/client/nodejs", "title": "Apify API and Hacker News keyword alert interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"keyword\": \"apify\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mvolfik/hacker-news-keyword-alert\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"keyword": "apify"}, "actor_id": "mvolfik/hacker-news-keyword-alert"}, "mshopik/decathlon-scraper": {"id": 1257, "url": "https://apify.com/mshopik/decathlon-scraper/api/client/nodejs", "title": "Apify API and Decathlon Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/decathlon-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/decathlon-scraper"}, "canadesk/amazon-products-reviews": {"id": 1258, "url": "https://apify.com/canadesk/amazon-products-reviews/api/client/nodejs", "title": "Apify API and Amazon Products & Reviews interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"keyword\": \"Mouse\",\n    \"asin\": \"B00P43PCI8\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/amazon-products-reviews\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"asin": "B00P43PCI8", "proxy": {"useApifyProxy": true}, "keyword": "Mouse"}, "actor_id": "canadesk/amazon-products-reviews"}, "maria.f/cnbc-scraper": {"id": 1259, "url": "https://apify.com/maria.f/cnbc-scraper/api/client/nodejs", "title": "Apify API and CNBC Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.cnbc.com/world/?region=world\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"maria.f/cnbc-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.cnbc.com/world/?region=world"}], "maxArticlesPerCrawl": 100}, "actor_id": "maria.f/cnbc-scraper"}, "jancurn/example-sitemap-cheerio": {"id": 1260, "url": "https://apify.com/jancurn/example-sitemap-cheerio/api/client/nodejs", "title": "Apify API and Example Sitemap Cheerio interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jancurn/example-sitemap-cheerio\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jancurn/example-sitemap-cheerio"}, "bebity/vinted-premium-actor": {"id": 1261, "url": "https://apify.com/bebity/vinted-premium-actor/api/client/nodejs", "title": "Apify API and \ud83d\udd25Vinted Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bebity/vinted-premium-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "bebity/vinted-premium-actor"}, "apify/python-example": {"id": 1262, "url": "https://apify.com/apify/python-example/api/client/nodejs", "title": "Apify API and Python Example interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"first_number\": 1,\n    \"second_number\": 2\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/python-example\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"first_number": 1, "second_number": 2}, "actor_id": "apify/python-example"}, "dtrungtin/aliexpress-price-scraper": {"id": 1263, "url": "https://apify.com/dtrungtin/aliexpress-price-scraper/api/client/nodejs", "title": "Apify API and AliExpress Price scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.aliexpress.com/item/4000955023443.html\"\n        }\n    ],\n    \"shipTo\": \"us\",\n    \"language\": \"en_US\",\n    \"currency\": \"USD\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/aliexpress-price-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "shipTo": "us", "currency": "USD", "language": "en_US", "startUrls": [{"url": "https://www.aliexpress.com/item/4000955023443.html"}]}, "actor_id": "dtrungtin/aliexpress-price-scraper"}, "jupri/wefunder": {"id": 1264, "url": "https://apify.com/jupri/wefunder/api/client/nodejs", "title": "Apify API and Wefunder Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/wefunder\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jupri/wefunder"}, "jupri/craft-scraper": {"id": 1265, "url": "https://apify.com/jupri/craft-scraper/api/client/nodejs", "title": "Apify API and Craft.co Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"google apple microsoft apify\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/craft-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "google apple microsoft apify"}, "actor_id": "jupri/craft-scraper"}, "mscraper/tiktok-video-downloader": {"id": 1266, "url": "https://apify.com/mscraper/tiktok-video-downloader/api/client/nodejs", "title": "Apify API and TikTok Video Downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.tiktok.com/@piotrekszumowski/video/7218831586127645978\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/tiktok-video-downloader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://www.tiktok.com/@piotrekszumowski/video/7218831586127645978"}]}, "actor_id": "mscraper/tiktok-video-downloader"}, "deadbone/odibets-standings-scraper": {"id": 1267, "url": "https://apify.com/deadbone/odibets-standings-scraper/api/client/nodejs", "title": "Apify API and Odibets Standings Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"deadbone/odibets-standings-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "deadbone/odibets-standings-scraper"}, "ivanvs/medium-scraper": {"id": 1268, "url": "https://apify.com/ivanvs/medium-scraper/api/client/nodejs", "title": "Apify API and Medium Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"tag\": \"java\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": false\n    },\n    \"maxConcurrency\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"ivanvs/medium-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"tag": "java", "maxConcurrency": 1, "proxyConfiguration": {"useApifyProxy": false}}, "actor_id": "ivanvs/medium-scraper"}, "nguyennk91/cainiao-tracking-actor": {"id": 1269, "url": "https://apify.com/nguyennk91/cainiao-tracking-actor/api/client/nodejs", "title": "Apify API and Cainiao Tracking Actor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"nguyennk91/cainiao-tracking-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "nguyennk91/cainiao-tracking-actor"}, "tiger_king/meta-threads-scraper": {"id": 1270, "url": "https://apify.com/tiger_king/meta-threads-scraper/api/client/nodejs", "title": "Apify API and Meta Threads Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"tiger_king/meta-threads-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "tiger_king/meta-threads-scraper"}, "dtrungtin/appstore-software-scraper": {"id": 1272, "url": "https://apify.com/dtrungtin/appstore-software-scraper/api/client/nodejs", "title": "Apify API and App Store Software Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://apify.com\"\n        }\n    ],\n    \"extendOutputFunction\": ($) => { return {} },\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/appstore-software-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://apify.com"}], "proxyConfig": {"useApifyProxy": true}}, "actor_id": "dtrungtin/appstore-software-scraper"}, "pocesar/download-twitter-video": {"id": 1273, "url": "https://apify.com/pocesar/download-twitter-video/api/client/nodejs", "title": "Apify API and Twitter video downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://twitter.com/apify/status/1454784661528485889\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    },\n    \"maxRequestRetries\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/download-twitter-video\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://twitter.com/apify/status/1454784661528485889"}], "maxRequestRetries": 10}, "actor_id": "pocesar/download-twitter-video"}, "curious_coder/onlyfans-post-scraper": {"id": 1274, "url": "https://apify.com/curious_coder/onlyfans-post-scraper/api/client/nodejs", "title": "Apify API and Onlyfans Post Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"source\": \"profile\",\n    \"profileUrl\": \"onlyfans\",\n    \"count\": 25,\n    \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36\",\n    \"minDelay\": 1,\n    \"maxDelay\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/onlyfans-post-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"count": 25, "source": "profile", "maxDelay": 5, "minDelay": 1, "userAgent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36", "profileUrl": "onlyfans"}, "actor_id": "curious_coder/onlyfans-post-scraper"}, "buseta/google-news-scraper": {"id": 1275, "url": "https://apify.com/buseta/google-news-scraper/api/client/nodejs", "title": "Apify API and Google News Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"keyword_list\": [\n        \"jeff bezos\"\n    ],\n    \"language\": \"en-US\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"buseta/google-news-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"language": "en-US", "keyword_list": ["jeff bezos"]}, "actor_id": "buseta/google-news-scraper"}, "yeyo/single-tweet-scraper": {"id": 1276, "url": "https://apify.com/yeyo/single-tweet-scraper/api/client/nodejs", "title": "Apify API and Fetch a Single Tweet interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"id\": \"20\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"yeyo/single-tweet-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"id": "20"}, "actor_id": "yeyo/single-tweet-scraper"}, "curious_coder/linkedin-post-reactions-scraper": {"id": 1277, "url": "https://apify.com/curious_coder/linkedin-post-reactions-scraper/api/client/nodejs", "title": "Apify API and Linkedin post reactions scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startPage\": 1,\n    \"minDelay\": 2,\n    \"maxDelay\": 5,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/linkedin-post-reactions-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "maxDelay": 5, "minDelay": 2, "startPage": 1}, "actor_id": "curious_coder/linkedin-post-reactions-scraper"}, "apify/example-call": {"id": 1278, "url": "https://apify.com/apify/example-call/api/client/nodejs", "title": "Apify API and Example Call interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/example-call\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/example-call"}, "curious_coder/shopee-data-scraper": {"id": 1281, "url": "https://apify.com/curious_coder/shopee-data-scraper/api/client/nodejs", "title": "Apify API and Shopee data scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://shopee.sg/search?keyword=earbud\",\n    \"count\": 120,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/shopee-data-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://shopee.sg/search?keyword=earbud", "count": 120, "proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "maxDelay": 3, "minDelay": 1}, "actor_id": "curious_coder/shopee-data-scraper"}, "mscraper/thefork-scraper": {"id": 1282, "url": "https://apify.com/mscraper/thefork-scraper/api/client/nodejs", "title": "Apify API and TheFork Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"British Museum\",\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"groups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/thefork-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"groups": ["RESIDENTIAL"], "useApifyProxy": true, "apifyProxyCountry": "US"}, "location": "British Museum"}, "actor_id": "mscraper/thefork-scraper"}, "petr_cermak/postgresql-insert": {"id": 1283, "url": "https://apify.com/petr_cermak/postgresql-insert/api/client/nodejs", "title": "Apify API and PostgreSQL Insert interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/postgresql-insert\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petr_cermak/postgresql-insert"}, "curious_coder/upwork-profile-scraper": {"id": 1284, "url": "https://apify.com/curious_coder/upwork-profile-scraper/api/client/nodejs", "title": "Apify API and Upwork profile scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchUrl\": \"https://www.upwork.com/search/profiles/?contract_to_hire=true&q=developer\",\n    \"startPage\": 1,\n    \"count\": 20,\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/upwork-profile-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"count": 20, "maxDelay": 3, "minDelay": 1, "searchUrl": "https://www.upwork.com/search/profiles/?contract_to_hire=true&q=developer", "startPage": 1}, "actor_id": "curious_coder/upwork-profile-scraper"}, "onidivo/covid-kr": {"id": 1285, "url": "https://apify.com/onidivo/covid-kr/api/client/nodejs", "title": "Apify API and Coronavirus stats in South Korea interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"onidivo/covid-kr\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "onidivo/covid-kr"}, "onidivo/covid-ir": {"id": 1286, "url": "https://apify.com/onidivo/covid-ir/api/client/nodejs", "title": "Apify API and Coronavirus stats in Iran interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"onidivo/covid-ir\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "onidivo/covid-ir"}, "dtrungtin/simple-realtor-scraper": {"id": 1287, "url": "https://apify.com/dtrungtin/simple-realtor-scraper/api/client/nodejs", "title": "Apify API and Simple Realtor Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.realtor.com/realestateandhomes-search/Oklahoma\"\n        }\n    ],\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/simple-realtor-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.realtor.com/realestateandhomes-search/Oklahoma"}], "proxyConfig": {"useApifyProxy": true}}, "actor_id": "dtrungtin/simple-realtor-scraper"}, "fdg43jkg33455/abc-test-sect": {"id": 1297, "url": "https://apify.com/fdg43jkg33455/abc-test-sect/api/client/nodejs", "title": "Apify API and Abc Test Sect interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"fdg43jkg33455/abc-test-sect\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "fdg43jkg33455/abc-test-sect"}, "argusapi/opensea-collection-activity-scraper": {"id": 1288, "url": "https://apify.com/argusapi/opensea-collection-activity-scraper/api/client/nodejs", "title": "Apify API and Opensea Collection Activity Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"collectionURL\": \"https://opensea.io/collection/boredapeyachtclub\",\n    \"proxies\": {\n        \"useApifyProxy\": false\n    },\n    \"scrapeEventCount\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"argusapi/opensea-collection-activity-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxies": {"useApifyProxy": false}, "collectionURL": "https://opensea.io/collection/boredapeyachtclub", "scrapeEventCount": 100}, "actor_id": "argusapi/opensea-collection-activity-scraper"}, "vaclavrut/downloadkvstorezip": {"id": 1289, "url": "https://apify.com/vaclavrut/downloadkvstorezip/api/client/nodejs", "title": "Apify API and Zip Download of KV Store interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"vaclavrut/downloadkvstorezip\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "vaclavrut/downloadkvstorezip"}, "k9tod/my-actor-1": {"id": 1290, "url": "https://apify.com/k9tod/my-actor-1/api/client/nodejs", "title": "Apify API and Test TikTok Top 10 TikTok Videos of the Day: interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.apify.com/\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"k9tod/my-actor-1\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.apify.com/"}, "actor_id": "k9tod/my-actor-1"}, "jupri/pinterest-explorer": {"id": 1291, "url": "https://apify.com/jupri/pinterest-explorer/api/client/nodejs", "title": "Apify API and Pinterest.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"@microsoft\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/pinterest-explorer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "@microsoft"}, "actor_id": "jupri/pinterest-explorer"}, "mantra_dev/spotify-playlist-and-album-website-scraper": {"id": 1292, "url": "https://apify.com/mantra_dev/spotify-playlist-and-album-website-scraper/api/client/nodejs", "title": "Apify API and Automate Spotify Playlist and Album Downloads with scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"link\": \"https://open.spotify.com/playlist/37i9dQZF1DXbYM3nMM0oPk\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mantra_dev/spotify-playlist-and-album-website-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"link": "https://open.spotify.com/playlist/37i9dQZF1DXbYM3nMM0oPk"}, "actor_id": "mantra_dev/spotify-playlist-and-album-website-scraper"}, "curious_coder/facebook-post-scraper": {"id": 1293, "url": "https://apify.com/curious_coder/facebook-post-scraper/api/client/nodejs", "title": "Apify API and Facebook group post scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"scrapeGroupPosts\",\n    \"minDelay\": 1,\n    \"maxDelay\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/facebook-post-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"action": "scrapeGroupPosts", "maxDelay": 10, "minDelay": 1}, "actor_id": "curious_coder/facebook-post-scraper"}, "medh/economic-calendar": {"id": 1294, "url": "https://apify.com/medh/economic-calendar/api/client/nodejs", "title": "Apify API and The DailyFX Economic Calendar interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"medh/economic-calendar\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "medh/economic-calendar"}, "apify/example-process-crawl-results": {"id": 1295, "url": "https://apify.com/apify/example-process-crawl-results/api/client/nodejs", "title": "Apify API and Example Process Crawl Results interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/example-process-crawl-results\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/example-process-crawl-results"}, "bebity/google-jobs-scraper": {"id": 1296, "url": "https://apify.com/bebity/google-jobs-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udd25 Google Jobs Scraper (with highlights) interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bebity/google-jobs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "bebity/google-jobs-scraper"}, "hamza.alwan/recipes-scraper": {"id": 1298, "url": "https://apify.com/hamza.alwan/recipes-scraper/api/client/nodejs", "title": "Apify API and Recipes Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": [\n        \"https://cookpad.com/us/recipes/16927787-lasagna\",\n        \"https://www.foodnetwork.com/recipes/ree-drummond/lasagna-2111724\",\n        \"https://tasty.co/recipe/the-best-lasagna\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/recipes-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"start_urls": ["https://cookpad.com/us/recipes/16927787-lasagna", "https://www.foodnetwork.com/recipes/ree-drummond/lasagna-2111724", "https://tasty.co/recipe/the-best-lasagna"]}, "actor_id": "hamza.alwan/recipes-scraper"}, "azibcepe/shopee-product-search-scraper": {"id": 1299, "url": "https://apify.com/azibcepe/shopee-product-search-scraper/api/client/nodejs", "title": "Apify API and Shopee Product Search Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchText\": \"Jaket Kulit\",\n    \"country\": \"id\",\n    \"order\": \"desc\",\n    \"sortBy\": \"relevancy\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"azibcepe/shopee-product-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"order": "desc", "sortBy": "relevancy", "country": "id", "searchText": "Jaket Kulit"}, "actor_id": "azibcepe/shopee-product-search-scraper"}, "katerinahronik/toggl-invoice-download": {"id": 1300, "url": "https://apify.com/katerinahronik/toggl-invoice-download/api/client/nodejs", "title": "Apify API and Toggl Invoice Download interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"togglUserName\": \"user@company.com\",\n    \"togglPassword\": \"password123\",\n    \"dropboxToken\": \"GCDRDJKU%$#%$(&f\",\n    \"pathToDropbox\": \"/2020_01\",\n    \"emailTo\": \"user@company.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"katerinahronik/toggl-invoice-download\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"emailTo": "user@company.com", "dropboxToken": "GCDRDJKU%$#%$(&f", "pathToDropbox": "/2020_01", "togglPassword": "password123", "togglUserName": "user@company.com"}, "actor_id": "katerinahronik/toggl-invoice-download"}, "spidoosho/notion-org-chart-generator": {"id": 1301, "url": "https://apify.com/spidoosho/notion-org-chart-generator/api/client/nodejs", "title": "Apify API and Notion Org Chart Generator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"database\": \"8b374794e9fc490fb0ea98619eb7796a\",\n    \"personName\": \"Name\",\n    \"relationName\": \"Leader\",\n    \"personDescription\": [\n        \"Role\",\n        \"Email\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"spidoosho/notion-org-chart-generator\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"database": "8b374794e9fc490fb0ea98619eb7796a", "personName": "Name", "relationName": "Leader", "personDescription": ["Role", "Email"]}, "actor_id": "spidoosho/notion-org-chart-generator"}, "qpayre/binance-leaderboard": {"id": 1302, "url": "https://apify.com/qpayre/binance-leaderboard/api/client/nodejs", "title": "Apify API and Binance Leaderboard \ud83d\ude80 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"limit\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"qpayre/binance-leaderboard\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"limit": 10}, "actor_id": "qpayre/binance-leaderboard"}, "genial_candlestand/youtube-subtitles-scraper": {"id": 1303, "url": "https://apify.com/genial_candlestand/youtube-subtitles-scraper/api/client/nodejs", "title": "Apify API and YouTube Video Subtitles (captions) Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.youtube.com/watch?v=dzjS9iYHbuU\"\n        },\n        {\n            \"url\": \"https://www.youtube.com/watch?v=yTRHomGg9uQ\"\n        }\n    ],\n    \"language\": \"en\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"genial_candlestand/youtube-subtitles-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"language": "en", "startUrls": [{"url": "https://www.youtube.com/watch?v=dzjS9iYHbuU"}, {"url": "https://www.youtube.com/watch?v=yTRHomGg9uQ"}]}, "actor_id": "genial_candlestand/youtube-subtitles-scraper"}, "mshopik/watchescom-scraper": {"id": 1304, "url": "https://apify.com/mshopik/watchescom-scraper/api/client/nodejs", "title": "Apify API and Watches.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/watchescom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/watchescom-scraper"}, "lukaskrivka/website-checker-workload": {"id": 1305, "url": "https://apify.com/lukaskrivka/website-checker-workload/api/client/nodejs", "title": "Apify API and Website Checker Workload interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"website\": \"https://apify.com\",\n    \"proxyGroups\": [\n        \"auto\",\n        \"BUYPROXIES84958\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/website-checker-workload\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"website": "https://apify.com", "proxyGroups": ["auto", "BUYPROXIES84958"]}, "actor_id": "lukaskrivka/website-checker-workload"}, "curious_coder/justdial-scraper": {"id": 1306, "url": "https://apify.com/curious_coder/justdial-scraper/api/client/nodejs", "title": "Apify API and Justdial Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchUrl\": \"https://www.justdial.com/Bangalore/Hotels-in-Bangalore-International-Airport/nct-10255012\",\n    \"startPage\": 1,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"IN\"\n    },\n    \"minDelay\": 2,\n    \"maxDelay\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/justdial-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "IN"}, "maxDelay": 5, "minDelay": 2, "searchUrl": "https://www.justdial.com/Bangalore/Hotels-in-Bangalore-International-Airport/nct-10255012", "startPage": 1}, "actor_id": "curious_coder/justdial-scraper"}, "natasha.lekh/forbes-scraper": {"id": 1307, "url": "https://apify.com/natasha.lekh/forbes-scraper/api/client/nodejs", "title": "Apify API and Forbes Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.forbes.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"natasha.lekh/forbes-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.forbes.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "natasha.lekh/forbes-scraper"}, "lexis-solutions/facebook-user-search-scraper": {"id": 1308, "url": "https://apify.com/lexis-solutions/facebook-user-search-scraper/api/client/nodejs", "title": "Apify API and Facebook User Search Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lexis-solutions/facebook-user-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lexis-solutions/facebook-user-search-scraper"}, "bibim/immowelt-scraper": {"id": 1309, "url": "https://apify.com/bibim/immowelt-scraper/api/client/nodejs", "title": "Apify API and Immowelt scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.immowelt.de/liste/berlin/haeuser/kaufen?sort=relevanz\"\n        }\n    ],\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bibim/immowelt-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.immowelt.de/liste/berlin/haeuser/kaufen?sort=relevanz"}], "proxyConfig": {"useApifyProxy": true}}, "actor_id": "bibim/immowelt-scraper"}, "ahmed-khaled/linkedin-engagement-scraper": {"id": 1310, "url": "https://apify.com/ahmed-khaled/linkedin-engagement-scraper/api/client/nodejs", "title": "Apify API and LinkedIn Profile Engagement Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.linkedin.com/in/ahmed-khaled-dev\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"ahmed-khaled/linkedin-engagement-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.linkedin.com/in/ahmed-khaled-dev"}, "actor_id": "ahmed-khaled/linkedin-engagement-scraper"}, "babak/free-angi-angie-s-list-scraper": {"id": 1311, "url": "https://apify.com/babak/free-angi-angie-s-list-scraper/api/client/nodejs", "title": "Apify API and Free Angi (Angie's List) Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"Portland\",\n    \"category\": \"air-duct-cleaning\",\n    \"directLinks\": [],\n    \"categoryLinks\": [],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"maxConcurrency\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"babak/free-angi-angie-s-list-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"category": "air-duct-cleaning", "location": "Portland", "directLinks": [], "categoryLinks": [], "maxConcurrency": 100, "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "babak/free-angi-angie-s-list-scraper"}, "apify/algolia-website-indexer": {"id": 1312, "url": "https://apify.com/apify/algolia-website-indexer/api/client/nodejs", "title": "Apify API and Algolia Website Indexer interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"http://example.com\"\n        }\n    ],\n    \"selectors\": [\n        {\n            \"key\": \"h1\",\n            \"value\": \"body\"\n        }\n    ],\n    \"requiredAttributes\": [],\n    \"additionalPageAttrs\": {}\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/algolia-website-indexer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"selectors": [{"key": "h1", "value": "body"}], "startUrls": [{"url": "http://example.com"}], "requiredAttributes": [], "additionalPageAttrs": {}}, "actor_id": "apify/algolia-website-indexer"}, "curious_coder/booking-scraper": {"id": 1313, "url": "https://apify.com/curious_coder/booking-scraper/api/client/nodejs", "title": "Apify API and Booking.com Scraper Pro interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"scrapeStays\",\n    \"scrapeStays.searchUrl\": \"https://www.booking.com/searchresults.en-gb.html?label=en-in-booking-desktop-CmH43mrsjzqEEFQPgVycoAS652796016141%3Apl%3Ata%3Ap1%3Ap2%3Aac%3Aap%3Aneg%3Afi%3Atikwd-65526620%3Alp1007772%3Ali%3Adec%3Adm&aid=2311236&ss=Lonavala%2C+India&lang=en-gb&sb=1&src_elem=sb&src=index&dest_id=-2102774&dest_type=city&checkin=2023-08-12&checkout=2023-09-01&group_adults=2&no_rooms=1&group_children=0&sb_travel_purpose=leisure&offset=0\",\n    \"startPage\": 1,\n    \"count\": 100,\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/booking-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"count": 100, "action": "scrapeStays", "maxDelay": 3, "minDelay": 1, "startPage": 1, "scrapeStays.searchUrl": "https://www.booking.com/searchresults.en-gb.html?label=en-in-booking-desktop-CmH43mrsjzqEEFQPgVycoAS652796016141%3Apl%3Ata%3Ap1%3Ap2%3Aac%3Aap%3Aneg%3Afi%3Atikwd-65526620%3Alp1007772%3Ali%3Adec%3Adm&aid=2311236&ss=Lonavala%2C+India&lang=en-gb&sb=1&src_elem=sb&src=index&dest_id=-2102774&dest_type=city&checkin=2023-08-12&checkout=2023-09-01&group_adults=2&no_rooms=1&group_children=0&sb_travel_purpose=leisure&offset=0"}, "actor_id": "curious_coder/booking-scraper"}, "mscraper/instagram-threads-post-scraper": {"id": 1314, "url": "https://apify.com/mscraper/instagram-threads-post-scraper/api/client/nodejs", "title": "Apify API and Instagram Threads Post Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.threads.net/@lewishowes/post/CuyXCSlu89_\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/instagram-threads-post-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyCountry": "US"}, "startUrls": ["https://www.threads.net/@lewishowes/post/CuyXCSlu89_"]}, "actor_id": "mscraper/instagram-threads-post-scraper"}, "natanielsantos/flipkart-reviews-scraper": {"id": 1315, "url": "https://apify.com/natanielsantos/flipkart-reviews-scraper/api/client/nodejs", "title": "Apify API and Flipkart Reviews Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": [\n        {\n            \"url\": \"https://www.flipkart.com/adidas-ampligy-m-running-shoes-men/p/itmab79cd4ce225d\"\n        }\n    ],\n    \"proxySettings\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"natanielsantos/flipkart-reviews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"start_urls": [{"url": "https://www.flipkart.com/adidas-ampligy-m-running-shoes-men/p/itmab79cd4ce225d"}], "proxySettings": {"useApifyProxy": true}}, "actor_id": "natanielsantos/flipkart-reviews-scraper"}, "curious_coder/discord-data-scraper": {"id": 1316, "url": "https://apify.com/curious_coder/discord-data-scraper/api/client/nodejs", "title": "Apify API and Discord Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"scrapeMessages\",\n    \"scrapeMessages.channelUrl\": \"https://discord.com/channels/662267976984297473/976997370132848640\",\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/discord-data-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"action": "scrapeMessages", "maxDelay": 3, "minDelay": 1, "scrapeMessages.channelUrl": "https://discord.com/channels/662267976984297473/976997370132848640"}, "actor_id": "curious_coder/discord-data-scraper"}, "misceres/h-m-scraper": {"id": 1317, "url": "https://apify.com/misceres/h-m-scraper/api/client/nodejs", "title": "Apify API and H&M Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"misceres/h-m-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "misceres/h-m-scraper"}, "useful-tools/google-lens-image-sources": {"id": 1318, "url": "https://apify.com/useful-tools/google-lens-image-sources/api/client/nodejs", "title": "Apify API and Google Lens - Image Sources interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://thumbs.dreamstime.com/z/beagle-dog-isolated-white-background-purebred-103538194.jpg\"\n        },\n        {\n            \"url\": \"https://images.megapixl.com/4976/49769857.jpg\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"useful-tools/google-lens-image-sources\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://thumbs.dreamstime.com/z/beagle-dog-isolated-white-background-purebred-103538194.jpg"}, {"url": "https://images.megapixl.com/4976/49769857.jpg"}], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "useful-tools/google-lens-image-sources"}, "petrpatek/covid-austria": {"id": 1319, "url": "https://apify.com/petrpatek/covid-austria/api/client/nodejs", "title": "Apify API and Coronavirus stats in Austria interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petrpatek/covid-austria\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petrpatek/covid-austria"}, "jupri/republic": {"id": 1320, "url": "https://apify.com/jupri/republic/api/client/nodejs", "title": "Apify API and Republic.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/republic\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jupri/republic"}, "smooth/sockjs-test": {"id": 1321, "url": "https://apify.com/smooth/sockjs-test/api/client/nodejs", "title": "Apify API and SockJS WebSocket Test interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"smooth/sockjs-test\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "smooth/sockjs-test"}, "newpo/nike-scraper": {"id": 1322, "url": "https://apify.com/newpo/nike-scraper/api/client/nodejs", "title": "Apify API and Nike Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.nike.com/w/members-save-more-sale-3re64\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"newpo/nike-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.nike.com/w/members-save-more-sale-3re64"}]}, "actor_id": "newpo/nike-scraper"}, "lukass/covid-cad": {"id": 1323, "url": "https://apify.com/lukass/covid-cad/api/client/nodejs", "title": "Apify API and Coronavirus stats in Canada interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukass/covid-cad\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukass/covid-cad"}, "babak/angi-angie-s-list-company-links-scraper": {"id": 1324, "url": "https://apify.com/babak/angi-angie-s-list-company-links-scraper/api/client/nodejs", "title": "Apify API and Angi (Angie's List) Company Links Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"Portland\",\n    \"category\": \"asbestos-removal\",\n    \"maxListings\": 1000,\n    \"categoryLinks\": [],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"maxConcurrency\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"babak/angi-angie-s-list-company-links-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"category": "asbestos-removal", "location": "Portland", "maxListings": 1000, "categoryLinks": [], "maxConcurrency": 100, "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "babak/angi-angie-s-list-company-links-scraper"}, "mtrunkat/puppeteer-promise-pool-example": {"id": 1325, "url": "https://apify.com/mtrunkat/puppeteer-promise-pool-example/api/client/nodejs", "title": "Apify API and Example Puppeteer Promise Pool interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mtrunkat/puppeteer-promise-pool-example\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "mtrunkat/puppeteer-promise-pool-example"}, "lhotanok/zalando-scraper": {"id": 1326, "url": "https://apify.com/lhotanok/zalando-scraper/api/client/nodejs", "title": "Apify API and Zalando Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.zalando.co.uk/womens-clothing/\",\n        \"https://www.zalando.co.uk/mens-clothing/\",\n        \"https://www.zalando.co.uk/childrens-clothing/\"\n    ],\n    \"maxItems\": 100,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lhotanok/zalando-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 100, "startUrls": ["https://www.zalando.co.uk/womens-clothing/", "https://www.zalando.co.uk/mens-clothing/", "https://www.zalando.co.uk/childrens-clothing/"], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "lhotanok/zalando-scraper"}, "datastorm/amazon-scraper": {"id": 1327, "url": "https://apify.com/datastorm/amazon-scraper/api/client/nodejs", "title": "Apify API and Amazon Search and Product Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"datastorm/amazon-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "datastorm/amazon-scraper"}, "juansgaitan/big-query": {"id": 1328, "url": "https://apify.com/juansgaitan/big-query/api/client/nodejs", "title": "Apify API and Big Query interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"juansgaitan/big-query\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "juansgaitan/big-query"}, "mnmkng/abort-actor-runs": {"id": 1329, "url": "https://apify.com/mnmkng/abort-actor-runs/api/client/nodejs", "title": "Apify API and Abort Actor Runs interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mnmkng/abort-actor-runs\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "mnmkng/abort-actor-runs"}, "onidivo/twilio-sms-parser": {"id": 1330, "url": "https://apify.com/onidivo/twilio-sms-parser/api/client/nodejs", "title": "Apify API and Twilio SMS Parser interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"onidivo/twilio-sms-parser\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "onidivo/twilio-sms-parser"}, "brazil-scrapers/quinto-andar-scraper": {"id": 1331, "url": "https://apify.com/brazil-scrapers/quinto-andar-scraper/api/client/nodejs", "title": "Apify API and Quinto Andar Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"https://www.quintoandar.com.br/alugar/imovel/sao-paulo-sp-brasil\",\n    \"searchType\": \"rent\",\n    \"maxItems\": 5,\n    \"maxConcurrency\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"brazil-scrapers/quinto-andar-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"location": "https://www.quintoandar.com.br/alugar/imovel/sao-paulo-sp-brasil", "maxItems": 5, "searchType": "rent", "maxConcurrency": 5}, "actor_id": "brazil-scrapers/quinto-andar-scraper"}, "cyberfly/dataset-toolbox": {"id": 1332, "url": "https://apify.com/cyberfly/dataset-toolbox/api/client/nodejs", "title": "Apify API and Dataset Toolbox interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"postProcessFun\": (item) => item\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"cyberfly/dataset-toolbox\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "cyberfly/dataset-toolbox"}, "drobnikj/mongodb-import": {"id": 1333, "url": "https://apify.com/drobnikj/mongodb-import/api/client/nodejs", "title": "Apify API and MongoDB Import interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"drobnikj/mongodb-import\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "drobnikj/mongodb-import"}, "kuaima/producthunt-scraper": {"id": 1387, "url": "https://apify.com/kuaima/producthunt-scraper/api/client/nodejs", "title": "Apify API and Product Hunt Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"kuaima/producthunt-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "kuaima/producthunt-scraper"}, "curious_coder/facebook-marketplace": {"id": 1334, "url": "https://apify.com/curious_coder/facebook-marketplace/api/client/nodejs", "title": "Apify API and Facebook marketplace scraper \u2705 FREE \u2705 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.facebook.com/marketplace/nyc/cars/\",\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/facebook-marketplace\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.facebook.com/marketplace/nyc/cars/", "proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}}, "actor_id": "curious_coder/facebook-marketplace"}, "timki.vn/mass-shopee-product-crawler": {"id": 1335, "url": "https://apify.com/timki.vn/mass-shopee-product-crawler/api/client/nodejs", "title": "Apify API and Mass Shopee Product Crawler interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"starturls\": [\n        {\n            \"url\": \"https://shopee.vn/%C3%81o-s%C6%A1-mi-nam-tr%C6%A1n-d%C3%A0i-tay-c%C3%B4ng-s%E1%BB%9F-d%C3%A1ng-%C3%B4m-H%C3%A0n-Qu%E1%BB%91c,-%C3%A1o-tr%E1%BA%AFng-nam-v%E1%BA%A3i-l%E1%BB%A5a-th%C3%A1i-cao-c%E1%BA%A5p-ch%E1%BB%91ng-nh%C4%83n-Vettino-i.61261760.1140923554\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"timki.vn/mass-shopee-product-crawler\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "starturls": [{"url": "https://shopee.vn/%C3%81o-s%C6%A1-mi-nam-tr%C6%A1n-d%C3%A0i-tay-c%C3%B4ng-s%E1%BB%9F-d%C3%A1ng-%C3%B4m-H%C3%A0n-Qu%E1%BB%91c,-%C3%A1o-tr%E1%BA%AFng-nam-v%E1%BA%A3i-l%E1%BB%A5a-th%C3%A1i-cao-c%E1%BA%A5p-ch%E1%BB%91ng-nh%C4%83n-Vettino-i.61261760.1140923554"}]}, "actor_id": "timki.vn/mass-shopee-product-crawler"}, "babak/nsfw-image-detector": {"id": 1336, "url": "https://apify.com/babak/nsfw-image-detector/api/client/nodejs", "title": "Apify API and NSFW Image Detector interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"imgUrl\": \"https://apify-image-uploads-prod.s3.amazonaws.com/Nbvb2ZAX96yhhtJQy/EQ2R5dkeB3Kizko6x-IMG_6685.png\",\n    \"directLinks\": [],\n    \"directLinksFile\": []\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"babak/nsfw-image-detector\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"imgUrl": "https://apify-image-uploads-prod.s3.amazonaws.com/Nbvb2ZAX96yhhtJQy/EQ2R5dkeB3Kizko6x-IMG_6685.png", "directLinks": [], "directLinksFile": []}, "actor_id": "babak/nsfw-image-detector"}, "harshmaur/website-traffic-generator": {"id": 1337, "url": "https://apify.com/harshmaur/website-traffic-generator/api/client/nodejs", "title": "Apify API and (New) Best Website Traffic Generator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.webscrapinghq.com\"\n    ],\n    \"multiply\": 1,\n    \"maxPageWaitSeconds\": 5,\n    \"minPageWaitSeconds\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"harshmaur/website-traffic-generator\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"multiply": 1, "startUrls": ["https://www.webscrapinghq.com"], "maxPageWaitSeconds": 5, "minPageWaitSeconds": 1}, "actor_id": "harshmaur/website-traffic-generator"}, "pocesar/actor-testing": {"id": 1338, "url": "https://apify.com/pocesar/actor-testing/api/client/nodejs", "title": "Apify API and Actor Testing interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"testSpec\": ({ it, xit, moment, _, run, expect, expectAsync, input, describe }) => {\n        (input.resource ? [\n            'beta',\n        ] : [\n            'latest',\n        ]).forEach((build) => {\n            describe(`${build} version`, () => {\n                it('test something-task', async () => {\n                    const runResult = await run({\n                        taskId: '',\n                    });\n    \n                    await expectAsync(runResult).toHaveStatus('SUCCEEDED');\n                    await expectAsync(runResult).withLog((log) => {\n                        expect(log)\n                            .withContext(runResult.format('ReferenceError'))\n                            .not.toContain('ReferenceError');\n                        expect(log)\n                            .withContext(runResult.format('TypeError'))\n                            .not.toContain('TypeError');\n                    });\n    \n                    await expectAsync(runResult).withStatistics((stats) => {\n                        expect(stats.requestsRetries)\n                            .withContext(runResult.format('Request retries'))\n                            .toBeLessThan(3);\n    \n                        expect(stats.crawlerRuntimeMillis)\n                            .withContext(runResult.format('Run time'))\n                            .toBeWithinRange(0.1 * 60000, 10 * 60000);\n                    });\n    \n                    await expectAsync(runResult).withDataset(({ dataset, info }) => {\n                        expect(info.cleanItemCount)\n                            .withContext(runResult.format('Dataset cleanItemCount'))\n                            .toBeGreaterThan(0);\n    \n                        expect(dataset.items)\n                            .withContext(runResult.format('Dataset items array'))\n                            .toBeNonEmptyArray();\n                    });\n                });\n            });\n        });\n    },\n    \"slackChannel\": \"#public-actors-tests-notifications\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/actor-testing\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"slackChannel": "#public-actors-tests-notifications"}, "actor_id": "pocesar/actor-testing"}, "drobnikj/covid-stats-france": {"id": 1339, "url": "https://apify.com/drobnikj/covid-stats-france/api/client/nodejs", "title": "Apify API and Coronavirus stats in France interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"drobnikj/covid-stats-france\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "drobnikj/covid-stats-france"}, "drobnikj/firestore-import": {"id": 1340, "url": "https://apify.com/drobnikj/firestore-import/api/client/nodejs", "title": "Apify API and Firestore Import interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"drobnikj/firestore-import\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "drobnikj/firestore-import"}, "eloquent_mountain/trustpilot-review-scraper-and-translator": {"id": 1341, "url": "https://apify.com/eloquent_mountain/trustpilot-review-scraper-and-translator/api/client/nodejs", "title": "Apify API and Trustpilot review scraper and translator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": \"https://www.trustpilot.com/review/catawiki.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"eloquent_mountain/trustpilot-review-scraper-and-translator\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"start_urls": "https://www.trustpilot.com/review/catawiki.com"}, "actor_id": "eloquent_mountain/trustpilot-review-scraper-and-translator"}, "cyberfly/covid-it": {"id": 1342, "url": "https://apify.com/cyberfly/covid-it/api/client/nodejs", "title": "Apify API and Coronavirus stats in Italy interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"cyberfly/covid-it\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "cyberfly/covid-it"}, "apify/monitoring-checker-duplication": {"id": 1343, "url": "https://apify.com/apify/monitoring-checker-duplication/api/client/nodejs", "title": "Apify API and Monitoring Checker Duplication interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/monitoring-checker-duplication\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/monitoring-checker-duplication"}, "strajk/tradeinn-tradeinn-com-scraper": {"id": 1344, "url": "https://apify.com/strajk/tradeinn-tradeinn-com-scraper/api/client/nodejs", "title": "Apify API and Tradeinn (tradeinn.com) scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"TEST\",\n    \"shop\": \"BIKEINN\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"strajk/tradeinn-tradeinn-com-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "TEST", "shop": "BIKEINN"}, "actor_id": "strajk/tradeinn-tradeinn-com-scraper"}, "surendra/airbnb-actor": {"id": 1345, "url": "https://apify.com/surendra/airbnb-actor/api/client/nodejs", "title": "Apify API and Airbnb Properties details by location search in USD interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"Prague\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"surendra/airbnb-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"location": "Prague"}, "actor_id": "surendra/airbnb-actor"}, "mscraper/zalando-scraper": {"id": 1346, "url": "https://apify.com/mscraper/zalando-scraper/api/client/nodejs", "title": "Apify API and Zalando Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://en.zalando.de/women/apple/\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"DE\",\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/zalando-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "DE"}, "startUrls": [{"url": "https://en.zalando.de/women/apple/"}]}, "actor_id": "mscraper/zalando-scraper"}, "autofacts/asos": {"id": 1347, "url": "https://apify.com/autofacts/asos/api/client/nodejs", "title": "Apify API and ASOS data spider interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.asos.com/us/men/accessories/face-coverings/cat/?cid=50036\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    },\n    \"maxConcurrency\": 20\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"autofacts/asos\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "startUrls": [{"url": "https://www.asos.com/us/men/accessories/face-coverings/cat/?cid=50036"}], "maxConcurrency": 20}, "actor_id": "autofacts/asos"}, "lexis-solutions/redfin-scraper": {"id": 1348, "url": "https://apify.com/lexis-solutions/redfin-scraper/api/client/nodejs", "title": "Apify API and Redfin Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.redfin.com/city/11203/CA/Los-Angeles/filter/property-type=house,min-price=80M,min-beds=5,min-baths=4\"\n        }\n    ],\n    \"mode\": \"buy\",\n    \"maxPages\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lexis-solutions/redfin-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "buy", "maxPages": 1, "startUrls": [{"url": "https://www.redfin.com/city/11203/CA/Los-Angeles/filter/property-type=house,min-price=80M,min-beds=5,min-baths=4"}]}, "actor_id": "lexis-solutions/redfin-scraper"}, "jupri/wiki-scraper": {"id": 1349, "url": "https://apify.com/jupri/wiki-scraper/api/client/nodejs", "title": "Apify API and MediaWiki Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"pages\": [\n        \"https://en.wikipedia.com/wiki/JavaScript\",\n        \"https://terraria.fandom.com/wiki/Bosses\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/wiki-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"pages": ["https://en.wikipedia.com/wiki/JavaScript", "https://terraria.fandom.com/wiki/Bosses"]}, "actor_id": "jupri/wiki-scraper"}, "petr_cermak/datasets-compare": {"id": 1350, "url": "https://apify.com/petr_cermak/datasets-compare/api/client/nodejs", "title": "Apify API and Datasets Compare interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/datasets-compare\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petr_cermak/datasets-compare"}, "davidrychly/covid-who-sprinklr": {"id": 1351, "url": "https://apify.com/davidrychly/covid-who-sprinklr/api/client/nodejs", "title": "Apify API and WHO Global Coronavirus stats interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"davidrychly/covid-who-sprinklr\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "davidrychly/covid-who-sprinklr"}, "lemoussel/discord-message": {"id": 1352, "url": "https://apify.com/lemoussel/discord-message/api/client/nodejs", "title": "Apify API and Discord Message interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"text\": \"Hello from 'Discord-Message' Apify actor!\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lemoussel/discord-message\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"text": "Hello from 'Discord-Message' Apify actor!"}, "actor_id": "lemoussel/discord-message"}, "epctex/tiktok-video-downloader": {"id": 1353, "url": "https://apify.com/epctex/tiktok-video-downloader/api/client/nodejs", "title": "Apify API and TikTok Video Downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.tiktok.com/@apifytech/video/7243020243059215621\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/tiktok-video-downloader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": ["https://www.tiktok.com/@apifytech/video/7243020243059215621"]}, "actor_id": "epctex/tiktok-video-downloader"}, "epctex/tiktok-comment-scraper": {"id": 1354, "url": "https://apify.com/epctex/tiktok-comment-scraper/api/client/nodejs", "title": "Apify API and Tiktok Comments Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.tiktok.com/@deborahyowa/video/7173615947603922181?is_copy_url=1&is_from_webapp=v1\",\n        \"https://www.tiktok.com/@argenby/video/7171782248281165058?is_copy_url=1&is_from_webapp=v1\",\n        \"https://www.tiktok.com/@cznburak/video/7161858444830461190?is_copy_url=1&is_from_webapp=v1\"\n    ],\n    \"endPage\": 2,\n    \"maxItems\": 40,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/tiktok-comment-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "endPage": 2, "maxItems": 40, "startUrls": ["https://www.tiktok.com/@deborahyowa/video/7173615947603922181?is_copy_url=1&is_from_webapp=v1", "https://www.tiktok.com/@argenby/video/7171782248281165058?is_copy_url=1&is_from_webapp=v1", "https://www.tiktok.com/@cznburak/video/7161858444830461190?is_copy_url=1&is_from_webapp=v1"]}, "actor_id": "epctex/tiktok-comment-scraper"}, "voyn/cars-scraper": {"id": 1355, "url": "https://apify.com/voyn/cars-scraper/api/client/nodejs", "title": "Apify API and Cars Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"carscom_link\": [\n        \"https://www.cars.com/shopping/results/?dealer_id=&keyword=&list_price_max=&list_price_min=&makes[]=&maximum_distance=all&mileage_max=&page_size=20&sort=best_match_desc&year_max=&year_min=&zip=\"\n    ],\n    \"proxy_config\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"voyn/cars-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"carscom_link": ["https://www.cars.com/shopping/results/?dealer_id=&keyword=&list_price_max=&list_price_min=&makes[]=&maximum_distance=all&mileage_max=&page_size=20&sort=best_match_desc&year_max=&year_min=&zip="], "proxy_config": {"useApifyProxy": true}}, "actor_id": "voyn/cars-scraper"}, "jupri/scrapyfy": {"id": 1417, "url": "https://apify.com/jupri/scrapyfy/api/client/nodejs", "title": "Apify API and ScrapyFy interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"spiders_code\": \"from urllib.parse import urljoin\\r\\n\\r\\n### multiple spiders can be specified\\r\\n\\r\\nclass TitleSpider(scrapy.Spider):\\r\\n\\r\\n    name = 'title_spider'\\r\\n    allowed_domains = [\\\"apify.com\\\"]\\r\\n    start_urls = [\\\"https://apify.com\\\"]\\r\\n\\r\\n    custom_settings = {\\r\\n        'REQUEST_FINGERPRINTER_IMPLEMENTATION'  : '2.7',\\r\\n        # Obey robots.txt rules\\r\\n        'ROBOTSTXT_OBEY'                        : True,\\r\\n        'DEPTH_LIMIT'                           : 2,\\r\\n        'LOG_ENABLED'                           : False,\\r\\n        #'CLOSESPIDER_PAGECOUNT'                 : 5,\\r\\n        'CLOSESPIDER_ITEMCOUNT'                 : 5,\\r\\n    }\\r\\n\\r\\n    def parse(self, response):\\r\\n        yield {\\r\\n            'url': response.url,\\r\\n            'title': response.css('title::text').extract_first(),\\r\\n        }\\r\\n        for link_href in response.css('a::attr(\\\"href\\\")'):\\r\\n            link_url = urljoin(response.url, link_href.get())\\r\\n            if link_url.startswith(('http://', 'https://')):\\r\\n                yield scrapy.Request(link_url)\",\n    \"DEFAULT_REQUEST_HEADERS\": {\n        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n        \"Accept-Language\": \"en\"\n    },\n    \"DOWNLOADER_MIDDLEWARES\": {},\n    \"DOWNLOADER_MIDDLEWARES_BASE\": {\n        \"scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware\": 100,\n        \"scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware\": 300,\n        \"scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware\": 350,\n        \"scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware\": 400,\n        \"scrapy.downloadermiddlewares.useragent.UserAgentMiddleware\": 500,\n        \"scrapy.downloadermiddlewares.retry.RetryMiddleware\": 550,\n        \"scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware\": 560,\n        \"scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware\": 580,\n        \"scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\": 590,\n        \"scrapy.downloadermiddlewares.redirect.RedirectMiddleware\": 600,\n        \"scrapy.downloadermiddlewares.cookies.CookiesMiddleware\": 700,\n        \"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\": 750,\n        \"scrapy.downloadermiddlewares.stats.DownloaderStats\": 850,\n        \"scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware\": 900\n    },\n    \"DOWNLOAD_HANDLERS\": {},\n    \"DOWNLOAD_HANDLERS_BASE\": {\n        \"data\": \"scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler\",\n        \"file\": \"scrapy.core.downloader.handlers.file.FileDownloadHandler\",\n        \"http\": \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n        \"https\": \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n        \"s3\": \"scrapy.core.downloader.handlers.s3.S3DownloadHandler\",\n        \"ftp\": \"scrapy.core.downloader.handlers.ftp.FTPDownloadHandler\"\n    },\n    \"EXTENSIONS\": {},\n    \"EXTENSIONS_BASE\": {\n        \"scrapy.extensions.corestats.CoreStats\": 0,\n        \"scrapy.extensions.telnet.TelnetConsole\": 0,\n        \"scrapy.extensions.memusage.MemoryUsage\": 0,\n        \"scrapy.extensions.memdebug.MemoryDebugger\": 0,\n        \"scrapy.extensions.closespider.CloseSpider\": 0,\n        \"scrapy.extensions.feedexport.FeedExporter\": 0,\n        \"scrapy.extensions.logstats.LogStats\": 0,\n        \"scrapy.extensions.spiderstate.SpiderState\": 0,\n        \"scrapy.extensions.throttle.AutoThrottle\": 0\n    },\n    \"FEEDS\": {},\n    \"FEED_EXPORTERS\": {},\n    \"FEED_EXPORTERS_BASE\": {\n        \"json\": \"scrapy.exporters.JsonItemExporter\",\n        \"jsonlines\": \"scrapy.exporters.JsonLinesItemExporter\",\n        \"jsonl\": \"scrapy.exporters.JsonLinesItemExporter\",\n        \"jl\": \"scrapy.exporters.JsonLinesItemExporter\",\n        \"csv\": \"scrapy.exporters.CsvItemExporter\",\n        \"xml\": \"scrapy.exporters.XmlItemExporter\",\n        \"marshal\": \"scrapy.exporters.MarshalItemExporter\",\n        \"pickle\": \"scrapy.exporters.PickleItemExporter\"\n    },\n    \"FEED_STORAGES\": {},\n    \"FEED_STORAGES_BASE\": {\n        \"\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n        \"file\": \"scrapy.extensions.feedexport.FileFeedStorage\",\n        \"ftp\": \"scrapy.extensions.feedexport.FTPFeedStorage\",\n        \"gs\": \"scrapy.extensions.feedexport.GCSFeedStorage\",\n        \"s3\": \"scrapy.extensions.feedexport.S3FeedStorage\",\n        \"stdout\": \"scrapy.extensions.feedexport.StdoutFeedStorage\"\n    },\n    \"HTTPCACHE_IGNORE_HTTP_CODES\": [],\n    \"HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS\": [],\n    \"HTTPCACHE_IGNORE_SCHEMES\": [\n        \"file\"\n    ],\n    \"ITEM_PIPELINES\": {},\n    \"ITEM_PIPELINES_BASE\": {},\n    \"MEMDEBUG_NOTIFY\": [],\n    \"MEMUSAGE_NOTIFY_MAIL\": [],\n    \"METAREFRESH_IGNORE_TAGS\": [],\n    \"RETRY_HTTP_CODES\": [\n        500,\n        502,\n        503,\n        504,\n        522,\n        524,\n        408,\n        429\n    ],\n    \"SPIDER_CONTRACTS\": {},\n    \"SPIDER_CONTRACTS_BASE\": {\n        \"scrapy.contracts.default.UrlContract\": 1,\n        \"scrapy.contracts.default.CallbackKeywordArgumentsContract\": 1,\n        \"scrapy.contracts.default.ReturnsContract\": 2,\n        \"scrapy.contracts.default.ScrapesContract\": 3\n    },\n    \"SPIDER_MIDDLEWARES\": {},\n    \"SPIDER_MIDDLEWARES_BASE\": {\n        \"scrapy.spidermiddlewares.httperror.HttpErrorMiddleware\": 50,\n        \"scrapy.spidermiddlewares.offsite.OffsiteMiddleware\": 500,\n        \"scrapy.spidermiddlewares.referer.RefererMiddleware\": 700,\n        \"scrapy.spidermiddlewares.urllength.UrlLengthMiddleware\": 800,\n        \"scrapy.spidermiddlewares.depth.DepthMiddleware\": 900\n    },\n    \"SPIDER_MODULES\": [],\n    \"STATSMAILER_RCPTS\": [],\n    \"TELNETCONSOLE_PORT\": [\n        6023,\n        6073\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/scrapyfy\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"FEEDS": {}, "EXTENSIONS": {}, "spiders_code": "from urllib.parse import urljoin\r\n\r\n### multiple spiders can be specified\r\n\r\nclass TitleSpider(scrapy.Spider):\r\n\r\n    name = 'title_spider'\r\n    allowed_domains = [\"apify.com\"]\r\n    start_urls = [\"https://apify.com\"]\r\n\r\n    custom_settings = {\r\n        'REQUEST_FINGERPRINTER_IMPLEMENTATION'  : '2.7',\r\n        # Obey robots.txt rules\r\n        'ROBOTSTXT_OBEY'                        : True,\r\n        'DEPTH_LIMIT'                           : 2,\r\n        'LOG_ENABLED'                           : False,\r\n        #'CLOSESPIDER_PAGECOUNT'                 : 5,\r\n        'CLOSESPIDER_ITEMCOUNT'                 : 5,\r\n    }\r\n\r\n    def parse(self, response):\r\n        yield {\r\n            'url': response.url,\r\n            'title': response.css('title::text').extract_first(),\r\n        }\r\n        for link_href in response.css('a::attr(\"href\")'):\r\n            link_url = urljoin(response.url, link_href.get())\r\n            if link_url.startswith(('http://', 'https://')):\r\n                yield scrapy.Request(link_url)", "FEED_STORAGES": {}, "FEED_EXPORTERS": {}, "ITEM_PIPELINES": {}, "SPIDER_MODULES": [], "EXTENSIONS_BASE": {"scrapy.extensions.logstats.LogStats": 0, "scrapy.extensions.corestats.CoreStats": 0, "scrapy.extensions.memusage.MemoryUsage": 0, "scrapy.extensions.telnet.TelnetConsole": 0, "scrapy.extensions.throttle.AutoThrottle": 0, "scrapy.extensions.closespider.CloseSpider": 0, "scrapy.extensions.feedexport.FeedExporter": 0, "scrapy.extensions.memdebug.MemoryDebugger": 0, "scrapy.extensions.spiderstate.SpiderState": 0}, "MEMDEBUG_NOTIFY": [], "RETRY_HTTP_CODES": [500, 502, 503, 504, 522, 524, 408, 429], "SPIDER_CONTRACTS": {}, "DOWNLOAD_HANDLERS": {}, "STATSMAILER_RCPTS": [], "FEED_STORAGES_BASE": {"": "scrapy.extensions.feedexport.FileFeedStorage", "gs": "scrapy.extensions.feedexport.GCSFeedStorage", "s3": "scrapy.extensions.feedexport.S3FeedStorage", "ftp": "scrapy.extensions.feedexport.FTPFeedStorage", "file": "scrapy.extensions.feedexport.FileFeedStorage", "stdout": "scrapy.extensions.feedexport.StdoutFeedStorage"}, "SPIDER_MIDDLEWARES": {}, "TELNETCONSOLE_PORT": [6023, 6073], "FEED_EXPORTERS_BASE": {"jl": "scrapy.exporters.JsonLinesItemExporter", "csv": "scrapy.exporters.CsvItemExporter", "xml": "scrapy.exporters.XmlItemExporter", "json": "scrapy.exporters.JsonItemExporter", "jsonl": "scrapy.exporters.JsonLinesItemExporter", "pickle": "scrapy.exporters.PickleItemExporter", "marshal": "scrapy.exporters.MarshalItemExporter", "jsonlines": "scrapy.exporters.JsonLinesItemExporter"}, "ITEM_PIPELINES_BASE": {}, "MEMUSAGE_NOTIFY_MAIL": [], "SPIDER_CONTRACTS_BASE": {"scrapy.contracts.default.UrlContract": 1, "scrapy.contracts.default.ReturnsContract": 2, "scrapy.contracts.default.ScrapesContract": 3, "scrapy.contracts.default.CallbackKeywordArgumentsContract": 1}, "DOWNLOADER_MIDDLEWARES": {}, "DOWNLOAD_HANDLERS_BASE": {"s3": "scrapy.core.downloader.handlers.s3.S3DownloadHandler", "ftp": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler", "data": "scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler", "file": "scrapy.core.downloader.handlers.file.FileDownloadHandler", "http": "scrapy.core.downloader.handlers.http.HTTPDownloadHandler", "https": "scrapy.core.downloader.handlers.http.HTTPDownloadHandler"}, "DEFAULT_REQUEST_HEADERS": {"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8", "Accept-Language": "en"}, "METAREFRESH_IGNORE_TAGS": [], "SPIDER_MIDDLEWARES_BASE": {"scrapy.spidermiddlewares.depth.DepthMiddleware": 900, "scrapy.spidermiddlewares.offsite.OffsiteMiddleware": 500, "scrapy.spidermiddlewares.referer.RefererMiddleware": 700, "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware": 50, "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware": 800}, "HTTPCACHE_IGNORE_SCHEMES": ["file"], "DOWNLOADER_MIDDLEWARES_BASE": {"scrapy.downloadermiddlewares.retry.RetryMiddleware": 550, "scrapy.downloadermiddlewares.stats.DownloaderStats": 850, "scrapy.downloadermiddlewares.cookies.CookiesMiddleware": 700, "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware": 300, "scrapy.downloadermiddlewares.redirect.RedirectMiddleware": 600, "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware": 560, "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware": 900, "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware": 750, "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware": 100, "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware": 500, "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware": 580, "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware": 400, "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware": 350, "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware": 590}, "HTTPCACHE_IGNORE_HTTP_CODES": [], "HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS": []}, "actor_id": "jupri/scrapyfy"}, "scrapingxpert/extract-eventbrite-online-events": {"id": 1356, "url": "https://apify.com/scrapingxpert/extract-eventbrite-online-events/api/client/nodejs", "title": "Apify API and Extract Eventbrite online events with apify scraping interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": [\n        {\n            \"url\": \"https://www.eventbrite.com/d/finland--turku/all-events/?page=1\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"scrapingxpert/extract-eventbrite-online-events\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"start_urls": [{"url": "https://www.eventbrite.com/d/finland--turku/all-events/?page=1"}]}, "actor_id": "scrapingxpert/extract-eventbrite-online-events"}, "jupri/realtor-explorer": {"id": 1357, "url": "https://apify.com/jupri/realtor-explorer/api/client/nodejs", "title": "Apify API and Realtor.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"New York, NY\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/realtor-explorer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"location": "New York, NY"}, "actor_id": "jupri/realtor-explorer"}, "mscraper/tradingview-news-scraper": {"id": 1358, "url": "https://apify.com/mscraper/tradingview-news-scraper/api/client/nodejs", "title": "Apify API and TradingView News Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"symbols\": [\n        \"NASDAQ:GOOGL\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"resultsLimit\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/tradingview-news-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyCountry": "US"}, "symbols": ["NASDAQ:GOOGL"], "resultsLimit": 3}, "actor_id": "mscraper/tradingview-news-scraper"}, "doctorfox/send-ebulksms": {"id": 1359, "url": "https://apify.com/doctorfox/send-ebulksms/api/client/nodejs", "title": "Apify API and Send EbulkSMS interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"doctorfox/send-ebulksms\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "doctorfox/send-ebulksms"}, "mscraper/vk-groups-scraper": {"id": 1360, "url": "https://apify.com/mscraper/vk-groups-scraper/api/client/nodejs", "title": "Apify API and VK Groups Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://vk.com/newyork.times\"\n        }\n    ],\n    \"resultsLimit\": 20,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/vk-groups-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://vk.com/newyork.times"}], "resultsLimit": 20}, "actor_id": "mscraper/vk-groups-scraper"}, "hamza.alwan/google-translator": {"id": 1361, "url": "https://apify.com/hamza.alwan/google-translator/api/client/nodejs", "title": "Apify API and Google Translator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"text\": \"I want to translate this text\",\n    \"sourceLanguage\": \"auto\",\n    \"targetLanguage\": \"es\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/google-translator\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"text": "I want to translate this text", "sourceLanguage": "auto", "targetLanguage": "es"}, "actor_id": "hamza.alwan/google-translator"}, "zuzka/covid-es": {"id": 1362, "url": "https://apify.com/zuzka/covid-es/api/client/nodejs", "title": "Apify API and Coronavirus stats in Spain interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"email\": \"zuzka@apify.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/covid-es\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"email": "zuzka@apify.com"}, "actor_id": "zuzka/covid-es"}, "adrian_horning/zillow-results-count": {"id": 1363, "url": "https://apify.com/adrian_horning/zillow-results-count/api/client/nodejs", "title": "Apify API and Zillow Results Count interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"Austin\",\n    \"status\": \"sale\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"adrian_horning/zillow-results-count\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"search": "Austin", "status": "sale"}, "actor_id": "adrian_horning/zillow-results-count"}, "mscraper/google-domains-scraper": {"id": 1364, "url": "https://apify.com/mscraper/google-domains-scraper/api/client/nodejs", "title": "Apify API and Google Domains Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": [\n        \"mydomainregister\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\",\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/google-domains-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "query": ["mydomainregister"]}, "actor_id": "mscraper/google-domains-scraper"}, "davidrychly/covid-sk-3": {"id": 1365, "url": "https://apify.com/davidrychly/covid-sk-3/api/client/nodejs", "title": "Apify API and Coronavirus stats in Slovakia interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"davidrychly/covid-sk-3\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "davidrychly/covid-sk-3"}, "jannovotny/failed-runs-monitor": {"id": 1366, "url": "https://apify.com/jannovotny/failed-runs-monitor/api/client/nodejs", "title": "Apify API and Failed Runs Monitor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"config\": [\n        {\n            \"actorId\": \"apify/web-scraper\",\n            \"minDatasetItems\": 0,\n            \"maxRunTimeSecs\": 3600\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jannovotny/failed-runs-monitor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"config": [{"actorId": "apify/web-scraper", "maxRunTimeSecs": 3600, "minDatasetItems": 0}]}, "actor_id": "jannovotny/failed-runs-monitor"}, "argusapi/opensea-collection-data-scraper": {"id": 1367, "url": "https://apify.com/argusapi/opensea-collection-data-scraper/api/client/nodejs", "title": "Apify API and Opensea Collection Data Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"openseaCollectionURLs\": [\n        \"https://opensea.io/collection/boredapeyachtclub\",\n        \"https://opensea.io/collection/mutant-ape-yacht-club\"\n    ],\n    \"proxies\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"argusapi/opensea-collection-data-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxies": {"useApifyProxy": false}, "openseaCollectionURLs": ["https://opensea.io/collection/boredapeyachtclub", "https://opensea.io/collection/mutant-ape-yacht-club"]}, "actor_id": "argusapi/opensea-collection-data-scraper"}, "katerinahronik/covid-sa": {"id": 1368, "url": "https://apify.com/katerinahronik/covid-sa/api/client/nodejs", "title": "Apify API and Coronavirus stats in Saudi Arabia interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"email\": \"example@example.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"katerinahronik/covid-sa\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"email": "example@example.com"}, "actor_id": "katerinahronik/covid-sa"}, "knght/google-product-price-scraper": {"id": 1369, "url": "https://apify.com/knght/google-product-price-scraper/api/client/nodejs", "title": "Apify API and Google Product Price Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"queries\": [\n        \"iPhone x\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"knght/google-product-price-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"queries": ["iPhone x"]}, "actor_id": "knght/google-product-price-scraper"}, "onidivo/google-drive": {"id": 1370, "url": "https://apify.com/onidivo/google-drive/api/client/nodejs", "title": "Apify API and Google Drive interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"onidivo/google-drive\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "onidivo/google-drive"}, "lukaskrivka/public-actor-input-example": {"id": 1371, "url": "https://apify.com/lukaskrivka/public-actor-input-example/api/client/nodejs", "title": "Apify API and Example Public Actor Input interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"extendOutputFunction\": ($) => {\n        const result = {};\n        // Uncomment to add a title to the output\n        // result.title = $('title').text().trim();\n    \n        return result;\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/public-actor-input-example\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "lukaskrivka/public-actor-input-example"}, "canadesk/google-play-store": {"id": 1372, "url": "https://apify.com/canadesk/google-play-store/api/client/nodejs", "title": "Apify API and Google Play Store interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"lang\": \"en\",\n    \"country\": \"us\",\n    \"appId\": \"com.google.android.apps.translate\",\n    \"developerId\": \"Google LLC\",\n    \"keyword\": \"translate\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/google-play-store\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"lang": "en", "appId": "com.google.android.apps.translate", "proxy": {"useApifyProxy": true}, "country": "us", "keyword": "translate", "developerId": "Google LLC"}, "actor_id": "canadesk/google-play-store"}, "buseta/sephora-scraper": {"id": 1373, "url": "https://apify.com/buseta/sephora-scraper/api/client/nodejs", "title": "Apify API and Sephora Advanced Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url_list\": [\n        \"https://www.sephora.com/product/sol-de-janeiro-mini-beija-flor-body-hair-mist-P482745\"\n    ],\n    \"proxy_config\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"buseta/sephora-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url_list": ["https://www.sephora.com/product/sol-de-janeiro-mini-beija-flor-body-hair-mist-P482745"], "proxy_config": {"useApifyProxy": true}}, "actor_id": "buseta/sephora-scraper"}, "medh/movies-quotes": {"id": 1374, "url": "https://apify.com/medh/movies-quotes/api/client/nodejs", "title": "Apify API and Movies Quotes interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"keyword\": \"Carpe diem\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"medh/movies-quotes\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"keyword": "Carpe diem"}, "actor_id": "medh/movies-quotes"}, "mshopik/cettire-scraper": {"id": 1375, "url": "https://apify.com/mshopik/cettire-scraper/api/client/nodejs", "title": "Apify API and Cettire Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/cettire-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/cettire-scraper"}, "vs4vijay/free-epic-games": {"id": 1376, "url": "https://apify.com/vs4vijay/free-epic-games/api/client/nodejs", "title": "Apify API and Free Epic Games interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"vs4vijay/free-epic-games\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "vs4vijay/free-epic-games"}, "deeper/youtube-channel-scrapper": {"id": 1377, "url": "https://apify.com/deeper/youtube-channel-scrapper/api/client/nodejs", "title": "Apify API and Youtube Channel Scrapper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": [\n        {\n            \"url\": \"https://www.youtube.com/channel/UC4f0qvPJLqGTuLyy2iHOd-g\"\n        }\n    ],\n    \"proxySettings\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"deeper/youtube-channel-scrapper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"start_urls": [{"url": "https://www.youtube.com/channel/UC4f0qvPJLqGTuLyy2iHOd-g"}], "proxySettings": {"useApifyProxy": true}}, "actor_id": "deeper/youtube-channel-scrapper"}, "jupri/tokopedia-scraper": {"id": 1378, "url": "https://apify.com/jupri/tokopedia-scraper/api/client/nodejs", "title": "Apify API and Tokopedia Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/tokopedia-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jupri/tokopedia-scraper"}, "mstephen190/s3-bucket-uploader": {"id": 1379, "url": "https://apify.com/mstephen190/s3-bucket-uploader/api/client/nodejs", "title": "Apify API and S3 Bucket Uploader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"pathName\": \"{actorName}/runs/{date}\",\n    \"fileName\": \"{runId}-{incrementor}\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mstephen190/s3-bucket-uploader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"fileName": "{runId}-{incrementor}", "pathName": "{actorName}/runs/{date}"}, "actor_id": "mstephen190/s3-bucket-uploader"}, "mshopik/jb-hi-fi-scraper": {"id": 1380, "url": "https://apify.com/mshopik/jb-hi-fi-scraper/api/client/nodejs", "title": "Apify API and JB Hi-Fi Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/jb-hi-fi-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/jb-hi-fi-scraper"}, "epctex/autotradercouk-scraper": {"id": 1381, "url": "https://apify.com/epctex/autotradercouk-scraper/api/client/nodejs", "title": "Apify API and Autotrader UK Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.autotrader.co.uk/car-search?page=2&postcode=E1%207DJ&sort=relevance\",\n        \"https://www.autotrader.co.uk/motorhome-search?postcode=E1%207DJ&berth=2&include-delivery-option=on&advertising-location=at_motorhomes&page=1\",\n        \"https://www.autotrader.co.uk/caravan-search?postcode=E1%207DJ&include-delivery-option=on&advertising-location=at_caravans&page=1\",\n        \"https://www.autotrader.co.uk/truck-search?postcode=E1%207DJ&make=DAF&include-delivery-option=on&advertising-location=at_trucks\",\n        \"https://www.autotrader.co.uk/farm-search?postcode=E1%207DJ&category=ATVS&include-delivery-option=on&advertising-location=at_farm\",\n        \"https://www.autotrader.co.uk/plant-search?postcode=E1%207DJ&category=ATTACHMENTS&include-delivery-option=on&advertising-location=at_plants\",\n        \"https://www.autotrader.co.uk/car-details/202306299089853\",\n        \"https://www.autotrader.co.uk/car-details/202304266675832?journey=FEATURED_LISTING_JOURNEY&advertising-location=at_cars&include-delivery-option=on&make=Audi&model=A6%20Saloon&page=1&postcode=E1%207DJ&fromsra\",\n        \"https://www.autotrader.co.uk/van-details/202306148523378?journey=FEATURED_LISTING_JOURNEY&advertising-location=at_vans&include-delivery-option=on&page=1&postcode=E1%207DJ&fromsra\",\n        \"https://www.autotrader.co.uk/bike-details/202303074975713?journey=FEATURED_LISTING_JOURNEY&advertising-location=at_bikes&include-delivery-option=on&page=1&postcode=E1%207DJ&fromsra\",\n        \"https://www.autotrader.co.uk/motorhome-details/202212162575381?journey=FEATURED_LISTING_JOURNEY&advertising-location=at_motorhomes&include-delivery-option=on&page=1&postcode=E1%207DJ&fromsra\",\n        \"https://www.autotrader.co.uk/caravan-details/202306138460879?advertising-location=at_caravans&include-delivery-option=on&page=1&postcode=E1%207DJ&fromsra\",\n        \"https://www.autotrader.co.uk/truck-details/202306128433310?journey=FEATURED_LISTING_JOURNEY&advertising-location=at_trucks&include-delivery-option=on&postcode=E1%207DJ&fromsra\",\n        \"https://www.autotrader.co.uk/plant-machinery-details/202304276732947?journey=FEATURED_LISTING_JOURNEY&advertising-location=at_plants&include-delivery-option=on&postcode=E1%207DJ&fromsra\",\n        \"https://www.autotrader.co.uk/farm-machinery-details/202305107209361?journey=YOU_MAY_ALSO_LIKE_JOURNEY&advertising-location=at_farm&include-delivery-option=on&postcode=E1%207DJ&fromsra\"\n    ],\n    \"maxItems\": 20,\n    \"endPage\": 1,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/autotradercouk-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "endPage": 1, "maxItems": 20, "startUrls": ["https://www.autotrader.co.uk/car-search?page=2&postcode=E1%207DJ&sort=relevance", "https://www.autotrader.co.uk/motorhome-search?postcode=E1%207DJ&berth=2&include-delivery-option=on&advertising-location=at_motorhomes&page=1", "https://www.autotrader.co.uk/caravan-search?postcode=E1%207DJ&include-delivery-option=on&advertising-location=at_caravans&page=1", "https://www.autotrader.co.uk/truck-search?postcode=E1%207DJ&make=DAF&include-delivery-option=on&advertising-location=at_trucks", "https://www.autotrader.co.uk/farm-search?postcode=E1%207DJ&category=ATVS&include-delivery-option=on&advertising-location=at_farm", "https://www.autotrader.co.uk/plant-search?postcode=E1%207DJ&category=ATTACHMENTS&include-delivery-option=on&advertising-location=at_plants", "https://www.autotrader.co.uk/car-details/202306299089853", "https://www.autotrader.co.uk/car-details/202304266675832?journey=FEATURED_LISTING_JOURNEY&advertising-location=at_cars&include-delivery-option=on&make=Audi&model=A6%20Saloon&page=1&postcode=E1%207DJ&fromsra", "https://www.autotrader.co.uk/van-details/202306148523378?journey=FEATURED_LISTING_JOURNEY&advertising-location=at_vans&include-delivery-option=on&page=1&postcode=E1%207DJ&fromsra", "https://www.autotrader.co.uk/bike-details/202303074975713?journey=FEATURED_LISTING_JOURNEY&advertising-location=at_bikes&include-delivery-option=on&page=1&postcode=E1%207DJ&fromsra", "https://www.autotrader.co.uk/motorhome-details/202212162575381?journey=FEATURED_LISTING_JOURNEY&advertising-location=at_motorhomes&include-delivery-option=on&page=1&postcode=E1%207DJ&fromsra", "https://www.autotrader.co.uk/caravan-details/202306138460879?advertising-location=at_caravans&include-delivery-option=on&page=1&postcode=E1%207DJ&fromsra", "https://www.autotrader.co.uk/truck-details/202306128433310?journey=FEATURED_LISTING_JOURNEY&advertising-location=at_trucks&include-delivery-option=on&postcode=E1%207DJ&fromsra", "https://www.autotrader.co.uk/plant-machinery-details/202304276732947?journey=FEATURED_LISTING_JOURNEY&advertising-location=at_plants&include-delivery-option=on&postcode=E1%207DJ&fromsra", "https://www.autotrader.co.uk/farm-machinery-details/202305107209361?journey=YOU_MAY_ALSO_LIKE_JOURNEY&advertising-location=at_farm&include-delivery-option=on&postcode=E1%207DJ&fromsra"]}, "actor_id": "epctex/autotradercouk-scraper"}, "perci/zappos-scraper": {"id": 1382, "url": "https://apify.com/perci/zappos-scraper/api/client/nodejs", "title": "Apify API and Zappos.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"drinkware\",\n    \"proxyConfig\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"perci/zappos-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"search": "drinkware", "proxyConfig": {"useApifyProxy": true, "apifyProxyCountry": "US"}}, "actor_id": "perci/zappos-scraper"}, "happyco/test": {"id": 1383, "url": "https://apify.com/happyco/test/api/client/nodejs", "title": "Apify API and Test JP interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"happyco/test\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "happyco/test"}, "dtrungtin/lazada-scraper": {"id": 1384, "url": "https://apify.com/dtrungtin/lazada-scraper/api/client/nodejs", "title": "Apify API and Lazada Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.lazada.com.ph/shop-mobiles-tablets/\"\n        }\n    ],\n    \"maxPages\": 10,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/lazada-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxPages": 10, "startUrls": [{"url": "https://www.lazada.com.ph/shop-mobiles-tablets/"}], "proxyConfig": {"useApifyProxy": true}}, "actor_id": "dtrungtin/lazada-scraper"}, "spiralcrew/hunter-domain-search": {"id": 1385, "url": "https://apify.com/spiralcrew/hunter-domain-search/api/client/nodejs", "title": "Apify API and Hunter.io Domain Search interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"api_key\": \"fill in your API key\",\n    \"domain\": \"stripe.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"spiralcrew/hunter-domain-search\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"domain": "stripe.com", "api_key": "fill in your API key"}, "actor_id": "spiralcrew/hunter-domain-search"}, "datastorm/aliexpress-data-scraper": {"id": 1386, "url": "https://apify.com/datastorm/aliexpress-data-scraper/api/client/nodejs", "title": "Apify API and AliExpress Data Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.aliexpress.com/item/32940810951.html\"\n        },\n        {\n            \"url\": \"https://aliexpress.com/category/100003109/women-clothing.html\"\n        }\n    ],\n    \"maxItems\": 10,\n    \"language\": \"en_US\",\n    \"shippingTo\": \"us\",\n    \"currency\": \"USD\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"datastorm/aliexpress-data-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"currency": "USD", "language": "en_US", "maxItems": 10, "startUrls": [{"url": "https://www.aliexpress.com/item/32940810951.html"}, {"url": "https://aliexpress.com/category/100003109/women-clothing.html"}], "shippingTo": "us"}, "actor_id": "datastorm/aliexpress-data-scraper"}, "hamza.alwan/youtube-downloader": {"id": 1388, "url": "https://apify.com/hamza.alwan/youtube-downloader/api/client/nodejs", "title": "Apify API and Youtube to MP3 Downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"youtubeUrls\": [\n        {\n            \"url\": \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n        }\n    ],\n    \"maxRequestRetries\": 25,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/youtube-downloader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"youtubeUrls": [{"url": "https://www.youtube.com/watch?v=dQw4w9WgXcQ"}], "maxRequestRetries": 25, "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "hamza.alwan/youtube-downloader"}, "theo/washington-post-scraper": {"id": 1389, "url": "https://apify.com/theo/washington-post-scraper/api/client/nodejs", "title": "Apify API and Washington Post Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.washingtonpost.com/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"theo/washington-post-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.washingtonpost.com/"}], "maxArticlesPerCrawl": 100}, "actor_id": "theo/washington-post-scraper"}, "lukaskrivka/log-scanner": {"id": 1390, "url": "https://apify.com/lukaskrivka/log-scanner/api/client/nodejs", "title": "Apify API and Log Scanner interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"runIdsOrUrls\": [\n        \"ILnaIxL36Qd9tLgUh\",\n        \"https://api.apify.com/v2/logs/ILnaIxL36Qd9tLgUh\"\n    ],\n    \"regexes\": [\n        \"Error\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/log-scanner\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"regexes": ["Error"], "runIdsOrUrls": ["ILnaIxL36Qd9tLgUh", "https://api.apify.com/v2/logs/ILnaIxL36Qd9tLgUh"]}, "actor_id": "lukaskrivka/log-scanner"}, "dlandberg/greenhouse": {"id": 1391, "url": "https://apify.com/dlandberg/greenhouse/api/client/nodejs", "title": "Apify API and Greenhouse interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"companies\": [\n        \"affirm\",\n        \"okta\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dlandberg/greenhouse\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"companies": ["affirm", "okta"]}, "actor_id": "dlandberg/greenhouse"}, "bibim/scraper": {"id": 1392, "url": "https://apify.com/bibim/scraper/api/client/nodejs", "title": "Apify API and ss.lv scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.ss.lv/lv/transport/cars/land-rover/freelander/\"\n        }\n    ],\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bibim/scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.ss.lv/lv/transport/cars/land-rover/freelander/"}], "proxyConfig": {"useApifyProxy": true}}, "actor_id": "bibim/scraper"}, "mshopik/poly-and-bark-scraper": {"id": 1393, "url": "https://apify.com/mshopik/poly-and-bark-scraper/api/client/nodejs", "title": "Apify API and Poly Bark Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/poly-and-bark-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/poly-and-bark-scraper"}, "mscraper/chewy-scraper": {"id": 1394, "url": "https://apify.com/mscraper/chewy-scraper/api/client/nodejs", "title": "Apify API and Chewy Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.chewy.com/b/plant-care-909\"\n        },\n        {\n            \"url\": \"https://www.chewy.com/multipet-lamb-chop-squeaky-plush-dog/dp/52997\"\n        }\n    ],\n    \"resultsLimit\": 36,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/chewy-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://www.chewy.com/b/plant-care-909"}, {"url": "https://www.chewy.com/multipet-lamb-chop-squeaky-plush-dog/dp/52997"}], "resultsLimit": 36}, "actor_id": "mscraper/chewy-scraper"}, "david.lukac/daily-mail-scraper": {"id": 1395, "url": "https://apify.com/david.lukac/daily-mail-scraper/api/client/nodejs", "title": "Apify API and Daily Mail Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.dailymail.co.uk/home/index.html\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"david.lukac/daily-mail-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.dailymail.co.uk/home/index.html"}], "maxArticlesPerCrawl": 100}, "actor_id": "david.lukac/daily-mail-scraper"}, "petr_cermak/execution-to-xlsx": {"id": 1396, "url": "https://apify.com/petr_cermak/execution-to-xlsx/api/client/nodejs", "title": "Apify API and Execution To Xlsx interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/execution-to-xlsx\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petr_cermak/execution-to-xlsx"}, "bebity/apple-store-api": {"id": 1397, "url": "https://apify.com/bebity/apple-store-api/api/client/nodejs", "title": "Apify API and \ud83d\udd25 Apple Store Api interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"getApps\",\n    \"lang\": \"en\",\n    \"country\": \"us\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bebity/apple-store-api\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"lang": "en", "action": "getApps", "country": "us"}, "actor_id": "bebity/apple-store-api"}, "equidem/price-detector-experimental": {"id": 1398, "url": "https://apify.com/equidem/price-detector-experimental/api/client/nodejs", "title": "Apify API and Price Detector (Experimental) interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.directcosmetics.com/shop/product/79990\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"equidem/price-detector-experimental\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.directcosmetics.com/shop/product/79990"}, "actor_id": "equidem/price-detector-experimental"}, "54skyxenon/sign-hack-actor": {"id": 1399, "url": "https://apify.com/54skyxenon/sign-hack-actor/api/client/nodejs", "title": "Apify API and Sign Friendly interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"word\": \"university\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"54skyxenon/sign-hack-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"word": "university"}, "actor_id": "54skyxenon/sign-hack-actor"}, "qpayre/traderwagon-leaderboard-scraper": {"id": 1400, "url": "https://apify.com/qpayre/traderwagon-leaderboard-scraper/api/client/nodejs", "title": "Apify API and TraderWagon Leaderboard Scraper \ud83d\ude80 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"limit\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"qpayre/traderwagon-leaderboard-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"limit": 10}, "actor_id": "qpayre/traderwagon-leaderboard-scraper"}, "bebity/binance-leaderboard-scraper": {"id": 1401, "url": "https://apify.com/bebity/binance-leaderboard-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udd25 Binance leaderboard scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bebity/binance-leaderboard-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "bebity/binance-leaderboard-scraper"}, "adrian_horning/best-angi-list-scraper": {"id": 1402, "url": "https://apify.com/adrian_horning/best-angi-list-scraper/api/client/nodejs", "title": "Apify API and Best Angi List Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"cities\": [\n        \"Canton\"\n    ],\n    \"state\": \"Ohio\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"adrian_horning/best-angi-list-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"state": "Ohio", "cities": ["Canton"]}, "actor_id": "adrian_horning/best-angi-list-scraper"}, "lukass/covid-jap": {"id": 1432, "url": "https://apify.com/lukass/covid-jap/api/client/nodejs", "title": "Apify API and Coronavirus stats in Japan interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukass/covid-jap\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukass/covid-jap"}, "katerinahronik/car-accidents-statistics-cz": {"id": 1403, "url": "https://apify.com/katerinahronik/car-accidents-statistics-cz/api/client/nodejs", "title": "Apify API and Car Accidents Statistics Cz interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"dateFrom\": \"2020-03-01\",\n    \"dateTo\": \"2020-03-31\",\n    \"areaCode\": \"3018\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"katerinahronik/car-accidents-statistics-cz\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"dateTo": "2020-03-31", "areaCode": "3018", "dateFrom": "2020-03-01"}, "actor_id": "katerinahronik/car-accidents-statistics-cz"}, "adrian_horning/free-google-serp-first-page": {"id": 1404, "url": "https://apify.com/adrian_horning/free-google-serp-first-page/api/client/nodejs", "title": "Apify API and FREE Google SERP First Page interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"Who is the web scraping guy?\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"adrian_horning/free-google-serp-first-page\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "Who is the web scraping guy?"}, "actor_id": "adrian_horning/free-google-serp-first-page"}, "apify/example-counter": {"id": 1405, "url": "https://apify.com/apify/example-counter/api/client/nodejs", "title": "Apify API and Example Counter interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/example-counter\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/example-counter"}, "jupri/get-your-guide-scraper": {"id": 1406, "url": "https://apify.com/jupri/get-your-guide-scraper/api/client/nodejs", "title": "Apify API and Get-Your-Guide Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/get-your-guide-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jupri/get-your-guide-scraper"}, "mscraper/wayfair-scraper": {"id": 1407, "url": "https://apify.com/mscraper/wayfair-scraper/api/client/nodejs", "title": "Apify API and Wayfair Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.wayfair.com/furniture/sb0/kitchen-dining-tables-c46129.html\"\n        },\n        {\n            \"url\": \"https://www.wayfair.com/bed-bath/pdp/ebern-designs-wynton-microfiber-reversible-3-piece-comforter-set-w002610239.html\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"resultsLimit\": 50\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/wayfair-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyCountry": "US"}, "startUrls": [{"url": "https://www.wayfair.com/furniture/sb0/kitchen-dining-tables-c46129.html"}, {"url": "https://www.wayfair.com/bed-bath/pdp/ebern-designs-wynton-microfiber-reversible-3-piece-comforter-set-w002610239.html"}], "resultsLimit": 50}, "actor_id": "mscraper/wayfair-scraper"}, "mshopik/gymshark-us-scraper": {"id": 1408, "url": "https://apify.com/mshopik/gymshark-us-scraper/api/client/nodejs", "title": "Apify API and Gymshark US Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/gymshark-us-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/gymshark-us-scraper"}, "canadesk/seo-site-checkup": {"id": 1409, "url": "https://apify.com/canadesk/seo-site-checkup/api/client/nodejs", "title": "Apify API and SEO Site Checkup interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://apify.com\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/seo-site-checkup\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://apify.com", "proxy": {"useApifyProxy": true}}, "actor_id": "canadesk/seo-site-checkup"}, "jupri/mastodon-scraper": {"id": 1441, "url": "https://apify.com/jupri/mastodon-scraper/api/client/nodejs", "title": "Apify API and Mastodon Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/mastodon-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jupri/mastodon-scraper"}, "mcdowell/google-maps-route-scraper": {"id": 1410, "url": "https://apify.com/mcdowell/google-maps-route-scraper/api/client/nodejs", "title": "Apify API and Google Maps Routes interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.google.com/maps/dir/Orlando,+FL,+USA/Oak+Ridge/@28.50464,-81.4524678,13z/am=t/data=!4m13!4m12!1m5!1m1!1s0x88e773d8fecdbc77:0xac3b2063ca5bf9e!2m2!1d-81.3789269!2d28.5383832!1m5!1m1!1s0x88e77c21b7cf3a27:0xda788a24a75ae598!2m2!1d-81.4245155!2d28.4711157\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mcdowell/google-maps-route-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.google.com/maps/dir/Orlando,+FL,+USA/Oak+Ridge/@28.50464,-81.4524678,13z/am=t/data=!4m13!4m12!1m5!1m1!1s0x88e773d8fecdbc77:0xac3b2063ca5bf9e!2m2!1d-81.3789269!2d28.5383832!1m5!1m1!1s0x88e77c21b7cf3a27:0xda788a24a75ae598!2m2!1d-81.4245155!2d28.4711157"}]}, "actor_id": "mcdowell/google-maps-route-scraper"}, "petr_cermak/galaxus-scraper": {"id": 1411, "url": "https://apify.com/petr_cermak/galaxus-scraper/api/client/nodejs", "title": "Apify API and Galaxus API Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.galaxus.at/de/s1/producttype/ereader-1138\"\n        }\n    ],\n    \"limitBypass\": \"filters\",\n    \"processOutputFunction\": async ({item, baseUrl, categories, formatDefault}) => {\n        return formatDefault(item);\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/galaxus-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.galaxus.at/de/s1/producttype/ereader-1138"}], "limitBypass": "filters", "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "petr_cermak/galaxus-scraper"}, "jupri/trulia": {"id": 1412, "url": "https://apify.com/jupri/trulia/api/client/nodejs", "title": "Apify API and Trulia Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"Alaska\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/trulia\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"location": "Alaska"}, "actor_id": "jupri/trulia"}, "mscraper/wellfound-startups-scraper": {"id": 1413, "url": "https://apify.com/mscraper/wellfound-startups-scraper/api/client/nodejs", "title": "Apify API and Wellfound Startups Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"cookie\": \"\",\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\",\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/wellfound-startups-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "cookie": ""}, "actor_id": "mscraper/wellfound-startups-scraper"}, "strajk/bike-discount-bike-discount-de-scraper": {"id": 1414, "url": "https://apify.com/strajk/bike-discount-bike-discount-de-scraper/api/client/nodejs", "title": "Apify API and Bike Discount (bike-discount.de) scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"TEST\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"strajk/bike-discount-bike-discount-de-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "TEST"}, "actor_id": "strajk/bike-discount-bike-discount-de-scraper"}, "dhrumil/onthemarket-scraper": {"id": 1415, "url": "https://apify.com/dhrumil/onthemarket-scraper/api/client/nodejs", "title": "Apify API and Onthemarket Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"listUrls\": [\n        {\n            \"url\": \"https://www.onthemarket.com/for-sale/property/ba1-2lr/?radius=0.5\"\n        }\n    ],\n    \"propertyUrls\": [],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dhrumil/onthemarket-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "listUrls": [{"url": "https://www.onthemarket.com/for-sale/property/ba1-2lr/?radius=0.5"}], "propertyUrls": []}, "actor_id": "dhrumil/onthemarket-scraper"}, "mshopik/mita-sneakers-scraper": {"id": 1416, "url": "https://apify.com/mshopik/mita-sneakers-scraper/api/client/nodejs", "title": "Apify API and mita sneakers Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/mita-sneakers-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/mita-sneakers-scraper"}, "strajk/cyklobazar-cyklobazar-cz-scraper-rss": {"id": 1418, "url": "https://apify.com/strajk/cyklobazar-cyklobazar-cz-scraper-rss/api/client/nodejs", "title": "Apify API and Cyklobazar (cyklobazar.cz) scraper RSS interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urls\": [\n        {\n            \"url\": \"https://www.cyklobazar.cz/vsechny-kategorie?q=canyon\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"strajk/cyklobazar-cyklobazar-cz-scraper-rss\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": [{"url": "https://www.cyklobazar.cz/vsechny-kategorie?q=canyon"}]}, "actor_id": "strajk/cyklobazar-cyklobazar-cz-scraper-rss"}, "mscraper/audible-scraper": {"id": 1419, "url": "https://apify.com/mscraper/audible-scraper/api/client/nodejs", "title": "Apify API and Audible Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.audible.com/adblbestsellers?overrideBaseCountry=true&ipRedirectOverride=true\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"maxPageNumber\": 0\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/audible-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyCountry": "US"}, "startUrls": [{"url": "https://www.audible.com/adblbestsellers?overrideBaseCountry=true&ipRedirectOverride=true"}], "maxPageNumber": 0}, "actor_id": "mscraper/audible-scraper"}, "katerinahronik/medium-response-notifications": {"id": 1420, "url": "https://apify.com/katerinahronik/medium-response-notifications/api/client/nodejs", "title": "Apify API and Medium Notifications interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"articleUrl\": [\n        \"https://blog.apify.com/get-custom-web-scraping-solutions-from-certified-developers-on-apify-marketplace-f1fe0302bcd\"\n    ],\n    \"articleId\": [\n        \"cd3220e9abf5\",\n        \"50417cc20994\"\n    ],\n    \"channel\": \"#general\",\n    \"emailTo\": \"user@company.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"katerinahronik/medium-response-notifications\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"channel": "#general", "emailTo": "user@company.com", "articleId": ["cd3220e9abf5", "50417cc20994"], "articleUrl": ["https://blog.apify.com/get-custom-web-scraping-solutions-from-certified-developers-on-apify-marketplace-f1fe0302bcd"]}, "actor_id": "katerinahronik/medium-response-notifications"}, "hanatsai/us-news-scraper": {"id": 1421, "url": "https://apify.com/hanatsai/us-news-scraper/api/client/nodejs", "title": "Apify API and US News Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.usnews.com/news?top_nav_News\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hanatsai/us-news-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.usnews.com/news?top_nav_News"}], "maxArticlesPerCrawl": 100}, "actor_id": "hanatsai/us-news-scraper"}, "medh/stack-overflow-users-urls": {"id": 1422, "url": "https://apify.com/medh/stack-overflow-users-urls/api/client/nodejs", "title": "Apify API and Stack Overflow Users URLs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"medh/stack-overflow-users-urls\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "medh/stack-overflow-users-urls"}, "pocesar/spawn-workers": {"id": 1423, "url": "https://apify.com/pocesar/spawn-workers/api/client/nodejs", "title": "Apify API and Spawn Workers interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"workerInput\": {},\n    \"workerOptions\": {},\n    \"workerCount\": 2\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/spawn-workers\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"workerCount": 2, "workerInput": {}, "workerOptions": {}}, "actor_id": "pocesar/spawn-workers"}, "vaclavrut/website-checker-starter": {"id": 1424, "url": "https://apify.com/vaclavrut/website-checker-starter/api/client/nodejs", "title": "Apify API and Website checker starter interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"http://idnes.cz/\",\n        \"https://news.ycombinator.com/\"\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"vaclavrut/website-checker-starter\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": ["http://idnes.cz/", "https://news.ycombinator.com/"], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "vaclavrut/website-checker-starter"}, "curious_coder/moz-domain-scraper": {"id": 1425, "url": "https://apify.com/curious_coder/moz-domain-scraper/api/client/nodejs", "title": "Apify API and Moz domain authority scraper and API interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urls\": [\n        \"https://apify.com\"\n    ],\n    \"minDelay\": 1,\n    \"maxDelay\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/moz-domain-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": ["https://apify.com"], "maxDelay": 5, "minDelay": 1}, "actor_id": "curious_coder/moz-domain-scraper"}, "newpo/costco-scraper": {"id": 1426, "url": "https://apify.com/newpo/costco-scraper/api/client/nodejs", "title": "Apify API and Costco Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.costco.com/televisions.html\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"newpo/costco-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.costco.com/televisions.html"}]}, "actor_id": "newpo/costco-scraper"}, "hamza.alwan/flickr-scraper": {"id": 1427, "url": "https://apify.com/hamza.alwan/flickr-scraper/api/client/nodejs", "title": "Apify API and Flickr Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchUrls\": [],\n    \"searchQueries\": [\n        \"Cat\"\n    ],\n    \"desiredMediaCount\": 100,\n    \"sortType\": \"RELEVANCE\",\n    \"licenseType\": \"ALL\",\n    \"mediaMinSize\": \"SMALL\",\n    \"dateSearchType\": \"DATE_TAKEN\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/flickr-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"sortType": "RELEVANCE", "searchUrls": [], "licenseType": "ALL", "mediaMinSize": "SMALL", "searchQueries": ["Cat"], "dateSearchType": "DATE_TAKEN", "desiredMediaCount": 100, "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "hamza.alwan/flickr-scraper"}, "mscraper/hotels-scraper": {"id": 1428, "url": "https://apify.com/mscraper/hotels-scraper/api/client/nodejs", "title": "Apify API and Hotels.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"Berlin\",\n    \"rooms\": 1,\n    \"adults\": 1,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\",\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    },\n    \"resultsLimit\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/hotels-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "rooms": 1, "adults": 1, "search": "Berlin", "resultsLimit": 10}, "actor_id": "mscraper/hotels-scraper"}, "lexis-solutions/reed-co-uk-scraper": {"id": 1429, "url": "https://apify.com/lexis-solutions/reed-co-uk-scraper/api/client/nodejs", "title": "Apify API and Reed.co.uk Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.reed.co.uk/jobs/senior-data-engineer/49999439\"\n        }\n    ],\n    \"jobType\": [\n        \"perm\",\n        \"temp\",\n        \"fulltime\",\n        \"contract\",\n        \"parttime\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lexis-solutions/reed-co-uk-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"jobType": ["perm", "temp", "fulltime", "contract", "parttime"], "startUrls": [{"url": "https://www.reed.co.uk/jobs/senior-data-engineer/49999439"}]}, "actor_id": "lexis-solutions/reed-co-uk-scraper"}, "vojtam/screenshots-from-html": {"id": 1430, "url": "https://apify.com/vojtam/screenshots-from-html/api/client/nodejs", "title": "Apify API and Screenshots from HTML interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"html\": \"\",\n    \"kvStoreId\": \"\",\n    \"kvStorePrefix\": \"\",\n    \"datasetId\": \"\",\n    \"datasetHtmlField\": \"\",\n    \"datasetKeyFields\": []\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"vojtam/screenshots-from-html\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"html": "", "datasetId": "", "kvStoreId": "", "kvStorePrefix": "", "datasetHtmlField": "", "datasetKeyFields": []}, "actor_id": "vojtam/screenshots-from-html"}, "rikunk/bet-prediction-scraper": {"id": 1431, "url": "https://apify.com/rikunk/bet-prediction-scraper/api/client/nodejs", "title": "Apify API and Daily Bet Prediction Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"rikunk/bet-prediction-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "rikunk/bet-prediction-scraper"}, "scrapingxpert/houzz": {"id": 1433, "url": "https://apify.com/scrapingxpert/houzz/api/client/nodejs", "title": "Apify API and houzz interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": [\n        {\n            \"url\": \"https://www.houzz.com/professionals/hznb/probr3-bo~t_34257~r_4930956~ps_4002-4003-4004\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"scrapingxpert/houzz\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"start_urls": [{"url": "https://www.houzz.com/professionals/hznb/probr3-bo~t_34257~r_4930956~ps_4002-4003-4004"}]}, "actor_id": "scrapingxpert/houzz"}, "canadesk/trustpilot": {"id": 1434, "url": "https://apify.com/canadesk/trustpilot/api/client/nodejs", "title": "Apify API and Trustpilot interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"website\": \"www.expensify.com\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/trustpilot\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "website": "www.expensify.com"}, "actor_id": "canadesk/trustpilot"}, "dtrungtin/gpuchecker-scraper": {"id": 1435, "url": "https://apify.com/dtrungtin/gpuchecker-scraper/api/client/nodejs", "title": "Apify API and gpu, cpu, quality, resolution, game, fps interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"gpuIncludeRegex\": [\n        \"460\"\n    ],\n    \"cpuIncludeRegex\": [\n        \"2200g\"\n    ],\n    \"gpuExcludeRegex\": [\n        \"Radeon HD\",\n        \"Radeon R\\\\d+\",\n        \"GT\\\\s+\",\n        \"GTS\",\n        \"Max-Q\",\n        \"GTX \\\\d{3}\",\n        \"mobile\",\n        \"GTX 10\\\\d+\",\n        \"Radeon VII\",\n        \"RX VEGA\",\n        \"TITAN\",\n        \"2080\"\n    ],\n    \"cpuExcludeRegex\": [\n        \"i[3-7].\\\\d{3}\\\\w{1}\",\n        \"i[3-7]-\\\\d{3}\",\n        \"Ryzen 3 1\\\\d+\",\n        \"Ryzen 5 1\\\\d+\",\n        \"Ryzen 7 1\\\\d+\",\n        \"Threadripper\",\n        \"AMD Athlon\",\n        \"AMD E2\",\n        \"AMD FX\",\n        \"AMD Phenom\",\n        \"Intel Core2 Duo\",\n        \"Intel Core2 Extreme\",\n        \"Intel Core2 Quad\",\n        \"Intel Pentium\",\n        \"i3-2\\\\d+\",\n        \"i5-2\\\\d+\",\n        \"i7-2\\\\d+\",\n        \"i3-3\\\\d+\",\n        \"i5-3\\\\d+\",\n        \"i7-3\\\\d+\",\n        \"i3-4\\\\d+\",\n        \"i5-4\\\\d+\",\n        \"i7-4\\\\d+\",\n        \"i3-5\\\\d+\",\n        \"i5-5\\\\d+\",\n        \"i7-5\\\\d+\",\n        \"i3-6\\\\d+\",\n        \"i5-6\\\\d+\",\n        \"i7-6\\\\d+\",\n        \"i3-7\\\\d+\",\n        \"i5-7\\\\d+\",\n        \"i7-7\\\\d+\"\n    ],\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/gpuchecker-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxyConfig": {"useApifyProxy": true}, "cpuExcludeRegex": ["i[3-7].\\d{3}\\w{1}", "i[3-7]-\\d{3}", "Ryzen 3 1\\d+", "Ryzen 5 1\\d+", "Ryzen 7 1\\d+", "Threadripper", "AMD Athlon", "AMD E2", "AMD FX", "AMD Phenom", "Intel Core2 Duo", "Intel Core2 Extreme", "Intel Core2 Quad", "Intel Pentium", "i3-2\\d+", "i5-2\\d+", "i7-2\\d+", "i3-3\\d+", "i5-3\\d+", "i7-3\\d+", "i3-4\\d+", "i5-4\\d+", "i7-4\\d+", "i3-5\\d+", "i5-5\\d+", "i7-5\\d+", "i3-6\\d+", "i5-6\\d+", "i7-6\\d+", "i3-7\\d+", "i5-7\\d+", "i7-7\\d+"], "cpuIncludeRegex": ["2200g"], "gpuExcludeRegex": ["Radeon HD", "Radeon R\\d+", "GT\\s+", "GTS", "Max-Q", "GTX \\d{3}", "mobile", "GTX 10\\d+", "Radeon VII", "RX VEGA", "TITAN", "2080"], "gpuIncludeRegex": ["460"]}, "actor_id": "dtrungtin/gpuchecker-scraper"}, "microworlds/datadome-sound-captcha": {"id": 1436, "url": "https://apify.com/microworlds/datadome-sound-captcha/api/client/nodejs", "title": "Apify API and Datadome Sound Captcha interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"audio_url\": \"https://dd.prod.captcha-delivery.com/audio/2023-05-19/en/3a1e6a23a02face80c0f26578049ced9.wav\",\n    \"proxyConfig\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"microworlds/datadome-sound-captcha\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"audio_url": "https://dd.prod.captcha-delivery.com/audio/2023-05-19/en/3a1e6a23a02face80c0f26578049ced9.wav", "proxyConfig": {"useApifyProxy": false}}, "actor_id": "microworlds/datadome-sound-captcha"}, "lukass/covid-ger": {"id": 1437, "url": "https://apify.com/lukass/covid-ger/api/client/nodejs", "title": "Apify API and Coronavirus stats in Germany interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukass/covid-ger\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukass/covid-ger"}, "lukaskrivka/rebirth-failed-requests": {"id": 1438, "url": "https://apify.com/lukaskrivka/rebirth-failed-requests/api/client/nodejs", "title": "Apify API and Rebirth failed requests interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"runIds\": [\n        \"ILnaIxL36Qd9tLgUh\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/rebirth-failed-requests\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"runIds": ["ILnaIxL36Qd9tLgUh"]}, "actor_id": "lukaskrivka/rebirth-failed-requests"}, "curious_coder/ai-assistant": {"id": 1439, "url": "https://apify.com/curious_coder/ai-assistant/api/client/nodejs", "title": "Apify API and GPT AI Assistant interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"sourceType\": \"source.csv\",\n    \"source.csv\": \"agencyName,lastName,chargeRateCurrency,chargeRate,totalCompletedJobs,locationCity,locationCountry,locationRegion,locationState,locationSubregion,locationTimezone,locationZip,firstName,title,description,skillsText\\nWTT Solutions,L.,USD,35.0,30,Kyiv,Ukraine,Europe,Kyiv City,Eastern Europe,UTC+03:00 Minsk,3148,Serhii,Full Stack Development / Front-End Development / Mobile Development,\\\"I have extremely rigorous and intense software development experiences since 2004. I was actively involved in back-end development, front-end development, solutions architecture, DevOps engineering, full stack development, technical product management. I have worked in many industries using many different technologies with many different types of stakeholders. \\n\\nI'm result-oriented and product-driven. I love learning new things and constantly strive to find the most efficient ways to get things done better and faster. \\n\\nBut no one can help client businesses grow alone. And I created a strong and professional team - WTT Solutions. We are a group of software experts who have delivered world-class solutions to businesses of all shapes and sizes. We understand that different businesses have different needs - so we adapt to you. Whether you're a small or medium enterprise or in the Fortune 500, we tailor our approach to you to ensure that you get exactly the solution you need and that the entire process is efficient and easy. \\n\\nOur main business domains are MarTech, EdTech, and FinTech, but we have tremendous experience in all types of SaaS solutions.\\n\\nWhat we do:\\n\\n- SaaS Development\\n- Startups Development\\n- Discovery Phase and MVP Development\\n- Mobile App Development\\n- Consulting: technical and strategy\\n- Integrations\\n- Migrations\\n- Support\\n\\nOur specialties:\\n\\n- UX/UI Design\\n- Full Stack Development\\n- Front-End Development\\n- Mobile App Development\\n- Back-End Development\\n- Software Development Tutoring\\n- Desktop Software Development\\n- Solutions Architecture\\n\\n> Main tech stack\\n\\n\ud83d\udcc3 React\\n\ud83d\udcc3 Angular\\n\ud83d\udcc3 NodeJS\\n\ud83d\udcc3 Flutter\\n\ud83d\udcc3 Typescript\\n\ud83d\udcc3AWS\\n\\nFeel free to reach out, and let's make it happen!\\\",\\\"Node.js,React Native,JavaScript,AngularJS,Amazon Web Services,React,AWS Lambda,SaaS,Ionic Framework,ChatGPT,GPT-4 Developer,Mobile App Development\\\"\\nRank One,N.,USD,45.0,48,Manila,Philippines,Asia,NCR,South-Eastern Asia,\\\"UTC+08:00 Hong Kong SAR, Perth, Singapore, Taipei\\\",1004,Michael Dave,\\\"Expert NLP with ASR, TTS, ChatGPT Experience\\\",\\\"Hi there! I'm a freelance NLP, ChatGPT, and ASR expert with over 4 years of experience in developing speech and text-based AI solutions for various domains and platforms. I have a strong background in natural language processing, deep learning, and Python programming. I have successfully completed projects involving:\\n\\n\u2022  Building chatbots and voice assistants using ChatGPT, Dialogflow, Rasa, and Amazon Lex\\n\\n\u2022  Developing NLP pipelines for text analysis, sentiment classification, named entity recognition, and topic modeling\\n\\n\u2022  Implementing state-of-the-art NLP models such as BERT, GPT-3, and Transformers\\n\\n\u2022  Creating engaging and natural dialogues for chatbots and voice assistants\\n\\n\u2022  Integrating NLP and ChatGPT solutions with web and mobile applications\\n\\n\u2022  Developing ASR systems using Kaldi, DeepSpeech, and Wav2Vec\\n\\n\u2022  Enhancing ASR performance using speech enhancement, noise reduction, and speaker diarization techniques\\n\\n\u2022  Transcribing audio and video files using ASR systems\\n\\n\u2022  Evaluating ASR accuracy and quality using metrics and tools\\n\\nI'm passionate about creating human-like and intelligent conversational agents that can understand and generate speech and text. I'm always eager to learn new technologies and frameworks in the NLP, ChatGPT, and ASR field. I'm also proficient in web development, data visualization, and machine learning.\\n\\nI value quality, efficiency, and communication in my work. I always deliver on time and within budget. I'm flexible and adaptable to different project requirements and scopes. I'm also friendly and easy to work with.\\n\\nIf you're looking for a reliable and skilled NLP, ChatGPT, and ASR freelancer, look no further. I'm ready to help you with your project and achieve your goals. Let's chat!\\\",\\\"C++,Image Processing,Computer Vision,Amazon EC2,Tesseract OCR,Automatic Speech Recognition,OpenCV,Machine Learning,Android NDK,OCR Algorithm,ChatGPT,Natural Language Processing,Web Scraping,Next.js,GPT-4 Developer\\\"\\nFriends Bit,M.,USD,60.0,42,Karachi,Pakistan,Asia,Sindh,Southern Asia,\\\"UTC+05:00 Islamabad, Karachi\\\",75950,Khalil,\\\"Full Stack Developer with API, SSO, SQL & Automation Experience.\\\",\\\"Hello,\\n\\nMy name is Khalil Mohammad Mirza, a seasoned programmer who is dedicated to delivering exceptional results and ensuring customer satisfaction. My core values are quality, timeliness, and adaptability, and I strive to provide nothing less than top-notch services to all my clients.\\n\\nMy areas of expertise include a broad range of technologies, including:\\n\\n1. Asp.net: full framework and core\\n2. NodeJs\\n3. Python\\n4. PHP\\n5. GoLang\\n6. Databases: SQL Server, MySql, Postgres, Azure SQL, Amazon RDS, and MongoDB\\n7. Front-end frameworks: Angular, ReactJs, and VueJs\\n8. Cloud Platforms: Azure, AWS, and GCP\\n9. Containerization: Docker/Kubernetes\\n10. Desktop Applications: ElectronJs and Winform\\n\\nMy career in programming began as a junior backend developer at TechnoLab360, where I played a significant role in the development of the Legacy Supply Chain Management EPR for Sysco Guest Supply. I then advanced to the position of senior lead backend developer, where I spearheaded the development of the new Supply Chain Management ERP, led a team of developers, and oversaw the seamless transition from the old ERP. Additionally, I created CLI apps that facilitated migration.\\n\\nCurrently, I work as a full-stack backend heavy developer for the United Nations, where I am leading the development of eLUNa, a web-based translation tool that features automatic identification of previously translated sentences and terminology, coupled with machine translation for new sentences. I am also working on the development of eLUNa Editorial, a custom-built editing program that caters to the needs of editors at the United Nations.\\n\\nIn addition to my work at the United Nations, I have undertaken several small-scale projects, including bot automations, SSO, performance optimization, web scraping projects, CLI tools, consultations, and small web applications.\\n\\nWith my commitment to excellence and experience in a broad range of technologies, I am confident that I can surpass your expectations and deliver exceptional results. Thank you for considering me for your programming needs.\\n\\nBest regards,\\nKhalil Mohammad Mirza\\\",\\\"React,REST,ASP.NET Core,SQL,Node.js,JavaScript,C#,API Integration,Cloud Architecture,SOAP,Automation,Python,NoSQL Database,API Development,JSON\\\"\\n,T.,USD,60.0,16,Kutaisi,Georgia,Asia,IM,Western Asia,\\\"UTC+04:00 Abu Dhabi, Muscat, Tbilisi, Kazan\\\",4600,Sergey,Salesforce Developer,\\\"I help Business to increase Sales and Revenue using Salesforce.com technologies and develop best-in-class Salesforce Apps for the Salesforce AppExchange marketplace.\\n\\nI have 12 years of Salesforce Experience and 10 Salesforce Certificates.\\n\u2705 Salesforce Sales Cloud\\n\u2705 Salesforce Service Cloud\\n\u2705 Salesforce Marketing Cloud\\n\u2705 Salesforce Experience Cloud \\n\u2705 Salesforce CPQ\\n\u2705 Salesforce Pardot and Marketo\\n\u2705 Salesforce AppExchange managed packages development\\n\u2705 Salesforce Integration\\n\u2705 Salesforce Customizations and Data Migration\\n\\nI am familiar with programming and declarative tools:\\n\ud83d\ude80Apex, Visualforce, Lightning, Aura, LWC, Lightning Web Components, Triggers, Batch, Scheduler, Feature Method, Queueable, SOQL, SOSL.\\n\ud83d\ude80Workflow Rules, Process Builder, Flow, Screen Flow, Validation Rule, Email, Email Templates, Journey Builder, Data Extension.\\n\ud83d\ude80JavaScript, Angular, Vue, React, jQuery, HTML, CSS, HTML, HTML5, CSS, CSS3.\\n\\n\ud83d\udee0\ufe0f I implemented Integration with the following systems: SAP, Zuora, DocuSign, Adobe Sign, PandaDoc, Wrike, Yildex, Operative, Weather.com, Stripe, Twilio, Blackthorn, Hubspot, Celigo, NetSuite, WordPress, FormAssembly, Form Assembly, Titan, Formstack, Docomotion, Nintex Drawloop DocGen, Chat GPT, ChatGPT, Zoho CRM. \\n\\n\ud83e\udd16 I built several Salesforce Chrome Extensions\\n\\nMy Salesforce Certifications:\\n\ud83c\udf932021 - Salesforce Certified Experience Cloud Consultant\\n\ud83c\udf932021 - Salesforce Certified Sales Cloud Consultant\\n\ud83c\udf932021 - Salesforce Certified Service Cloud Consultant\\n\ud83c\udf932020 - Salesforce Certified Javascript Developer 1\\n\ud83c\udf932020 - Salesforce Certified Platform Developer 2\\n\ud83c\udf932020 - Salesforce Certified Administrator\\n\ud83c\udf932017 - Salesforce Certified Platform App Builder\\n\ud83c\udf932015 - Salesforce Certified Platform Developer 1\\n\ud83c\udf932012 - Salesforce.com Certified Force.com Developer\\n\\n\u2699\ufe0f Other used programming languages: Java, C#, .NET, Python, PHP, Laravel, WordPress.\\n\\nKeywords: Salesforce, SalesForce, Sales Force, Salesforce.com, SF, SFDC, Sales Force, SF DC, Sales, Sales Funnel, Duplication, Service, Marketing, Cloud, Marketing Campaign, MC, Date Extension, DE, Clouds, Pardot, Salesforce Developer, Salesforce Development, Salesforce Admin, Salesforce Administration, Salesforce Integration, Salesforce setup, Salesforce set up, Salesforce Customization, Salesforce dev, Salesforce Account, Lead, Contact, Opportunity, Quote, Order, CPQ, PDF, SF dev, sfdc dev, Salesforce optimization, AppExchange, Managed Package, Unmanaged Package, app, application, publishing, publish, LWC, Lightning, Classic, Aura, Lightning Web Component, Lightning Framework, Visual force, Visualforce, VF, Lightning page, Apex, Trigger, Batch, Apex Class, Future Methods, Scheduler, Scheduled Apex, Batch, Queue, Queueable, Standard Objects, Custom Objects, Custom Fields, Validation, Validation Rules, Automation, Pardot, Data, Setup, Process, Form, ChatGPT, Chat GPT, Chatbot, Chat Bot, AI, Open AI, OpenAI, Einstein, Einstein GPT, EinsteinGPT, LinkedIn, Linked In, Apollo, Workflow, work flow, WF, Screen Flow, screenflow, Flow, Trigger Flow, triggerflow, Process Builder, processbuilder, email, email template, MailChimp, Mail Chimp, Einstein GPT, ChatGPT, GPT4, Conga Composer & Conga Sign, Salesforce Maps, High Velocity Sales, Digital Experience, Cloud.\\\",\\\"JavaScript,API,Pardot,Apex,Salesforce Service Cloud,Salesforce Lightning,Salesforce Marketing Cloud,DocuSign,Salesforce,HTML,AppExchange,CSS,API Integration,Salesforce Sales Cloud,Salesforce CPQ\\\"\\nArmstep,S.,USD,60.0,34,Yerevan,Armenia,Asia,ER,Western Asia,UTC+04:00 Armenia,14,Arman,Full Stack Web3 | Blockchain Developer,\\\"Top 3% Qualified Freelancer With 7000+ Hours of Proven Track Record Delivering Consistently High-Quality Work.\\n\\nAs a full-stack Web3 and Blockchain developer, I have over 6 years of experience in developing end-to-end software solutions. I am proficient in several programming languages including JavaScript, Python, Solidity, and Java. My experience with front-end frameworks includes React,js and I am skilled in back-end development using Node.js, Django, and Flask.\\n\\nIn the area of AI, I have over 5 years of experience with machine learning, natural language processing, computer vision, and deep learning. I am skilled in using popular libraries such as TensorFlow, Keras, PyTorch, and OpenCV to build AI-powered applications.\\n\\nAs a blockchain developer, I have over 4 years of experience in building decentralized applications using Ethereum, Solana, and Binance Smart Chain. I am proficient in writing smart contracts using Solidity and Rust, and I have experience with web3.js and other blockchain tools.\\n\\nIn addition to my technical skills, I have experience with cloud computing platforms, specifically Google Cloud Platform (GCP). I am skilled in using GCP tools such as App Engine, Compute Engine, Kubernetes, and Cloud Functions, and I have experience in deploying and managing applications on GCP.\\n\\nOverall, my diverse skillset and experience in web development, AI, and blockchain, make me a versatile and adaptable developer who can work on various projects.\\n\\nI always provide great quality work, meet all the deadlines and maintain excellent long-term cooperation with my clients.\\\",\\\"ChatGPT,Java,Python,Node.js,Django,React,Solana,Rust,Smart Contract,Blockchain Development,Blockchain,Solidity,React Native,Web Development,PostgreSQL\\\"\\nCRMOZ,K.,USD,45.0,55,Kiev,Ukraine,Europe,Kyiv City,Eastern Europe,UTC+02:00 Israel,39000,Oleksandr,Zoho developer,\\\"Result-oriented expert in the implementation of Zoho products with more than 7 years of experience in Zoho, plus 15 years of app&web&db development for dozens of sectors.\\nExpand the core functionality of Zoho apps. I can develop custom Zoho CRM (Books, Desk, etc.) integrations for your business needs (via API, Zapier, Make, etc.).\\nAutomate your sales, marketing, and business processes and standardize working methods.\\nCustomize standard modules, add additional functionalities, and make Zoho CRM work as you do. With custom views, filters, and fields.\\n \\n- ZeptoMail\\n- Zoho CRM\\n- Zoho Flow\\n- Zoho Desk\\n- Zoho Analytics\\n- Zoho Books\\n- Zoho Inventory\\n- Zoho Creator\\n- Zoho Recruit and Zoho People\\n- Zoho Subscriptions\\n- Orchestly\\n- Zoho Catalyst and other API integration tools\\\",\\\"Zoho CRM,Zoho Creator,Data Migration,Business Process Model & Notation,API Development,Zoho Analytics,MySQL,API Integration,API,Plugin Development,Enterprise Resource Planning,Marketing Automation Strategy,Business Consulting,OAuth,Telemarketing\\\"\\nStudio45 Creations  - TOP RATED PLUS,A.,USD,18.0,49,Mohali,India,Asia,PUNJAB,Southern Asia,\\\"UTC+05:30 Mumbai, Kolkata, Chennai, New Delhi\\\",160071,Vijay,Python/Django Developer,\\\"With 9 years of diverse experience as a Python/Django developer, I have a proven track record of successfully completing projects from design to implementation and integration. My expertise includes Python Django, PostgreSQL/SQLite, REST/GraphQL, as well as handling DevOps tasks such as CI/CD and Docker.\\n\\nPYTHON | DJANGO | REACT | VIRTUALIZATION | API | DATABASE DEVELOPMENT | \\n\\nI have developed entire front-end and back-end modules using Java and Python web frameworks and have been actively involved in maintenance, redesign, project management, and the development of different web applications. Working with a Scrum process and agile methodologies, I have developed a variety of websites, including educational platforms, e-commerce, file-sharing, job portals, and various others. I have a creative and performance-oriented work approach and I am adept at directing software development. \\n\\nBeing inspired by market-leading technology providers and constantly striving to keep up with the most recent developments in software design, I am also quite skilled and enthusiastic when it comes to UI and UX design.\\n\\nMy competencies include but are not limited to:\\n\\n\u272a Development of intuitive and cloud-based software products and applications using clean object-oriented design.\\n\\n\u272a Extensive expertise in working with Pyramid and Django Python frameworks, offering the optimum solution to deal with all kinds of project complexities.\\n\\n\u272a Development of custom Python applications, keeping in mind your business\u2019s key objectives while ensuring data security and protection and identifying data storage solutions.\\n\\n\u272a Strong focus on implementing agile, secure, scalable, performance-based Python development practices.\\n\\n\u272a Designing and creating RESTful APIs for internal and partner consumption\\n\\n\u272a Experience with databases such as MySQL or PostgreSQL\\n\\n\u272a Design and development of data management systems using MySQL\\n\\n\u272a Debugging and troubleshooting web applications.\\n\\nSkills\\n\\nPython, Python Script, Python Requests, Django, React/Redux, React Bootstrap, React Native, JavaScript, PHP, HTML5, CSS3, GraphQL, My SQL, PostgreSQL, jQuery, AJAX, Network Security, API Development, API Integration, Project Management, Scrum, Backend & Front Development.\\n\\n\u272a I am also an  Oracle Database PL/SQL Developer Certified Professional.\\n\\nWith a strong background in a disciplined software development lifecycle process (SDLC) and excellent analytical, programming, and problem-solving skills, I am all set to help you build a winning solution.\\n\\nI will be delighted to work with you on your project whether you are seeking a stand-alone developer or expanding your existing team.\\n\\nLet\u2019s connect...\\\",\\\"GraphQL,React Bootstrap,Python,React Native,React,API Development,Django Stack,Website Security,API Integration,Python Script,Django,Docker Compose,JavaScript\\\"\\niRoid Solutions | Upwork Top Rated Agency | Clutch 2022 Award Winner IT Company,P.,USD,24.99,20,Surat,India,Asia,Gujarat,Southern Asia,\\\"UTC+05:30 Mumbai, Kolkata, Chennai, New Delhi\\\",395005,Daxesh,Top Rated Mobile App Developer | iOS Developer | Android Developer,\\\"\u2705  Rewarded as \ud83c\udfc6 \ud835\udde7\ud835\udde2\ud835\udde3 \ud835\udde5\ud835\uddd4\ud835\udde7\ud835\uddd8\ud835\uddd7 \ud835\udde3\ud835\udddf\ud835\udde8\ud835\udde6 \ud83c\udfc6 by Upwork  (\ud835\udde7\ud835\uddfc\ud835\uddfd \ud835\udfef%)\\n\u2705  7000+ hours and still counting \\n\u2705  100+ jobs completed successfully \\n\u2705  100% Job Success (Client Satisfaction Ratio)\\n\u2705  90% Clients Recommended to work with me\\n\u2705  86% Long-term Repeat Clients\\n\u2705  9+ Years Of Experience\\n\u2705  High-Quality Work\\n\u2705  Cost-Effective\\n\u2705  Fluent English Verified\\n\\nWhether you are a \ud835\udde6\ud835\ude01\ud835\uddee\ud835\uddff\ud835\ude01\ud835\ude02\ud835\uddfd, \ud835\uddd8\ud835\ude00\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddf9\ud835\uddf6\ud835\ude00\ud835\uddf5\ud835\uddf2\ud835\uddf1 \ud835\uddd5\ud835\ude02\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\ude00\ud835\ude00 \ud835\uddfc\ud835\uddff \ud835\uddf9\ud835\uddfc\ud835\uddfc\ud835\uddf8\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf3\ud835\uddfc\ud835\uddff \ud835\udde0\ud835\udde9\ud835\udde3, you will get \ud835\udddb\ud835\uddf6\ud835\uddf4\ud835\uddf5-\ud835\udde4\ud835\ude02\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\ude01\ud835\ude06 \ud835\udde6\ud835\uddf2\ud835\uddff\ud835\ude03\ud835\uddf6\ud835\uddf0\ud835\uddf2\ud835\ude00 at an \ud835\uddd4\ud835\uddf3\ud835\uddf3\ud835\uddfc\ud835\uddff\ud835\uddf1\ud835\uddee\ud835\uddef\ud835\uddf9\ud835\uddf2 \ud835\uddd6\ud835\uddfc\ud835\ude00\ud835\ude01, \ud835\uddda\ud835\ude02\ud835\uddee\ud835\uddff\ud835\uddee\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\uddf2\ud835\uddf1. \\nInterested? Just click on the '\ud835\udddc\ud835\uddfb\ud835\ude03\ud835\uddf6\ud835\ude01\ud835\uddf2 \ud835\ude01\ud835\uddfc \ud835\udddd\ud835\uddfc\ud835\uddef' button and lets discuss about your project and your vision.\\n\\nMY UPWORK CLIENTS SHARING THEIR EXPERIENCE WORKING WITH ME:\\n\\n\u2605\u2605\u2605\u2605\u2605 5.00\\n\\\"\\\"Daxesh and his team are amazing. They are super customer oriented, very friendly, easy to work with and very experienced and skilled. It was an absolute pleasure working with Daxesh and his team and we will for sure engage them again and I can 100% recommend him to work with. The best freelancer / agency on Upwork I have worked with so far.\\\"\\\"\\n- Christian, Germany\\n\\n\u2605\u2605\u2605\u2605\u2605 5.00\\n\\\"\\\"Daxesh and his team were incredibly skilled, thoughtful, and responsive throughout the entire project! I couldn't have asked for a better team of developers to work with, and I greatly look forward to working with them again in the near future.\\\"\\\"\\n- Aimun, USA \\n\\n\u2605\u2605\u2605\u2605\u2605 5.00\\n\\\"\\\"Exceeds expectations and always a great experience. Even with an extended break between jobs, picked up where we left off with no problems whatsoever.\\\"\\\"\\n- Beau, Canada\\n\\nThese are some of the testimonials taken directly from my Upwork profile \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\ude03\ud835\uddf2 \ud835\ude01\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\udddc \ud835\uddf0\ud835\uddee\ud835\uddfb \ud835\uddf1\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\ude03\ud835\uddf2\ud835\uddff \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01 \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\uddf4\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\ude01 \ud835\uddfe\ud835\ude02\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\ude01\ud835\ude06 \ud835\uddfc\ud835\uddfb \ud835\ude01\ud835\uddf6\ud835\uddfa\ud835\uddf2 \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\ude07\ud835\uddf2\ud835\uddff\ud835\uddfc \ud835\ude04\ud835\uddfc\ud835\uddff\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude00. \ud835\uddd9\ud835\uddfc\ud835\uddff \ud835\uddfa\ud835\uddf2, \ud835\uddf0\ud835\uddf9\ud835\uddf6\ud835\uddf2\ud835\uddfb\ud835\ude01 \ud835\ude00\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\ude00\ud835\uddf3\ud835\uddee\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddf0\ud835\uddfc\ud835\uddfa\ud835\uddf2\ud835\ude00 \ud835\uddf3\ud835\uddf6\ud835\uddff\ud835\ude00\ud835\ude01.\\n\\nDuring the last 9+ years, I have developed many different types of iPhone and Android apps.\\n\ud835\uddd9\ud835\uddfc\ud835\uddfc\ud835\uddf1 \ud835\uddf1\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\ude03\ud835\uddf2\ud835\uddff\ud835\ude06 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde2\ud835\uddfb-\ud835\uddf1\ud835\uddf2\ud835\uddfa\ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\uddd4\ud835\uddfd\ud835\uddfd, \ud835\udddb\ud835\uddfc\ud835\uddfa\ud835\uddf2 \ud835\ude00\ud835\uddf2\ud835\uddff\ud835\ude03\ud835\uddf6\ud835\uddf0\ud835\uddf2\ud835\ude00 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udddb\ud835\uddee\ud835\uddfb\ud835\uddf1\ud835\ude06 \ud835\uddfa\ud835\uddee\ud835\uddfb \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde6\ud835\uddee\ud835\uddf9\ud835\uddfc\ud835\uddfb \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddd5\ud835\uddf2\ud835\uddee\ud835\ude02\ud835\ude01\ud835\ude06 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde7\ud835\uddee\ud835\ude05\ud835\uddf6 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde5\ud835\uddf6\ud835\uddf1\ud835\uddf2 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddd8-\ud835\uddf0\ud835\uddfc\ud835\uddfa\ud835\uddfa\ud835\uddf2\ud835\uddff\ud835\uddf0\ud835\uddf2 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udddb\ud835\uddf2\ud835\uddee\ud835\uddf9\ud835\ude01\ud835\uddf5 \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\uddf3\ud835\uddf6\ud835\ude01\ud835\uddfb\ud835\uddf2\ud835\ude00\ud835\ude00 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde7\ud835\uddff\ud835\uddee\ud835\ude03\ud835\uddf2\ud835\uddf9 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde0\ud835\ude02\ud835\ude00\ud835\uddf6\ud835\uddf0 \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde9\ud835\uddf6\ud835\uddf1\ud835\uddf2\ud835\uddfc \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddd9\ud835\uddf6\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\uddf0\ud835\uddf5 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde6\ud835\uddf5\ud835\uddfc\ud835\uddfd\ud835\uddfd\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde7\ud835\uddf2\ud835\uddf9\ud835\uddf2\ud835\uddfa\ud835\uddf2\ud835\uddf1\ud835\uddf6\ud835\uddf0\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddd9\ud835\uddf6\ud835\uddfb\ud835\uddee\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\uddd4\ud835\uddfd\ud835\uddfd, \ud835\udde5\ud835\uddf2\ud835\uddee\ud835\uddf9 \ud835\uddf2\ud835\ude00\ud835\ude01\ud835\uddee\ud835\ude01\ud835\uddf2 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde6\ud835\uddfc\ud835\uddf0\ud835\uddf6\ud835\uddee\ud835\uddf9 \ud835\uddfa\ud835\uddf2\ud835\uddf1\ud835\uddf6\ud835\uddee \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddd6\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddd5\ud835\ude02\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\ude00\ud835\ude00 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde0\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddee\ud835\uddf4\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde2\ud835\uddfb\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\uddf0\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2\ud835\ude00 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde2\ud835\uddfb\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\uddf2\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde7\ud835\ude02\ud835\ude01\ud835\uddfc\ud835\uddff \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udddb\ud835\uddfc\ud835\ude01\ud835\uddf2\ud835\uddf9 \ud835\uddef\ud835\uddfc\ud835\uddfc\ud835\uddf8\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde2\ud835\uddfb\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\uddf4\ud835\uddff\ud835\uddfc\ud835\uddf0\ud835\uddf2\ud835\uddff\ud835\ude06 \ud835\uddfc\ud835\uddff\ud835\uddf1\ud835\uddf2\ud835\uddff \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\uddfa\ud835\uddee\ud835\uddfb\ud835\ude06 \ud835\uddfa\ud835\uddfc\ud835\uddff\ud835\uddf2.\\n\\n\ud83d\udd39 Why you should hire me? \\n\u2714 I always work towards your success\\n\u2714 Quality work at affordable prices\\n\u2714 Strong Android, iPhone, and backend architecture and database knowledge\\n\u2714 Develop applications that support all devices\\n\u2714 Delivering product on/before time\\n\u2714 Always up-to-date with the latest technologies\\n\\n\ud83d\udd39 My core skills and experiences:\\n\u25c9 Interaction with backend - Rest API, JSON, XML, PHP, Node.js, Python\\n\u25c9 Databases / Data Management - SQLite, MySQL, Parse, MongoDB\\n\u25c9 Social Integration - Facebook, Twitter, Youtube, Instagram, Snapchat, Tinder\\n\u25c9 Chat - XMPP, Socket, Firebase, GetStream, Twilio\\n\u25c9 Version Control Tools - Github, Bitbucket, Gitlab\\n\u25c9 Google Map, Apple Map kit, Geolocation, GPS, Location Tracking\\n\u25c9 Audio / Video Live Streaming\\n\u25c9 Firebase APIs\\n\u25c9 Paypal, Braintree, Stripe Payment Gateway \\n\u25c9 QR Codes, Bar Codes, VIN Scanner, NFC Tags Reading\\n\u25c9 In-App Purchase, Advertisement, Push Notifications, GCM, Google Analytics\\n\u25c9 Photo & Video Sharing, Editor, Filter\\n\\n\ud83d\udd39 We work with the below technologies for development:\\n\ud835\uddd4\ud835\uddfb\ud835\uddf1\ud835\uddff\ud835\uddfc\ud835\uddf6\ud835\uddf1: Kotlin, Java\\n\ud835\uddf6\ud835\udde2\ud835\udde6: Swift, Objective-C\\n\ud835\uddd6\ud835\uddff\ud835\uddfc\ud835\ude00\ud835\ude00-\ud835\uddfd\ud835\uddf9\ud835\uddee\ud835\ude01\ud835\uddf3\ud835\uddfc\ud835\uddff\ud835\uddfa (\ud835\udddb\ud835\ude06\ud835\uddef\ud835\uddff\ud835\uddf6\ud835\uddf1): Flutter (Dart)\\n\ud835\uddea\ud835\uddf2\ud835\uddef / \ud835\uddd5\ud835\uddee\ud835\uddf0\ud835\uddf8\ud835\uddf2\ud835\uddfb\ud835\uddf1: PHP, Node.js, Python, Laravel, WordPress, Express.js, Nest,js, Django\\n\ud835\uddd9\ud835\uddff\ud835\uddfc\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\uddfb\ud835\uddf1: React.Js, HTML5, CSS3, BootStrap, JavaScript, jQuery\\nAugmented Reality (AR), Internet Of Things (IoT), Beacon, Ricoh Theta 360\\n\\n\ud83d\udcac \ud835\uddd9\ud835\udde5\ud835\uddd8\ud835\uddd8 \ud835\uddd6\ud835\udde2\ud835\udde1\ud835\udde6\ud835\udde8\ud835\udddf\ud835\udde7\ud835\uddd4\ud835\udde7\ud835\udddc\ud835\udde2\ud835\udde1: \\nMy consultation is \ud835\uddf3\ud835\uddff\ud835\uddf2\ud835\uddf2. Let's schedule an introductory call and discover the possibilities of working together. \\nAnd even if we don't work together, you will get some tips/ideas that can be helpful to your project.\\nJust click on the '\ud835\udddc\ud835\uddfb\ud835\ude03\ud835\uddf6\ud835\ude01\ud835\uddf2 \ud835\ude01\ud835\uddfc \ud835\udddd\ud835\uddfc\ud835\uddef' button and experience yourself.\\n\\n\ud835\udde7\ud835\uddf5\ud835\uddee\ud835\uddfb\ud835\uddf8 \ud835\ude06\ud835\uddfc\ud835\ude02 for spending your $$ to review my profile because TIME IS MONEY :)\\\",\\\"iPhone App Development,Android App Development,iOS Development,Node.js,Flutter,iPad App Development,Laravel,Swift,React Native,Android,PHP,Kotlin,Apple Xcode,Front-End Development,Mobile App\\\"\",\n    \"prompt\": \"Write a personalized first line for a cold email to be sent to given upwork freelancer profile\",\n    \"outputFields\": [\n        \"firstLine\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/ai-assistant\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"prompt": "Write a personalized first line for a cold email to be sent to given upwork freelancer profile", "source.csv": "agencyName,lastName,chargeRateCurrency,chargeRate,totalCompletedJobs,locationCity,locationCountry,locationRegion,locationState,locationSubregion,locationTimezone,locationZip,firstName,title,description,skillsText\nWTT Solutions,L.,USD,35.0,30,Kyiv,Ukraine,Europe,Kyiv City,Eastern Europe,UTC+03:00 Minsk,3148,Serhii,Full Stack Development / Front-End Development / Mobile Development,\"I have extremely rigorous and intense software development experiences since 2004. I was actively involved in back-end development, front-end development, solutions architecture, DevOps engineering, full stack development, technical product management. I have worked in many industries using many different technologies with many different types of stakeholders. \n\nI'm result-oriented and product-driven. I love learning new things and constantly strive to find the most efficient ways to get things done better and faster. \n\nBut no one can help client businesses grow alone. And I created a strong and professional team - WTT Solutions. We are a group of software experts who have delivered world-class solutions to businesses of all shapes and sizes. We understand that different businesses have different needs - so we adapt to you. Whether you're a small or medium enterprise or in the Fortune 500, we tailor our approach to you to ensure that you get exactly the solution you need and that the entire process is efficient and easy. \n\nOur main business domains are MarTech, EdTech, and FinTech, but we have tremendous experience in all types of SaaS solutions.\n\nWhat we do:\n\n- SaaS Development\n- Startups Development\n- Discovery Phase and MVP Development\n- Mobile App Development\n- Consulting: technical and strategy\n- Integrations\n- Migrations\n- Support\n\nOur specialties:\n\n- UX/UI Design\n- Full Stack Development\n- Front-End Development\n- Mobile App Development\n- Back-End Development\n- Software Development Tutoring\n- Desktop Software Development\n- Solutions Architecture\n\n> Main tech stack\n\n\ud83d\udcc3 React\n\ud83d\udcc3 Angular\n\ud83d\udcc3 NodeJS\n\ud83d\udcc3 Flutter\n\ud83d\udcc3 Typescript\n\ud83d\udcc3AWS\n\nFeel free to reach out, and let's make it happen!\",\"Node.js,React Native,JavaScript,AngularJS,Amazon Web Services,React,AWS Lambda,SaaS,Ionic Framework,ChatGPT,GPT-4 Developer,Mobile App Development\"\nRank One,N.,USD,45.0,48,Manila,Philippines,Asia,NCR,South-Eastern Asia,\"UTC+08:00 Hong Kong SAR, Perth, Singapore, Taipei\",1004,Michael Dave,\"Expert NLP with ASR, TTS, ChatGPT Experience\",\"Hi there! I'm a freelance NLP, ChatGPT, and ASR expert with over 4 years of experience in developing speech and text-based AI solutions for various domains and platforms. I have a strong background in natural language processing, deep learning, and Python programming. I have successfully completed projects involving:\n\n\u2022  Building chatbots and voice assistants using ChatGPT, Dialogflow, Rasa, and Amazon Lex\n\n\u2022  Developing NLP pipelines for text analysis, sentiment classification, named entity recognition, and topic modeling\n\n\u2022  Implementing state-of-the-art NLP models such as BERT, GPT-3, and Transformers\n\n\u2022  Creating engaging and natural dialogues for chatbots and voice assistants\n\n\u2022  Integrating NLP and ChatGPT solutions with web and mobile applications\n\n\u2022  Developing ASR systems using Kaldi, DeepSpeech, and Wav2Vec\n\n\u2022  Enhancing ASR performance using speech enhancement, noise reduction, and speaker diarization techniques\n\n\u2022  Transcribing audio and video files using ASR systems\n\n\u2022  Evaluating ASR accuracy and quality using metrics and tools\n\nI'm passionate about creating human-like and intelligent conversational agents that can understand and generate speech and text. I'm always eager to learn new technologies and frameworks in the NLP, ChatGPT, and ASR field. I'm also proficient in web development, data visualization, and machine learning.\n\nI value quality, efficiency, and communication in my work. I always deliver on time and within budget. I'm flexible and adaptable to different project requirements and scopes. I'm also friendly and easy to work with.\n\nIf you're looking for a reliable and skilled NLP, ChatGPT, and ASR freelancer, look no further. I'm ready to help you with your project and achieve your goals. Let's chat!\",\"C++,Image Processing,Computer Vision,Amazon EC2,Tesseract OCR,Automatic Speech Recognition,OpenCV,Machine Learning,Android NDK,OCR Algorithm,ChatGPT,Natural Language Processing,Web Scraping,Next.js,GPT-4 Developer\"\nFriends Bit,M.,USD,60.0,42,Karachi,Pakistan,Asia,Sindh,Southern Asia,\"UTC+05:00 Islamabad, Karachi\",75950,Khalil,\"Full Stack Developer with API, SSO, SQL & Automation Experience.\",\"Hello,\n\nMy name is Khalil Mohammad Mirza, a seasoned programmer who is dedicated to delivering exceptional results and ensuring customer satisfaction. My core values are quality, timeliness, and adaptability, and I strive to provide nothing less than top-notch services to all my clients.\n\nMy areas of expertise include a broad range of technologies, including:\n\n1. Asp.net: full framework and core\n2. NodeJs\n3. Python\n4. PHP\n5. GoLang\n6. Databases: SQL Server, MySql, Postgres, Azure SQL, Amazon RDS, and MongoDB\n7. Front-end frameworks: Angular, ReactJs, and VueJs\n8. Cloud Platforms: Azure, AWS, and GCP\n9. Containerization: Docker/Kubernetes\n10. Desktop Applications: ElectronJs and Winform\n\nMy career in programming began as a junior backend developer at TechnoLab360, where I played a significant role in the development of the Legacy Supply Chain Management EPR for Sysco Guest Supply. I then advanced to the position of senior lead backend developer, where I spearheaded the development of the new Supply Chain Management ERP, led a team of developers, and oversaw the seamless transition from the old ERP. Additionally, I created CLI apps that facilitated migration.\n\nCurrently, I work as a full-stack backend heavy developer for the United Nations, where I am leading the development of eLUNa, a web-based translation tool that features automatic identification of previously translated sentences and terminology, coupled with machine translation for new sentences. I am also working on the development of eLUNa Editorial, a custom-built editing program that caters to the needs of editors at the United Nations.\n\nIn addition to my work at the United Nations, I have undertaken several small-scale projects, including bot automations, SSO, performance optimization, web scraping projects, CLI tools, consultations, and small web applications.\n\nWith my commitment to excellence and experience in a broad range of technologies, I am confident that I can surpass your expectations and deliver exceptional results. Thank you for considering me for your programming needs.\n\nBest regards,\nKhalil Mohammad Mirza\",\"React,REST,ASP.NET Core,SQL,Node.js,JavaScript,C#,API Integration,Cloud Architecture,SOAP,Automation,Python,NoSQL Database,API Development,JSON\"\n,T.,USD,60.0,16,Kutaisi,Georgia,Asia,IM,Western Asia,\"UTC+04:00 Abu Dhabi, Muscat, Tbilisi, Kazan\",4600,Sergey,Salesforce Developer,\"I help Business to increase Sales and Revenue using Salesforce.com technologies and develop best-in-class Salesforce Apps for the Salesforce AppExchange marketplace.\n\nI have 12 years of Salesforce Experience and 10 Salesforce Certificates.\n\u2705 Salesforce Sales Cloud\n\u2705 Salesforce Service Cloud\n\u2705 Salesforce Marketing Cloud\n\u2705 Salesforce Experience Cloud \n\u2705 Salesforce CPQ\n\u2705 Salesforce Pardot and Marketo\n\u2705 Salesforce AppExchange managed packages development\n\u2705 Salesforce Integration\n\u2705 Salesforce Customizations and Data Migration\n\nI am familiar with programming and declarative tools:\n\ud83d\ude80Apex, Visualforce, Lightning, Aura, LWC, Lightning Web Components, Triggers, Batch, Scheduler, Feature Method, Queueable, SOQL, SOSL.\n\ud83d\ude80Workflow Rules, Process Builder, Flow, Screen Flow, Validation Rule, Email, Email Templates, Journey Builder, Data Extension.\n\ud83d\ude80JavaScript, Angular, Vue, React, jQuery, HTML, CSS, HTML, HTML5, CSS, CSS3.\n\n\ud83d\udee0\ufe0f I implemented Integration with the following systems: SAP, Zuora, DocuSign, Adobe Sign, PandaDoc, Wrike, Yildex, Operative, Weather.com, Stripe, Twilio, Blackthorn, Hubspot, Celigo, NetSuite, WordPress, FormAssembly, Form Assembly, Titan, Formstack, Docomotion, Nintex Drawloop DocGen, Chat GPT, ChatGPT, Zoho CRM. \n\n\ud83e\udd16 I built several Salesforce Chrome Extensions\n\nMy Salesforce Certifications:\n\ud83c\udf932021 - Salesforce Certified Experience Cloud Consultant\n\ud83c\udf932021 - Salesforce Certified Sales Cloud Consultant\n\ud83c\udf932021 - Salesforce Certified Service Cloud Consultant\n\ud83c\udf932020 - Salesforce Certified Javascript Developer 1\n\ud83c\udf932020 - Salesforce Certified Platform Developer 2\n\ud83c\udf932020 - Salesforce Certified Administrator\n\ud83c\udf932017 - Salesforce Certified Platform App Builder\n\ud83c\udf932015 - Salesforce Certified Platform Developer 1\n\ud83c\udf932012 - Salesforce.com Certified Force.com Developer\n\n\u2699\ufe0f Other used programming languages: Java, C#, .NET, Python, PHP, Laravel, WordPress.\n\nKeywords: Salesforce, SalesForce, Sales Force, Salesforce.com, SF, SFDC, Sales Force, SF DC, Sales, Sales Funnel, Duplication, Service, Marketing, Cloud, Marketing Campaign, MC, Date Extension, DE, Clouds, Pardot, Salesforce Developer, Salesforce Development, Salesforce Admin, Salesforce Administration, Salesforce Integration, Salesforce setup, Salesforce set up, Salesforce Customization, Salesforce dev, Salesforce Account, Lead, Contact, Opportunity, Quote, Order, CPQ, PDF, SF dev, sfdc dev, Salesforce optimization, AppExchange, Managed Package, Unmanaged Package, app, application, publishing, publish, LWC, Lightning, Classic, Aura, Lightning Web Component, Lightning Framework, Visual force, Visualforce, VF, Lightning page, Apex, Trigger, Batch, Apex Class, Future Methods, Scheduler, Scheduled Apex, Batch, Queue, Queueable, Standard Objects, Custom Objects, Custom Fields, Validation, Validation Rules, Automation, Pardot, Data, Setup, Process, Form, ChatGPT, Chat GPT, Chatbot, Chat Bot, AI, Open AI, OpenAI, Einstein, Einstein GPT, EinsteinGPT, LinkedIn, Linked In, Apollo, Workflow, work flow, WF, Screen Flow, screenflow, Flow, Trigger Flow, triggerflow, Process Builder, processbuilder, email, email template, MailChimp, Mail Chimp, Einstein GPT, ChatGPT, GPT4, Conga Composer & Conga Sign, Salesforce Maps, High Velocity Sales, Digital Experience, Cloud.\",\"JavaScript,API,Pardot,Apex,Salesforce Service Cloud,Salesforce Lightning,Salesforce Marketing Cloud,DocuSign,Salesforce,HTML,AppExchange,CSS,API Integration,Salesforce Sales Cloud,Salesforce CPQ\"\nArmstep,S.,USD,60.0,34,Yerevan,Armenia,Asia,ER,Western Asia,UTC+04:00 Armenia,14,Arman,Full Stack Web3 | Blockchain Developer,\"Top 3% Qualified Freelancer With 7000+ Hours of Proven Track Record Delivering Consistently High-Quality Work.\n\nAs a full-stack Web3 and Blockchain developer, I have over 6 years of experience in developing end-to-end software solutions. I am proficient in several programming languages including JavaScript, Python, Solidity, and Java. My experience with front-end frameworks includes React,js and I am skilled in back-end development using Node.js, Django, and Flask.\n\nIn the area of AI, I have over 5 years of experience with machine learning, natural language processing, computer vision, and deep learning. I am skilled in using popular libraries such as TensorFlow, Keras, PyTorch, and OpenCV to build AI-powered applications.\n\nAs a blockchain developer, I have over 4 years of experience in building decentralized applications using Ethereum, Solana, and Binance Smart Chain. I am proficient in writing smart contracts using Solidity and Rust, and I have experience with web3.js and other blockchain tools.\n\nIn addition to my technical skills, I have experience with cloud computing platforms, specifically Google Cloud Platform (GCP). I am skilled in using GCP tools such as App Engine, Compute Engine, Kubernetes, and Cloud Functions, and I have experience in deploying and managing applications on GCP.\n\nOverall, my diverse skillset and experience in web development, AI, and blockchain, make me a versatile and adaptable developer who can work on various projects.\n\nI always provide great quality work, meet all the deadlines and maintain excellent long-term cooperation with my clients.\",\"ChatGPT,Java,Python,Node.js,Django,React,Solana,Rust,Smart Contract,Blockchain Development,Blockchain,Solidity,React Native,Web Development,PostgreSQL\"\nCRMOZ,K.,USD,45.0,55,Kiev,Ukraine,Europe,Kyiv City,Eastern Europe,UTC+02:00 Israel,39000,Oleksandr,Zoho developer,\"Result-oriented expert in the implementation of Zoho products with more than 7 years of experience in Zoho, plus 15 years of app&web&db development for dozens of sectors.\nExpand the core functionality of Zoho apps. I can develop custom Zoho CRM (Books, Desk, etc.) integrations for your business needs (via API, Zapier, Make, etc.).\nAutomate your sales, marketing, and business processes and standardize working methods.\nCustomize standard modules, add additional functionalities, and make Zoho CRM work as you do. With custom views, filters, and fields.\n \n- ZeptoMail\n- Zoho CRM\n- Zoho Flow\n- Zoho Desk\n- Zoho Analytics\n- Zoho Books\n- Zoho Inventory\n- Zoho Creator\n- Zoho Recruit and Zoho People\n- Zoho Subscriptions\n- Orchestly\n- Zoho Catalyst and other API integration tools\",\"Zoho CRM,Zoho Creator,Data Migration,Business Process Model & Notation,API Development,Zoho Analytics,MySQL,API Integration,API,Plugin Development,Enterprise Resource Planning,Marketing Automation Strategy,Business Consulting,OAuth,Telemarketing\"\nStudio45 Creations  - TOP RATED PLUS,A.,USD,18.0,49,Mohali,India,Asia,PUNJAB,Southern Asia,\"UTC+05:30 Mumbai, Kolkata, Chennai, New Delhi\",160071,Vijay,Python/Django Developer,\"With 9 years of diverse experience as a Python/Django developer, I have a proven track record of successfully completing projects from design to implementation and integration. My expertise includes Python Django, PostgreSQL/SQLite, REST/GraphQL, as well as handling DevOps tasks such as CI/CD and Docker.\n\nPYTHON | DJANGO | REACT | VIRTUALIZATION | API | DATABASE DEVELOPMENT | \n\nI have developed entire front-end and back-end modules using Java and Python web frameworks and have been actively involved in maintenance, redesign, project management, and the development of different web applications. Working with a Scrum process and agile methodologies, I have developed a variety of websites, including educational platforms, e-commerce, file-sharing, job portals, and various others. I have a creative and performance-oriented work approach and I am adept at directing software development. \n\nBeing inspired by market-leading technology providers and constantly striving to keep up with the most recent developments in software design, I am also quite skilled and enthusiastic when it comes to UI and UX design.\n\nMy competencies include but are not limited to:\n\n\u272a Development of intuitive and cloud-based software products and applications using clean object-oriented design.\n\n\u272a Extensive expertise in working with Pyramid and Django Python frameworks, offering the optimum solution to deal with all kinds of project complexities.\n\n\u272a Development of custom Python applications, keeping in mind your business\u2019s key objectives while ensuring data security and protection and identifying data storage solutions.\n\n\u272a Strong focus on implementing agile, secure, scalable, performance-based Python development practices.\n\n\u272a Designing and creating RESTful APIs for internal and partner consumption\n\n\u272a Experience with databases such as MySQL or PostgreSQL\n\n\u272a Design and development of data management systems using MySQL\n\n\u272a Debugging and troubleshooting web applications.\n\nSkills\n\nPython, Python Script, Python Requests, Django, React/Redux, React Bootstrap, React Native, JavaScript, PHP, HTML5, CSS3, GraphQL, My SQL, PostgreSQL, jQuery, AJAX, Network Security, API Development, API Integration, Project Management, Scrum, Backend & Front Development.\n\n\u272a I am also an  Oracle Database PL/SQL Developer Certified Professional.\n\nWith a strong background in a disciplined software development lifecycle process (SDLC) and excellent analytical, programming, and problem-solving skills, I am all set to help you build a winning solution.\n\nI will be delighted to work with you on your project whether you are seeking a stand-alone developer or expanding your existing team.\n\nLet\u2019s connect...\",\"GraphQL,React Bootstrap,Python,React Native,React,API Development,Django Stack,Website Security,API Integration,Python Script,Django,Docker Compose,JavaScript\"\niRoid Solutions | Upwork Top Rated Agency | Clutch 2022 Award Winner IT Company,P.,USD,24.99,20,Surat,India,Asia,Gujarat,Southern Asia,\"UTC+05:30 Mumbai, Kolkata, Chennai, New Delhi\",395005,Daxesh,Top Rated Mobile App Developer | iOS Developer | Android Developer,\"\u2705  Rewarded as \ud83c\udfc6 \ud835\udde7\ud835\udde2\ud835\udde3 \ud835\udde5\ud835\uddd4\ud835\udde7\ud835\uddd8\ud835\uddd7 \ud835\udde3\ud835\udddf\ud835\udde8\ud835\udde6 \ud83c\udfc6 by Upwork  (\ud835\udde7\ud835\uddfc\ud835\uddfd \ud835\udfef%)\n\u2705  7000+ hours and still counting \n\u2705  100+ jobs completed successfully \n\u2705  100% Job Success (Client Satisfaction Ratio)\n\u2705  90% Clients Recommended to work with me\n\u2705  86% Long-term Repeat Clients\n\u2705  9+ Years Of Experience\n\u2705  High-Quality Work\n\u2705  Cost-Effective\n\u2705  Fluent English Verified\n\nWhether you are a \ud835\udde6\ud835\ude01\ud835\uddee\ud835\uddff\ud835\ude01\ud835\ude02\ud835\uddfd, \ud835\uddd8\ud835\ude00\ud835\ude01\ud835\uddee\ud835\uddef\ud835\uddf9\ud835\uddf6\ud835\ude00\ud835\uddf5\ud835\uddf2\ud835\uddf1 \ud835\uddd5\ud835\ude02\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\ude00\ud835\ude00 \ud835\uddfc\ud835\uddff \ud835\uddf9\ud835\uddfc\ud835\uddfc\ud835\uddf8\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf3\ud835\uddfc\ud835\uddff \ud835\udde0\ud835\udde9\ud835\udde3, you will get \ud835\udddb\ud835\uddf6\ud835\uddf4\ud835\uddf5-\ud835\udde4\ud835\ude02\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\ude01\ud835\ude06 \ud835\udde6\ud835\uddf2\ud835\uddff\ud835\ude03\ud835\uddf6\ud835\uddf0\ud835\uddf2\ud835\ude00 at an \ud835\uddd4\ud835\uddf3\ud835\uddf3\ud835\uddfc\ud835\uddff\ud835\uddf1\ud835\uddee\ud835\uddef\ud835\uddf9\ud835\uddf2 \ud835\uddd6\ud835\uddfc\ud835\ude00\ud835\ude01, \ud835\uddda\ud835\ude02\ud835\uddee\ud835\uddff\ud835\uddee\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\uddf2\ud835\uddf1. \nInterested? Just click on the '\ud835\udddc\ud835\uddfb\ud835\ude03\ud835\uddf6\ud835\ude01\ud835\uddf2 \ud835\ude01\ud835\uddfc \ud835\udddd\ud835\uddfc\ud835\uddef' button and lets discuss about your project and your vision.\n\nMY UPWORK CLIENTS SHARING THEIR EXPERIENCE WORKING WITH ME:\n\n\u2605\u2605\u2605\u2605\u2605 5.00\n\"\"Daxesh and his team are amazing. They are super customer oriented, very friendly, easy to work with and very experienced and skilled. It was an absolute pleasure working with Daxesh and his team and we will for sure engage them again and I can 100% recommend him to work with. The best freelancer / agency on Upwork I have worked with so far.\"\"\n- Christian, Germany\n\n\u2605\u2605\u2605\u2605\u2605 5.00\n\"\"Daxesh and his team were incredibly skilled, thoughtful, and responsive throughout the entire project! I couldn't have asked for a better team of developers to work with, and I greatly look forward to working with them again in the near future.\"\"\n- Aimun, USA \n\n\u2605\u2605\u2605\u2605\u2605 5.00\n\"\"Exceeds expectations and always a great experience. Even with an extended break between jobs, picked up where we left off with no problems whatsoever.\"\"\n- Beau, Canada\n\nThese are some of the testimonials taken directly from my Upwork profile \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\ude03\ud835\uddf2 \ud835\ude01\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\udddc \ud835\uddf0\ud835\uddee\ud835\uddfb \ud835\uddf1\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\ude03\ud835\uddf2\ud835\uddff \ud835\ude01\ud835\uddf5\ud835\uddf2 \ud835\uddfd\ud835\uddff\ud835\uddfc\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\ude01 \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\uddf4\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\ude01 \ud835\uddfe\ud835\ude02\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\ude01\ud835\ude06 \ud835\uddfc\ud835\uddfb \ud835\ude01\ud835\uddf6\ud835\uddfa\ud835\uddf2 \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\ude07\ud835\uddf2\ud835\uddff\ud835\uddfc \ud835\ude04\ud835\uddfc\ud835\uddff\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude00. \ud835\uddd9\ud835\uddfc\ud835\uddff \ud835\uddfa\ud835\uddf2, \ud835\uddf0\ud835\uddf9\ud835\uddf6\ud835\uddf2\ud835\uddfb\ud835\ude01 \ud835\ude00\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\ude00\ud835\uddf3\ud835\uddee\ud835\uddf0\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddf0\ud835\uddfc\ud835\uddfa\ud835\uddf2\ud835\ude00 \ud835\uddf3\ud835\uddf6\ud835\uddff\ud835\ude00\ud835\ude01.\n\nDuring the last 9+ years, I have developed many different types of iPhone and Android apps.\n\ud835\uddd9\ud835\uddfc\ud835\uddfc\ud835\uddf1 \ud835\uddf1\ud835\uddf2\ud835\uddf9\ud835\uddf6\ud835\ude03\ud835\uddf2\ud835\uddff\ud835\ude06 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde2\ud835\uddfb-\ud835\uddf1\ud835\uddf2\ud835\uddfa\ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\uddd4\ud835\uddfd\ud835\uddfd, \ud835\udddb\ud835\uddfc\ud835\uddfa\ud835\uddf2 \ud835\ude00\ud835\uddf2\ud835\uddff\ud835\ude03\ud835\uddf6\ud835\uddf0\ud835\uddf2\ud835\ude00 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udddb\ud835\uddee\ud835\uddfb\ud835\uddf1\ud835\ude06 \ud835\uddfa\ud835\uddee\ud835\uddfb \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde6\ud835\uddee\ud835\uddf9\ud835\uddfc\ud835\uddfb \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddd5\ud835\uddf2\ud835\uddee\ud835\ude02\ud835\ude01\ud835\ude06 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde7\ud835\uddee\ud835\ude05\ud835\uddf6 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde5\ud835\uddf6\ud835\uddf1\ud835\uddf2 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddd8-\ud835\uddf0\ud835\uddfc\ud835\uddfa\ud835\uddfa\ud835\uddf2\ud835\uddff\ud835\uddf0\ud835\uddf2 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udddb\ud835\uddf2\ud835\uddee\ud835\uddf9\ud835\ude01\ud835\uddf5 \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\uddf3\ud835\uddf6\ud835\ude01\ud835\uddfb\ud835\uddf2\ud835\ude00\ud835\ude00 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde7\ud835\uddff\ud835\uddee\ud835\ude03\ud835\uddf2\ud835\uddf9 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde0\ud835\ude02\ud835\ude00\ud835\uddf6\ud835\uddf0 \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde9\ud835\uddf6\ud835\uddf1\ud835\uddf2\ud835\uddfc \ud835\ude00\ud835\ude01\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\uddfa\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddd9\ud835\uddf6\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\uddf0\ud835\uddf5 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde6\ud835\uddf5\ud835\uddfc\ud835\uddfd\ud835\uddfd\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde7\ud835\uddf2\ud835\uddf9\ud835\uddf2\ud835\uddfa\ud835\uddf2\ud835\uddf1\ud835\uddf6\ud835\uddf0\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddd9\ud835\uddf6\ud835\uddfb\ud835\uddee\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\uddd4\ud835\uddfd\ud835\uddfd, \ud835\udde5\ud835\uddf2\ud835\uddee\ud835\uddf9 \ud835\uddf2\ud835\ude00\ud835\ude01\ud835\uddee\ud835\ude01\ud835\uddf2 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde6\ud835\uddfc\ud835\uddf0\ud835\uddf6\ud835\uddee\ud835\uddf9 \ud835\uddfa\ud835\uddf2\ud835\uddf1\ud835\uddf6\ud835\uddee \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddd6\ud835\uddf5\ud835\uddee\ud835\ude01 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddd5\ud835\ude02\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\ude00\ud835\ude00 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde0\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddee\ud835\uddf4\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde2\ud835\uddfb\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\uddf0\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\ude00\ud835\uddf2\ud835\ude00 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde2\ud835\uddfb\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\uddf2\ud835\uddf1\ud835\ude02\ud835\uddf0\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddf9\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde7\ud835\ude02\ud835\ude01\ud835\uddfc\ud835\uddff \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udddb\ud835\uddfc\ud835\ude01\ud835\uddf2\ud835\uddf9 \ud835\uddef\ud835\uddfc\ud835\uddfc\ud835\uddf8\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\udde2\ud835\uddfb\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2 \ud835\uddf4\ud835\uddff\ud835\uddfc\ud835\uddf0\ud835\uddf2\ud835\uddff\ud835\ude06 \ud835\uddfc\ud835\uddff\ud835\uddf1\ud835\uddf2\ud835\uddff \ud835\uddee\ud835\uddfd\ud835\uddfd, \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\uddfa\ud835\uddee\ud835\uddfb\ud835\ude06 \ud835\uddfa\ud835\uddfc\ud835\uddff\ud835\uddf2.\n\n\ud83d\udd39 Why you should hire me? \n\u2714 I always work towards your success\n\u2714 Quality work at affordable prices\n\u2714 Strong Android, iPhone, and backend architecture and database knowledge\n\u2714 Develop applications that support all devices\n\u2714 Delivering product on/before time\n\u2714 Always up-to-date with the latest technologies\n\n\ud83d\udd39 My core skills and experiences:\n\u25c9 Interaction with backend - Rest API, JSON, XML, PHP, Node.js, Python\n\u25c9 Databases / Data Management - SQLite, MySQL, Parse, MongoDB\n\u25c9 Social Integration - Facebook, Twitter, Youtube, Instagram, Snapchat, Tinder\n\u25c9 Chat - XMPP, Socket, Firebase, GetStream, Twilio\n\u25c9 Version Control Tools - Github, Bitbucket, Gitlab\n\u25c9 Google Map, Apple Map kit, Geolocation, GPS, Location Tracking\n\u25c9 Audio / Video Live Streaming\n\u25c9 Firebase APIs\n\u25c9 Paypal, Braintree, Stripe Payment Gateway \n\u25c9 QR Codes, Bar Codes, VIN Scanner, NFC Tags Reading\n\u25c9 In-App Purchase, Advertisement, Push Notifications, GCM, Google Analytics\n\u25c9 Photo & Video Sharing, Editor, Filter\n\n\ud83d\udd39 We work with the below technologies for development:\n\ud835\uddd4\ud835\uddfb\ud835\uddf1\ud835\uddff\ud835\uddfc\ud835\uddf6\ud835\uddf1: Kotlin, Java\n\ud835\uddf6\ud835\udde2\ud835\udde6: Swift, Objective-C\n\ud835\uddd6\ud835\uddff\ud835\uddfc\ud835\ude00\ud835\ude00-\ud835\uddfd\ud835\uddf9\ud835\uddee\ud835\ude01\ud835\uddf3\ud835\uddfc\ud835\uddff\ud835\uddfa (\ud835\udddb\ud835\ude06\ud835\uddef\ud835\uddff\ud835\uddf6\ud835\uddf1): Flutter (Dart)\n\ud835\uddea\ud835\uddf2\ud835\uddef / \ud835\uddd5\ud835\uddee\ud835\uddf0\ud835\uddf8\ud835\uddf2\ud835\uddfb\ud835\uddf1: PHP, Node.js, Python, Laravel, WordPress, Express.js, Nest,js, Django\n\ud835\uddd9\ud835\uddff\ud835\uddfc\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\uddfb\ud835\uddf1: React.Js, HTML5, CSS3, BootStrap, JavaScript, jQuery\nAugmented Reality (AR), Internet Of Things (IoT), Beacon, Ricoh Theta 360\n\n\ud83d\udcac \ud835\uddd9\ud835\udde5\ud835\uddd8\ud835\uddd8 \ud835\uddd6\ud835\udde2\ud835\udde1\ud835\udde6\ud835\udde8\ud835\udddf\ud835\udde7\ud835\uddd4\ud835\udde7\ud835\udddc\ud835\udde2\ud835\udde1: \nMy consultation is \ud835\uddf3\ud835\uddff\ud835\uddf2\ud835\uddf2. Let's schedule an introductory call and discover the possibilities of working together. \nAnd even if we don't work together, you will get some tips/ideas that can be helpful to your project.\nJust click on the '\ud835\udddc\ud835\uddfb\ud835\ude03\ud835\uddf6\ud835\ude01\ud835\uddf2 \ud835\ude01\ud835\uddfc \ud835\udddd\ud835\uddfc\ud835\uddef' button and experience yourself.\n\n\ud835\udde7\ud835\uddf5\ud835\uddee\ud835\uddfb\ud835\uddf8 \ud835\ude06\ud835\uddfc\ud835\ude02 for spending your $$ to review my profile because TIME IS MONEY :)\",\"iPhone App Development,Android App Development,iOS Development,Node.js,Flutter,iPad App Development,Laravel,Swift,React Native,Android,PHP,Kotlin,Apple Xcode,Front-End Development,Mobile App\"", "sourceType": "source.csv", "outputFields": ["firstLine"]}, "actor_id": "curious_coder/ai-assistant"}, "kirada/email-scraper": {"id": 1440, "url": "https://apify.com/kirada/email-scraper/api/client/nodejs", "title": "Apify API and Contact Details Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"websites\": \"https://cience.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"kirada/email-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"websites": "https://cience.com"}, "actor_id": "kirada/email-scraper"}, "tkapler/deepl-actor": {"id": 1442, "url": "https://apify.com/tkapler/deepl-actor/api/client/nodejs", "title": "Apify API and DeepL (AI translation) Actor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"tkapler/deepl-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "tkapler/deepl-actor"}, "lucier/buzz-feed-scraper": {"id": 1443, "url": "https://apify.com/lucier/buzz-feed-scraper/api/client/nodejs", "title": "Apify API and Buzz Feed Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.buzzfeed.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lucier/buzz-feed-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.buzzfeed.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "lucier/buzz-feed-scraper"}, "epctex/youtube-video-downloader": {"id": 1444, "url": "https://apify.com/epctex/youtube-video-downloader/api/client/nodejs", "title": "Apify API and Youtube Video Downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.youtube.com/watch?v=etEJKUNqKJk&ab_channel=LineDeGuzman\"\n    ],\n    \"quality\": \"720\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/youtube-video-downloader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "quality": "720", "startUrls": ["https://www.youtube.com/watch?v=etEJKUNqKJk&ab_channel=LineDeGuzman"]}, "actor_id": "epctex/youtube-video-downloader"}, "bares43/rmsystem": {"id": 1445, "url": "https://apify.com/bares43/rmsystem/api/client/nodejs", "title": "Apify API and RMSystem stocks scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.rmsystem.cz/kurzy-online/akcie/easyclick\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bares43/rmsystem\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.rmsystem.cz/kurzy-online/akcie/easyclick"}]}, "actor_id": "bares43/rmsystem"}, "zyberg/qpublic": {"id": 1446, "url": "https://apify.com/zyberg/qpublic/api/client/nodejs", "title": "Apify API and QPublic interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zyberg/qpublic\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "zyberg/qpublic"}, "petr_cermak/save-linkedin-code": {"id": 1447, "url": "https://apify.com/petr_cermak/save-linkedin-code/api/client/nodejs", "title": "Apify API and Save Linkedin PIN Code interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/save-linkedin-code\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petr_cermak/save-linkedin-code"}, "web.harvester/twitter-users-scraper": {"id": 1448, "url": "https://apify.com/web.harvester/twitter-users-scraper/api/client/nodejs", "title": "Apify API and Twitter Users Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"handles\": [\n        \"elonmusk\"\n    ],\n    \"userQueries\": [],\n    \"profilesDesired\": 10,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"web.harvester/twitter-users-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"handles": ["elonmusk"], "proxyConfig": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "userQueries": [], "profilesDesired": 10}, "actor_id": "web.harvester/twitter-users-scraper"}, "microworlds/lighthouse-audit": {"id": 1449, "url": "https://apify.com/microworlds/lighthouse-audit/api/client/nodejs", "title": "Apify API and Lighthouse Audit interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urls\": [\n        \"https://apify.com/\"\n    ],\n    \"device\": \"desktop\",\n    \"maxRequestsPerCrawl\": 500,\n    \"maxConcurrency\": 5,\n    \"maxRequestRetries\": 3,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"microworlds/lighthouse-audit\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": ["https://apify.com/"], "device": "desktop", "proxyConfig": {"useApifyProxy": true}, "maxConcurrency": 5, "maxRequestRetries": 3, "maxRequestsPerCrawl": 500}, "actor_id": "microworlds/lighthouse-audit"}, "jupri/streeteasy-scraper": {"id": 1450, "url": "https://apify.com/jupri/streeteasy-scraper/api/client/nodejs", "title": "Apify API and StreetEasy Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": [\n        \"Manhattan\"\n    ],\n    \"search_type\": \"sale\",\n    \"limit\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/streeteasy-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"limit": 5, "location": ["Manhattan"], "search_type": "sale"}, "actor_id": "jupri/streeteasy-scraper"}, "curious_coder/twitter-community-members-scraper": {"id": 1451, "url": "https://apify.com/curious_coder/twitter-community-members-scraper/api/client/nodejs", "title": "Apify API and Twitter community members scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"communityUrl\": \"https://twitter.com/i/communities/1506800881525829633\",\n    \"minDelay\": 2,\n    \"maxDelay\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/twitter-community-members-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDelay": 5, "minDelay": 2, "communityUrl": "https://twitter.com/i/communities/1506800881525829633"}, "actor_id": "curious_coder/twitter-community-members-scraper"}, "zuzka/india-times-scraper": {"id": 1452, "url": "https://apify.com/zuzka/india-times-scraper/api/client/nodejs", "title": "Apify API and India Times Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.indiatimes.com/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/india-times-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.indiatimes.com/"}], "maxArticlesPerCrawl": 100}, "actor_id": "zuzka/india-times-scraper"}, "qpayre/medium-scraper": {"id": 1453, "url": "https://apify.com/qpayre/medium-scraper/api/client/nodejs", "title": "Apify API and Medium Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"qpayre/medium-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "qpayre/medium-scraper"}, "lukaskrivka/contact-details-merge-deduplicate": {"id": 1454, "url": "https://apify.com/lukaskrivka/contact-details-merge-deduplicate/api/client/nodejs", "title": "Apify API and Contact Details Merge & Deduplicate interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/contact-details-merge-deduplicate\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukaskrivka/contact-details-merge-deduplicate"}, "genial_candlestand/turo-scraper": {"id": 1455, "url": "https://apify.com/genial_candlestand/turo-scraper/api/client/nodejs", "title": "Apify API and Turo Car Listing Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://turo.com/us/en/search?country=US&defaultZoomLevel=11&delivery=true&deliveryLocationType=airport&endDate=08%2F12%2F2023&endTime=10%3A00&isMapSearch=false&itemsPerPage=200&latitude=42.36561&location=BOS%20-%20Boston%20Logan%20International%20Airport&locationType=AIRPORT&longitude=-71.00956&pickupType=ALL&placeId=ChIJN0na1RRw44kRRFEtH8OUkww&sortType=RELEVANCE&startDate=08%2F09%2F2023&startTime=10%3A00&useDefaultMaximumDistance=true\"\n        }\n    ],\n    \"queries\": [\n        \"Boston, MA\"\n    ],\n    \"limit\": 15,\n    \"minConcurrency\": 1,\n    \"maxConcurrency\": 3,\n    \"maxRequestRetries\": 2,\n    \"maxRequestsPerCrawl\": 1000,\n    \"handlePageTimeoutSecs\": 60\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"genial_candlestand/turo-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"limit": 15, "queries": ["Boston, MA"], "startUrls": [{"url": "https://turo.com/us/en/search?country=US&defaultZoomLevel=11&delivery=true&deliveryLocationType=airport&endDate=08%2F12%2F2023&endTime=10%3A00&isMapSearch=false&itemsPerPage=200&latitude=42.36561&location=BOS%20-%20Boston%20Logan%20International%20Airport&locationType=AIRPORT&longitude=-71.00956&pickupType=ALL&placeId=ChIJN0na1RRw44kRRFEtH8OUkww&sortType=RELEVANCE&startDate=08%2F09%2F2023&startTime=10%3A00&useDefaultMaximumDistance=true"}], "maxConcurrency": 3, "minConcurrency": 1, "maxRequestRetries": 2, "maxRequestsPerCrawl": 1000, "handlePageTimeoutSecs": 60}, "actor_id": "genial_candlestand/turo-scraper"}, "hamza.alwan/gab-scraper": {"id": 1456, "url": "https://apify.com/hamza.alwan/gab-scraper/api/client/nodejs", "title": "Apify API and Gab Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://gab.com/Ori\"\n        }\n    ],\n    \"desiredPostsCount\": 100,\n    \"desiredCommentsCount\": 0,\n    \"commentsSortBy\": \"MOST_LIKED\",\n    \"accountPostsSortBy\": \"NEWEST\",\n    \"groupPostsSortBy\": \"NEWEST\",\n    \"desiredAccountsCount\": 100,\n    \"desiredGroupsCount\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/gab-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://gab.com/Ori"}], "commentsSortBy": "MOST_LIKED", "groupPostsSortBy": "NEWEST", "desiredPostsCount": 100, "accountPostsSortBy": "NEWEST", "desiredGroupsCount": 100, "desiredAccountsCount": 100, "desiredCommentsCount": 0}, "actor_id": "hamza.alwan/gab-scraper"}, "medh/imdb-titles-scraper-by-company": {"id": 1457, "url": "https://apify.com/medh/imdb-titles-scraper-by-company/api/client/nodejs", "title": "Apify API and Scrape a production company movies in IMDB interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"CompanyId\": \"co0591889\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"medh/imdb-titles-scraper-by-company\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"CompanyId": "co0591889"}, "actor_id": "medh/imdb-titles-scraper-by-company"}, "pocesar/request-list-bridge": {"id": 1458, "url": "https://apify.com/pocesar/request-list-bridge/api/client/nodejs", "title": "Apify API and RequestList Bridge interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"targetStartUrlsProperty\": \"startUrls\",\n    \"map\": (req) => {\n    \treturn req;\n    },\n    \"filter\": (req) => {\n    \treturn true;\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/request-list-bridge\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"targetStartUrlsProperty": "startUrls"}, "actor_id": "pocesar/request-list-bridge"}, "katerinahronik/covid-china": {"id": 1459, "url": "https://apify.com/katerinahronik/covid-china/api/client/nodejs", "title": "Apify API and Coronavirus stats in China interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"email\": \"example@example.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"katerinahronik/covid-china\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"email": "example@example.com"}, "actor_id": "katerinahronik/covid-china"}, "jindrich.bar/playwright-test": {"id": 1460, "url": "https://apify.com/jindrich.bar/playwright-test/api/client/nodejs", "title": "Apify API and Playwright Test Runner interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"testCode\": \"import { test, expect } from '@playwright/test';    \\n    test('has title', async ({ page }) => {      \\n      await page.goto('https://playwright.dev/');\\n      // Expect a title \\\"to contain\\\" a substring.\\n      await expect(page).toHaveTitle(/Playwright/);    \\n    });\\n    test('get started link', async ({ page }) => {\\n        await page.goto('https://playwright.dev/');\\n        // Click the get started link.\\n        await page.getByRole('link', { name: 'Get started' }).click();\\n        // Expects the URL to contain intro. \\n        await expect(page).toHaveURL(/.*intro/);    \\n    });\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jindrich.bar/playwright-test\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"testCode": ""}, "actor_id": "jindrich.bar/playwright-test"}, "dtrungtin/covid-ch": {"id": 1461, "url": "https://apify.com/dtrungtin/covid-ch/api/client/nodejs", "title": "Apify API and Coronavirus stats in Switzerland interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/covid-ch\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "dtrungtin/covid-ch"}, "valek.josef/cypress-test-runner": {"id": 1462, "url": "https://apify.com/valek.josef/cypress-test-runner/api/client/nodejs", "title": "Apify API and Cypress Test Runner interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"specCode\": `describe('spec', () => {\n            it('test that should succeed', () => {\n                cy.visit('/');\n                cy.get('h1').should('contain', 'Example Domain');\n                cy.screenshot();\n            });\n        \n             it('test that should fail', () => {\n                cy.visit('/');\n                cy.get('h1', { timeout: 1000 }).should('contain', 'Some different text');\n            });\n        });`,\n    \"baseUrl\": \"https://example.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"valek.josef/cypress-test-runner\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"baseUrl": "https://example.com", "specCode": "describe('spec', () => {\n            it('test that should succeed', () => {\n                cy.visit('/');\n                cy.get('h1').should('contain', 'Example Domain');\n                cy.screenshot();\n            });\n        \n             it('test that should fail', () => {\n                cy.visit('/');\n                cy.get('h1', { timeout: 1000 }).should('contain', 'Some different text');\n            });\n        });"}, "actor_id": "valek.josef/cypress-test-runner"}, "jannovotny/status-page": {"id": 1463, "url": "https://apify.com/jannovotny/status-page/api/client/nodejs", "title": "Apify API and Status Page interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"charts\": [\n        {\n            \"id\": \"my-super-chart-1\",\n            \"name\": \"My chart\",\n            \"showTable\": false\n        }\n    ],\n    \"rebrandly\": {\n        \"id\": \"LinkId\",\n        \"apiKey\": \"YourApiKey\",\n        \"workspace\": \"YourWorkspaceId\",\n        \"title\": \"My cool dashboard\"\n    },\n    \"intervals\": [\n        \"day\",\n        \"week\",\n        \"two-weeks\",\n        \"month\",\n        \"two-months\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jannovotny/status-page\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"charts": [{"id": "my-super-chart-1", "name": "My chart", "showTable": false}], "intervals": ["day", "week", "two-weeks", "month", "two-months"], "rebrandly": {"id": "LinkId", "title": "My cool dashboard", "apiKey": "YourApiKey", "workspace": "YourWorkspaceId"}}, "actor_id": "jannovotny/status-page"}, "pocesar/aggregate-fields": {"id": 1464, "url": "https://apify.com/pocesar/aggregate-fields/api/client/nodejs", "title": "Apify API and Aggregate Fields interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/aggregate-fields\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "pocesar/aggregate-fields"}, "onidivo/restart-run": {"id": 1465, "url": "https://apify.com/onidivo/restart-run/api/client/nodejs", "title": "Apify API and Restart run interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"onidivo/restart-run\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "onidivo/restart-run"}, "mshopik/vuori-clothing-scraper": {"id": 1466, "url": "https://apify.com/mshopik/vuori-clothing-scraper/api/client/nodejs", "title": "Apify API and Vuori Clothing Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/vuori-clothing-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/vuori-clothing-scraper"}, "rrortega/google-currency-exchange": {"id": 1467, "url": "https://apify.com/rrortega/google-currency-exchange/api/client/nodejs", "title": "Apify API and Google Currency Exchange interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"rrortega/google-currency-exchange\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "rrortega/google-currency-exchange"}, "pocesar/download-linkedin-video": {"id": 1468, "url": "https://apify.com/pocesar/download-linkedin-video/api/client/nodejs", "title": "Apify API and LinkedIn Video Downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.linkedin.com/posts/apifytech_tech-cto-leadership-activity-6911945063627464704-LlVH\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    },\n    \"maxRequestRetries\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/download-linkedin-video\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://www.linkedin.com/posts/apifytech_tech-cto-leadership-activity-6911945063627464704-LlVH"}], "maxRequestRetries": 10}, "actor_id": "pocesar/download-linkedin-video"}, "onidivo/restore-failed-requests": {"id": 1469, "url": "https://apify.com/onidivo/restore-failed-requests/api/client/nodejs", "title": "Apify API and Restore failed requests interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"onidivo/restore-failed-requests\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "onidivo/restore-failed-requests"}, "jakubbalada/html-renderer": {"id": 1470, "url": "https://apify.com/jakubbalada/html-renderer/api/client/nodejs", "title": "Apify API and Html Renderer interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jakubbalada/html-renderer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jakubbalada/html-renderer"}, "mshopik/apt2b-scraper": {"id": 1471, "url": "https://apify.com/mshopik/apt2b-scraper/api/client/nodejs", "title": "Apify API and Apt2B Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/apt2b-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/apt2b-scraper"}, "onidivo/covid-be": {"id": 1480, "url": "https://apify.com/onidivo/covid-be/api/client/nodejs", "title": "Apify API and Coronavirus stats in Belgium interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"onidivo/covid-be\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "onidivo/covid-be"}, "epctex/twitter-video-downloader": {"id": 1472, "url": "https://apify.com/epctex/twitter-video-downloader/api/client/nodejs", "title": "Apify API and Twitter Video Downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://twitter.com/tina__nigro/status/1677753111161569281\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/twitter-video-downloader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "startUrls": ["https://twitter.com/tina__nigro/status/1677753111161569281"]}, "actor_id": "epctex/twitter-video-downloader"}, "qpayre/substack-scraper": {"id": 1473, "url": "https://apify.com/qpayre/substack-scraper/api/client/nodejs", "title": "Apify API and Substack Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"qpayre/substack-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "qpayre/substack-scraper"}, "curious_coder/reddit-scraper": {"id": 1474, "url": "https://apify.com/curious_coder/reddit-scraper/api/client/nodejs", "title": "Apify API and Reddit scraper and API interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/reddit-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDelay": 3, "minDelay": 1}, "actor_id": "curious_coder/reddit-scraper"}, "mshopik/bookxcess-online-scraper": {"id": 1475, "url": "https://apify.com/mshopik/bookxcess-online-scraper/api/client/nodejs", "title": "Apify API and BookXcess Online Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bookxcess-online-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bookxcess-online-scraper"}, "mscraper/vk-comments-scraper": {"id": 1476, "url": "https://apify.com/mscraper/vk-comments-scraper/api/client/nodejs", "title": "Apify API and VK Comments Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://vk.com/wall-201571192_2798\"\n        }\n    ],\n    \"resultsLimit\": 20,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/vk-comments-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://vk.com/wall-201571192_2798"}], "resultsLimit": 20}, "actor_id": "mscraper/vk-comments-scraper"}, "petr_cermak/json-compare": {"id": 1477, "url": "https://apify.com/petr_cermak/json-compare/api/client/nodejs", "title": "Apify API and Json Compare interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/json-compare\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petr_cermak/json-compare"}, "curious_coder/imdb-scraper": {"id": 1478, "url": "https://apify.com/curious_coder/imdb-scraper/api/client/nodejs", "title": "Apify API and Imdb Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"scrapeReviews\",\n    \"scrapeReviews.url\": \"https://www.imdb.com/title/tt2024544/reviews\",\n    \"startPage\": 1,\n    \"count\": 10,\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/imdb-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"count": 10, "action": "scrapeReviews", "maxDelay": 3, "minDelay": 1, "startPage": 1, "scrapeReviews.url": "https://www.imdb.com/title/tt2024544/reviews"}, "actor_id": "curious_coder/imdb-scraper"}, "mtrunkat/delete-untitled-actors": {"id": 1479, "url": "https://apify.com/mtrunkat/delete-untitled-actors/api/client/nodejs", "title": "Apify API and Delete Untitled Actors interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mtrunkat/delete-untitled-actors\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "mtrunkat/delete-untitled-actors"}, "undrtkr984/softwareadvicescrapereviews": {"id": 1481, "url": "https://apify.com/undrtkr984/softwareadvicescrapereviews/api/client/nodejs", "title": "Apify API and Softwareadvicescrapereviews interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.softwareadvice.com/fleet-management/gps-insight-profile/\"\n        }\n    ],\n    \"maxRequestRetries\": 2\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"undrtkr984/softwareadvicescrapereviews\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.softwareadvice.com/fleet-management/gps-insight-profile/"}], "maxRequestRetries": 2}, "actor_id": "undrtkr984/softwareadvicescrapereviews"}, "bebich/sreality-statistics": {"id": 1482, "url": "https://apify.com/bebich/sreality-statistics/api/client/nodejs", "title": "Apify API and Sreality statistics interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"AREA\": [\n        \"Praha\",\n        \"St\u0159edo\u010desk\u00fd\"\n    ],\n    \"CITY\": [\n        \"\u010cesk\u00e9 Bud\u011bjovice\",\n        \"Plze\u0148\",\n        \"Praha 1\",\n        \"Praha 2\",\n        \"Brno\",\n        \"Ostrava\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bebich/sreality-statistics\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"AREA": ["Praha", "St\u0159edo\u010desk\u00fd"], "CITY": ["\u010cesk\u00e9 Bud\u011bjovice", "Plze\u0148", "Praha 1", "Praha 2", "Brno", "Ostrava"]}, "actor_id": "bebich/sreality-statistics"}, "petr_cermak/crawler-results-deduplicate": {"id": 1483, "url": "https://apify.com/petr_cermak/crawler-results-deduplicate/api/client/nodejs", "title": "Apify API and Crawler Results Deduplicate interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/crawler-results-deduplicate\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petr_cermak/crawler-results-deduplicate"}, "epctex/threads-video-downloader": {"id": 1484, "url": "https://apify.com/epctex/threads-video-downloader/api/client/nodejs", "title": "Apify API and Threads Video Downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.threads.net/@paramore/post/CxvoXXBu6ao\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/threads-video-downloader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "startUrls": ["https://www.threads.net/@paramore/post/CxvoXXBu6ao"]}, "actor_id": "epctex/threads-video-downloader"}, "mshopik/headphone-zone-scraper": {"id": 1485, "url": "https://apify.com/mshopik/headphone-zone-scraper/api/client/nodejs", "title": "Apify API and Headphone Zone Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/headphone-zone-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/headphone-zone-scraper"}, "natanielsantos/sainsbury-s-scraper": {"id": 1486, "url": "https://apify.com/natanielsantos/sainsbury-s-scraper/api/client/nodejs", "title": "Apify API and Sainsbury's Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": [\n        {\n            \"url\": \"https://www.sainsburys.co.uk/gol-ui/product/ben---jerrys-ice-cream-cookie-dough-500ml\"\n        },\n        {\n            \"url\": \"https://www.sainsburys.co.uk/gol-ui/SearchResults/fruit\"\n        }\n    ],\n    \"max_items\": 30,\n    \"proxySettings\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"natanielsantos/sainsbury-s-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"max_items": 30, "start_urls": [{"url": "https://www.sainsburys.co.uk/gol-ui/product/ben---jerrys-ice-cream-cookie-dough-500ml"}, {"url": "https://www.sainsburys.co.uk/gol-ui/SearchResults/fruit"}], "proxySettings": {"useApifyProxy": true}}, "actor_id": "natanielsantos/sainsbury-s-scraper"}, "valek.josef/append-to-dataset": {"id": 1514, "url": "https://apify.com/valek.josef/append-to-dataset/api/client/nodejs", "title": "Apify API and Append to dataset interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"valek.josef/append-to-dataset\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "valek.josef/append-to-dataset"}, "inquisitive_sarangi/facebook-marketplace-scraper": {"id": 1487, "url": "https://apify.com/inquisitive_sarangi/facebook-marketplace-scraper/api/client/nodejs", "title": "Apify API and Facebook Marketplace Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchUrls\": [\n        \"https://www.facebook.com/marketplace/telaviv/vehicles/\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"lat\": \"40.68\",\n    \"lng\": \"-73.87\",\n    \"radius\": \"10\",\n    \"maxPage\": 1,\n    \"maxConcurrency\": 5,\n    \"maxRequestsPerMinute\": 200\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"inquisitive_sarangi/facebook-marketplace-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"lat": "40.68", "lng": "-73.87", "proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "radius": "10", "maxPage": 1, "searchUrls": ["https://www.facebook.com/marketplace/telaviv/vehicles/"], "maxConcurrency": 5, "maxRequestsPerMinute": 200}, "actor_id": "inquisitive_sarangi/facebook-marketplace-scraper"}, "theo/the-guardian-scraper": {"id": 1488, "url": "https://apify.com/theo/the-guardian-scraper/api/client/nodejs", "title": "Apify API and The Guardian Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.theguardian.com/international\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"theo/the-guardian-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.theguardian.com/international"}], "maxArticlesPerCrawl": 100}, "actor_id": "theo/the-guardian-scraper"}, "jupri/stepstone-scraper": {"id": 1489, "url": "https://apify.com/jupri/stepstone-scraper/api/client/nodejs", "title": "Apify API and Stepstone Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"python\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/stepstone-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "python"}, "actor_id": "jupri/stepstone-scraper"}, "petr_cermak/mssql-insert": {"id": 1490, "url": "https://apify.com/petr_cermak/mssql-insert/api/client/nodejs", "title": "Apify API and Microsoft SQL Server Insert interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/mssql-insert\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petr_cermak/mssql-insert"}, "mshopik/yellow-octopus-scraper": {"id": 1491, "url": "https://apify.com/mshopik/yellow-octopus-scraper/api/client/nodejs", "title": "Apify API and Yellow Octopus Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/yellow-octopus-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/yellow-octopus-scraper"}, "autodomme/loyalfans-profile-scraper": {"id": 1492, "url": "https://apify.com/autodomme/loyalfans-profile-scraper/api/client/nodejs", "title": "Apify API and Loyalfans Profile Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"profiles\": [\n        \"autosissy\",\n        \"goddesslivs\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"autodomme/loyalfans-profile-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"profiles": ["autosissy", "goddesslivs"]}, "actor_id": "autodomme/loyalfans-profile-scraper"}, "lexis-solutions/bing-ads-scraper": {"id": 1493, "url": "https://apify.com/lexis-solutions/bing-ads-scraper/api/client/nodejs", "title": "Apify API and Bing Ads Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"apify\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lexis-solutions/bing-ads-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "apify"}, "actor_id": "lexis-solutions/bing-ads-scraper"}, "diogobruni/bancodeportugal": {"id": 1524, "url": "https://apify.com/diogobruni/bancodeportugal/api/client/nodejs", "title": "Apify API and BancoDePortugal interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"diogobruni/bancodeportugal\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "diogobruni/bancodeportugal"}, "apify/ai-web-agent": {"id": 1494, "url": "https://apify.com/apify/ai-web-agent/api/client/nodejs", "title": "Apify API and AI Web Agent interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrl\": \"https://apify.com\",\n    \"instructions\": \"Go to the pricing page, extract information about all pricing plans, and save it into dataset.\",\n    \"model\": \"gpt-3.5-turbo-16k\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/ai-web-agent\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"model": "gpt-3.5-turbo-16k", "startUrl": "https://apify.com", "instructions": "Go to the pricing page, extract information about all pricing plans, and save it into dataset.", "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "apify/ai-web-agent"}, "anchor/boulanger": {"id": 1495, "url": "https://apify.com/anchor/boulanger/api/client/nodejs", "title": "Apify API and Boulanger interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.boulanger.com/resultats?tr=samsung\"\n        }\n    ],\n    \"pageFunction\": async function pageFunction(context) {\n        let data = {}\n        let userData = context.request.userData\n        data.url = context.request.url\n        data.label = userData.label\n    \n        let items = await context.page.evaluate(() => {\n            const item = $('.product-item')\n            // const item = $('.rm-product')\n            const itemInfo = item.map(function(i,elem) {\n                let obj = {}\n                obj.title = $(this).find('h2').text()\n                obj.sponsored = false\n                obj.price = $(this).find('.price__amount').text()\n                obj.img = $(this).find('img').attr('src')\n                obj.rank = i+1\n                return obj\n            }).get()\n    \n            const itemSponsored = $('.rm-product')\n            const itemInfoSponsored = itemSponsored.map(function(i,elem) {\n                let obj = {}\n                obj.title = $(this).find('h2').text()\n                obj.sponsored = true\n                obj.price = $(this).find('.rm_price').text()\n                obj.img = $(this).find('img').attr('src')\n                obj.rank = i+1\n                return obj\n            }).get()\n    \n            // return [...itemInfo,...itemInfoSponsored]\n            const allitems = itemInfoSponsored.concat(itemInfo)\n            return allitems\n        })\n        \n        let itemsWithDataProp = items.map(obj => { \n            for(const key of Object.keys(data) ){\n                obj[key] = data[key]\n            }\n            return obj\n        })\n        return itemsWithDataProp;\n    },\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"FR\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"anchor/boulanger\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.boulanger.com/resultats?tr=samsung"}], "proxyConfiguration": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "FR"}}, "actor_id": "anchor/boulanger"}, "canadesk/spyfu": {"id": 1496, "url": "https://apify.com/canadesk/spyfu/api/client/nodejs", "title": "Apify API and Spyfu interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.apify.com/\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/spyfu\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.apify.com/", "proxy": {"useApifyProxy": true}}, "actor_id": "canadesk/spyfu"}, "mscraper/hotels-reviews-scraper": {"id": 1497, "url": "https://apify.com/mscraper/hotels-reviews-scraper/api/client/nodejs", "title": "Apify API and Hotels.com Reviews Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.hotels.com/ho118903/the-strat-hotel-casino-skypod-bw-premier-collection-las-vegas-united-states-of-america/\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\",\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    },\n    \"resultsLimit\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/hotels-reviews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "startUrls": [{"url": "https://www.hotels.com/ho118903/the-strat-hotel-casino-skypod-bw-premier-collection-las-vegas-united-states-of-america/"}], "resultsLimit": 10}, "actor_id": "mscraper/hotels-reviews-scraper"}, "valek.josef/create-mini-actor": {"id": 1498, "url": "https://apify.com/valek.josef/create-mini-actor/api/client/nodejs", "title": "Apify API and Create Mini Actor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"valek.josef/create-mini-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "valek.josef/create-mini-actor"}, "david.lukac/le-monde-scraper": {"id": 1499, "url": "https://apify.com/david.lukac/le-monde-scraper/api/client/nodejs", "title": "Apify API and Le Monde Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.lemonde.fr/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"david.lukac/le-monde-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.lemonde.fr/"}], "maxArticlesPerCrawl": 100}, "actor_id": "david.lukac/le-monde-scraper"}, "mnmkng/repl": {"id": 1515, "url": "https://apify.com/mnmkng/repl/api/client/nodejs", "title": "Apify API and REPL interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"code\": \"console.log('Hello version 1!');\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mnmkng/repl\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"code": "console.log('Hello version 1!');"}, "actor_id": "mnmkng/repl"}, "mscraper/ulta-scraper": {"id": 1500, "url": "https://apify.com/mscraper/ulta-scraper/api/client/nodejs", "title": "Apify API and Ulta Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.ulta.com/shop/hair/shampoo-conditioner/dry-shampoo\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"resultsLimit\": 44\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/ulta-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyCountry": "US"}, "startUrls": [{"url": "https://www.ulta.com/shop/hair/shampoo-conditioner/dry-shampoo"}], "resultsLimit": 44}, "actor_id": "mscraper/ulta-scraper"}, "adrian_horning/best-google-maps-scraper": {"id": 1501, "url": "https://apify.com/adrian_horning/best-google-maps-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udd25 Google Maps + Emails Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"Canton, OH\",\n    \"query\": \"Salons\",\n    \"excludedTerms\": [\n        \"Great Clips\",\n        \"Cost Cutters\",\n        \"Supercuts\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"adrian_horning/best-google-maps-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "Salons", "location": "Canton, OH", "excludedTerms": ["Great Clips", "Cost Cutters", "Supercuts"]}, "actor_id": "adrian_horning/best-google-maps-scraper"}, "spidoosho/bing-wallpaper": {"id": 1502, "url": "https://apify.com/spidoosho/bing-wallpaper/api/client/nodejs", "title": "Apify API and Bing Daily Wallpaper Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"selection\": [\n        1,\n        2,\n        3\n    ],\n    \"marketCode\": \"en-US\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"spidoosho/bing-wallpaper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"selection": [1, 2, 3], "marketCode": "en-US"}, "actor_id": "spidoosho/bing-wallpaper"}, "apify/example-secret-input": {"id": 1503, "url": "https://apify.com/apify/example-secret-input/api/client/nodejs", "title": "Apify API and Example Secret Input interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/example-secret-input\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/example-secret-input"}, "ondrejklinovsky/torrent-scraper": {"id": 1504, "url": "https://apify.com/ondrejklinovsky/torrent-scraper/api/client/nodejs", "title": "Apify API and Torrent Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"debian\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"ondrejklinovsky/torrent-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "debian", "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "ondrejklinovsky/torrent-scraper"}, "canadesk/builtwith": {"id": 1505, "url": "https://apify.com/canadesk/builtwith/api/client/nodejs", "title": "Apify API and BuiltWith (Technology Looker) interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"apify.com\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/builtwith\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "apify.com", "proxy": {"useApifyProxy": true}}, "actor_id": "canadesk/builtwith"}, "mtrunkatova/politico-scraper": {"id": 1506, "url": "https://apify.com/mtrunkatova/politico-scraper/api/client/nodejs", "title": "Apify API and Politico Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.politico.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mtrunkatova/politico-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.politico.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "mtrunkatova/politico-scraper"}, "canadesk/zendesk": {"id": 1523, "url": "https://apify.com/canadesk/zendesk/api/client/nodejs", "title": "Apify API and Zendesk Help Center interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"support\",\n    \"locale\": \"en-us\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/zendesk\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "support", "proxy": {"useApifyProxy": true}, "locale": "en-us"}, "actor_id": "canadesk/zendesk"}, "mshopik/stussycom-scraper": {"id": 1507, "url": "https://apify.com/mshopik/stussycom-scraper/api/client/nodejs", "title": "Apify API and Stussy.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/stussycom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/stussycom-scraper"}, "patrik.trefil/c-sharp-apify-actor-example": {"id": 1508, "url": "https://apify.com/patrik.trefil/c-sharp-apify-actor-example/api/client/nodejs", "title": "Apify API and Actor in C# Example interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://apify.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"patrik.trefil/c-sharp-apify-actor-example\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://apify.com"}, "actor_id": "patrik.trefil/c-sharp-apify-actor-example"}, "dtrungtin/covid-fi": {"id": 1509, "url": "https://apify.com/dtrungtin/covid-fi/api/client/nodejs", "title": "Apify API and Coronavirus stats in Finland interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/covid-fi\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "dtrungtin/covid-fi"}, "mshopik/windsor-fashions-scraper": {"id": 1510, "url": "https://apify.com/mshopik/windsor-fashions-scraper/api/client/nodejs", "title": "Apify API and Windsor Fashions Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/windsor-fashions-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/windsor-fashions-scraper"}, "topaz_sharingan/youtube-transcript-scraper": {"id": 1511, "url": "https://apify.com/topaz_sharingan/youtube-transcript-scraper/api/client/nodejs", "title": "Apify API and Youtube Transcript Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.youtube.com/@Apify\"\n    ],\n    \"maxRequest\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"topaz_sharingan/youtube-transcript-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": ["https://www.youtube.com/@Apify"], "maxRequest": 5}, "actor_id": "topaz_sharingan/youtube-transcript-scraper"}, "microworlds/screenshotpal": {"id": 1512, "url": "https://apify.com/microworlds/screenshotpal/api/client/nodejs", "title": "Apify API and ScreenshotPal interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urls\": [\n        \"https://apify.com/\"\n    ],\n    \"output_type\": \"png\",\n    \"device_scale_factor\": \"2\",\n    \"viewport_width\": 1280,\n    \"viewport_height\": 1024,\n    \"delay_between_scrolls\": 1000,\n    \"initial_page_scroll_delay_interval\": 300,\n    \"handlePageTimeoutSecs\": 5000,\n    \"maxRequestRetries\": 3,\n    \"proxyConfig\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"microworlds/screenshotpal\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": ["https://apify.com/"], "output_type": "png", "proxyConfig": {"useApifyProxy": false}, "viewport_width": 1280, "viewport_height": 1024, "maxRequestRetries": 3, "device_scale_factor": "2", "delay_between_scrolls": 1000, "handlePageTimeoutSecs": 5000, "initial_page_scroll_delay_interval": 300}, "actor_id": "microworlds/screenshotpal"}, "onidivo/dummy-run": {"id": 1513, "url": "https://apify.com/onidivo/dummy-run/api/client/nodejs", "title": "Apify API and Dummy Run interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"onidivo/dummy-run\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "onidivo/dummy-run"}, "eloquent_mountain/universal-google-analytics-ga4-ua-datalayer-product-scraper": {"id": 1516, "url": "https://apify.com/eloquent_mountain/universal-google-analytics-ga4-ua-datalayer-product-scraper/api/client/nodejs", "title": "Apify API and GA Scraper: Extract E-commerce Data from DataLayer interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": [\n        {\n            \"url\": \"https://www.bol.com/nl/nl/l/boeken/8299/\"\n        },\n        {\n            \"url\": \"https://www.bol.com/nl/nl/ra/kleine-cadeautjes-tot-15-euro/134688/?12194=1-15&rating=all&PROMO=Kidscadeaus_occasions_2021_2912_banner_1_LTI\"\n        }\n    ],\n    \"cookie_accept\": \"#js-first-screen-accept-all-button\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"eloquent_mountain/universal-google-analytics-ga4-ua-datalayer-product-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"start_urls": [{"url": "https://www.bol.com/nl/nl/l/boeken/8299/"}, {"url": "https://www.bol.com/nl/nl/ra/kleine-cadeautjes-tot-15-euro/134688/?12194=1-15&rating=all&PROMO=Kidscadeaus_occasions_2021_2912_banner_1_LTI"}], "cookie_accept": "#js-first-screen-accept-all-button"}, "actor_id": "eloquent_mountain/universal-google-analytics-ga4-ua-datalayer-product-scraper"}, "zuzka/covid-bg": {"id": 1517, "url": "https://apify.com/zuzka/covid-bg/api/client/nodejs", "title": "Apify API and Coronavirus stats in Bulgaria interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"email\": \"zuzka@apify.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/covid-bg\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"email": "zuzka@apify.com"}, "actor_id": "zuzka/covid-bg"}, "bebich/imdb-ratings": {"id": 1518, "url": "https://apify.com/bebich/imdb-ratings/api/client/nodejs", "title": "Apify API and IMDB Ratings interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"MOVIE_IDS\": [\n        \"tt0111161\"\n    ],\n    \"QUERY\": \"\",\n    \"CHART\": \"\",\n    \"DATASET_NAME\": \"\",\n    \"DATASET_NAME_RATINGS\": \"\",\n    \"DATASET_NAME_ID\": \"\",\n    \"CLEAN_DATASET_NAMES\": [],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bebich/imdb-ratings\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"CHART": "", "QUERY": "", "MOVIE_IDS": ["tt0111161"], "DATASET_NAME": "", "DATASET_NAME_ID": "", "proxyConfiguration": {"useApifyProxy": false}, "CLEAN_DATASET_NAMES": [], "DATASET_NAME_RATINGS": ""}, "actor_id": "bebich/imdb-ratings"}, "curious_coder/temu-reviews-scraper": {"id": 1519, "url": "https://apify.com/curious_coder/temu-reviews-scraper/api/client/nodejs", "title": "Apify API and Temu Reviews Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.temu.com/womens-indoor-pillow-slides-super-soft-wear-resistant-non-slip-eva-shoes-home-bath-cloud-slides-g-601099514896977.html?top_gallery_url=https%3A%2F%2Fimg.kwcdn.com%2Fproduct%2FFancyalgo%2FVirtualModelMatting%2F6136a787d69d3baf57dcde6cd81cd9b5.jpg&spec_gallery_id=27395562&refer_page_sn=10443&refer_source=0&freesia_scene=293&_oak_freesia_scene=293&_x_channel_scene=spike&_x_channel_src=1&_x_sessn_id=dvzkcle4di&refer_page_name=5-Star%20Rated&refer_page_id=10443_1688882853127_op5zfnbpzz\",\n    \"count\": 4,\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/temu-reviews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.temu.com/womens-indoor-pillow-slides-super-soft-wear-resistant-non-slip-eva-shoes-home-bath-cloud-slides-g-601099514896977.html?top_gallery_url=https%3A%2F%2Fimg.kwcdn.com%2Fproduct%2FFancyalgo%2FVirtualModelMatting%2F6136a787d69d3baf57dcde6cd81cd9b5.jpg&spec_gallery_id=27395562&refer_page_sn=10443&refer_source=0&freesia_scene=293&_oak_freesia_scene=293&_x_channel_scene=spike&_x_channel_src=1&_x_sessn_id=dvzkcle4di&refer_page_name=5-Star%20Rated&refer_page_id=10443_1688882853127_op5zfnbpzz", "count": 4, "proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "maxDelay": 3, "minDelay": 1}, "actor_id": "curious_coder/temu-reviews-scraper"}, "supernovo.ai/instagram-threads-scraper": {"id": 1520, "url": "https://apify.com/supernovo.ai/instagram-threads-scraper/api/client/nodejs", "title": "Apify API and Instagram Threads Post Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.threads.net/@vikkotaruc/post/Cwgf52_ySDG\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"supernovo.ai/instagram-threads-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.threads.net/@vikkotaruc/post/Cwgf52_ySDG", "proxy": {"useApifyProxy": true}}, "actor_id": "supernovo.ai/instagram-threads-scraper"}, "jupri/scraper": {"id": 1521, "url": "https://apify.com/jupri/scraper/api/client/nodejs", "title": "Apify API and Movoto Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jupri/scraper"}, "mshopik/mastercard-scraper": {"id": 1522, "url": "https://apify.com/mshopik/mastercard-scraper/api/client/nodejs", "title": "Apify API and Mastercard Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/mastercard-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/mastercard-scraper"}, "pocesar/diff-datasets": {"id": 1525, "url": "https://apify.com/pocesar/diff-datasets/api/client/nodejs", "title": "Apify API and Diff Datasets interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/diff-datasets\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "pocesar/diff-datasets"}, "jupri/rss-xml-scraper": {"id": 1526, "url": "https://apify.com/jupri/rss-xml-scraper/api/client/nodejs", "title": "Apify API and RSS / XML Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://rss.nytimes.com/services/xml/rss/nyt/US.xml\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/rss-xml-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://rss.nytimes.com/services/xml/rss/nyt/US.xml"}, "actor_id": "jupri/rss-xml-scraper"}, "tlinhart/slack-notification-webhook": {"id": 1527, "url": "https://apify.com/tlinhart/slack-notification-webhook/api/client/nodejs", "title": "Apify API and Slack Notification Webhook interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"tlinhart/slack-notification-webhook\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "tlinhart/slack-notification-webhook"}, "mihails/github-champion": {"id": 1528, "url": "https://apify.com/mihails/github-champion/api/client/nodejs", "title": "Apify API and Github Champion interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"repositoryOwner\": \"apify\",\n    \"repositories\": []\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mihails/github-champion\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"repositories": [], "repositoryOwner": "apify"}, "actor_id": "mihails/github-champion"}, "mshopik/100percent-pure-scraper": {"id": 1529, "url": "https://apify.com/mshopik/100percent-pure-scraper/api/client/nodejs", "title": "Apify API and 100% PURE Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/100percent-pure-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/100percent-pure-scraper"}, "krakorj/covid-serbia": {"id": 1530, "url": "https://apify.com/krakorj/covid-serbia/api/client/nodejs", "title": "Apify API and Coronavirus stats in the Serbia interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"krakorj/covid-serbia\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "krakorj/covid-serbia"}, "lukaskrivka/github-issues-to-spreadsheet": {"id": 1531, "url": "https://apify.com/lukaskrivka/github-issues-to-spreadsheet/api/client/nodejs", "title": "Apify API and Github Issues Tracker interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"repositories\": [\n        \"apifytech/apify-js\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/github-issues-to-spreadsheet\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"repositories": ["apifytech/apify-js"]}, "actor_id": "lukaskrivka/github-issues-to-spreadsheet"}, "oelin/stable-diffusion-scraper": {"id": 1532, "url": "https://apify.com/oelin/stable-diffusion-scraper/api/client/nodejs", "title": "Apify API and Stable Diffusion Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"prompt\": \"an astronaut riding on a horse\",\n    \"model\": \"ai-forever/kandinsky-2.2:ea1addaab376f4dc227f5368bbd8eff901820fd1cc14ed8cad63b29249e9d463\",\n    \"options\": {\n        \"width\": 1024,\n        \"height\": 1024\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"oelin/stable-diffusion-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"model": "ai-forever/kandinsky-2.2:ea1addaab376f4dc227f5368bbd8eff901820fd1cc14ed8cad63b29249e9d463", "prompt": "an astronaut riding on a horse", "options": {"width": 1024, "height": 1024}}, "actor_id": "oelin/stable-diffusion-scraper"}, "lukaskrivka/google-maps-scraper-orchestrator": {"id": 1533, "url": "https://apify.com/lukaskrivka/google-maps-scraper-orchestrator/api/client/nodejs", "title": "Apify API and Google Maps Scraper Orchestrator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"locations\": [\n        \"New York\",\n        \"Los Angeles\",\n        \"Chicago\"\n    ],\n    \"locationPrefix\": \"USA\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/google-maps-scraper-orchestrator\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"locations": ["New York", "Los Angeles", "Chicago"], "locationPrefix": "USA"}, "actor_id": "lukaskrivka/google-maps-scraper-orchestrator"}, "mshopik/c-a-r-i-scraper": {"id": 1534, "url": "https://apify.com/mshopik/c-a-r-i-scraper/api/client/nodejs", "title": "Apify API and Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/c-a-r-i-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/c-a-r-i-scraper"}, "vaclavrut/motionmail-send-invoice": {"id": 1535, "url": "https://apify.com/vaclavrut/motionmail-send-invoice/api/client/nodejs", "title": "Apify API and Motionmail Send Invoice interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"vaclavrut/motionmail-send-invoice\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "vaclavrut/motionmail-send-invoice"}, "datastorm/gpt-code-scraper": {"id": 1536, "url": "https://apify.com/datastorm/gpt-code-scraper/api/client/nodejs", "title": "Apify API and Chat GPT Code Interpreter Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"prompt\": \"Plot a chart of BTC's price for 2023 YTD\",\n    \"model\": \"gpt-3.5-turbo\",\n    \"datasetIds\": [],\n    \"files\": []\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"datastorm/gpt-code-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"files": [], "model": "gpt-3.5-turbo", "prompt": "Plot a chart of BTC's price for 2023 YTD", "datasetIds": []}, "actor_id": "datastorm/gpt-code-scraper"}, "ivanvs/autoscout-scraper": {"id": 1537, "url": "https://apify.com/ivanvs/autoscout-scraper/api/client/nodejs", "title": "Apify API and Autoscout Scraper, Autoscout API, Autoscout crawler interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urls\": [\n        {\n            \"url\": \"https://www.autoscout24.com/lst/zastava?atype=C&desc=0&sort=standard&source=homepage_search-mask&ustate=N%2CU\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"maxConcurrency\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"ivanvs/autoscout-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": [{"url": "https://www.autoscout24.com/lst/zastava?atype=C&desc=0&sort=standard&source=homepage_search-mask&ustate=N%2CU"}], "maxConcurrency": 1, "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "ivanvs/autoscout-scraper"}, "mshopik/arccos-golf-scraper": {"id": 1538, "url": "https://apify.com/mshopik/arccos-golf-scraper/api/client/nodejs", "title": "Apify API and Arccos Golf Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/arccos-golf-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/arccos-golf-scraper"}, "mshopik/mistorepk-scraper": {"id": 1539, "url": "https://apify.com/mshopik/mistorepk-scraper/api/client/nodejs", "title": "Apify API and mistore.pk Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/mistorepk-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/mistorepk-scraper"}, "drobnikj/ruby-example": {"id": 1540, "url": "https://apify.com/drobnikj/ruby-example/api/client/nodejs", "title": "Apify API and Ruby Example interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://apify.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"drobnikj/ruby-example\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://apify.com"}, "actor_id": "drobnikj/ruby-example"}, "mshopik/public-goods-scraper": {"id": 1541, "url": "https://apify.com/mshopik/public-goods-scraper/api/client/nodejs", "title": "Apify API and Public Goods Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/public-goods-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/public-goods-scraper"}, "jupri/apify-tv": {"id": 1542, "url": "https://apify.com/jupri/apify-tv/api/client/nodejs", "title": "Apify API and Video Explorer interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"sex requires two adults\",\n    \"portal\": \"bing\",\n    \"limit\": 10,\n    \"dev_proxy_config\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/apify-tv\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"limit": 10, "query": "sex requires two adults", "portal": "bing", "dev_proxy_config": {"useApifyProxy": true, "apifyProxyCountry": "US"}}, "actor_id": "jupri/apify-tv"}, "jupri/reddit-explorer": {"id": 1543, "url": "https://apify.com/jupri/reddit-explorer/api/client/nodejs", "title": "Apify API and Reddit Media Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"apify\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/reddit-explorer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "apify"}, "actor_id": "jupri/reddit-explorer"}, "datastorm/nsfw-scraper": {"id": 1544, "url": "https://apify.com/datastorm/nsfw-scraper/api/client/nodejs", "title": "Apify API and NSFW Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"items\": [\n        \"https://firebasestorage.googleapis.com/v0/b/kongo-ln.appspot.com/o/girl%20sexy%20hot%20sun%20sea%20beach.jpg?alt=media&token=7ec36226-6a7a-4241-99a3-5bde4bbde2c6\",\n        \"https://firebasestorage.googleapis.com/v0/b/kongo-ln.appspot.com/o/emma%20watson%20in%20bikini%20on%20the%20beach.jpg?alt=media&token=73af0ea9-c80e-4aa8-be7c-7e79917e6559\",\n        \"https://firebasestorage.googleapis.com/v0/b/kongo-ln.appspot.com/o/chris%20evans%20as%20peaky%20blinders%20higly%20detailed.jpg?alt=media&token=e8167234-c864-4df3-9590-2a932edb0a4a\"\n    ],\n    \"datasets\": [\n        {\n            \"id\": \"serTsUnXrLOVXjPtd\",\n            \"fields\": [\n                \"value\"\n            ]\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"datastorm/nsfw-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"items": ["https://firebasestorage.googleapis.com/v0/b/kongo-ln.appspot.com/o/girl%20sexy%20hot%20sun%20sea%20beach.jpg?alt=media&token=7ec36226-6a7a-4241-99a3-5bde4bbde2c6", "https://firebasestorage.googleapis.com/v0/b/kongo-ln.appspot.com/o/emma%20watson%20in%20bikini%20on%20the%20beach.jpg?alt=media&token=73af0ea9-c80e-4aa8-be7c-7e79917e6559", "https://firebasestorage.googleapis.com/v0/b/kongo-ln.appspot.com/o/chris%20evans%20as%20peaky%20blinders%20higly%20detailed.jpg?alt=media&token=e8167234-c864-4df3-9590-2a932edb0a4a"], "datasets": [{"id": "serTsUnXrLOVXjPtd", "fields": ["value"]}]}, "actor_id": "datastorm/nsfw-scraper"}, "mscraper/publicsq-scraper": {"id": 1545, "url": "https://apify.com/mscraper/publicsq-scraper/api/client/nodejs", "title": "Apify API and PublicSq Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://app.publicsq.com/marketplace/1140af70-cd90-11ed-8d0c-1138304e5366?type=online\"\n        },\n        {\n            \"url\": \"https://app.publicsq.com/marketplace/categories/apparel_and_accessories/jewelry?type=online\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\",\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    },\n    \"resultsLimit\": 20\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/publicsq-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "startUrls": [{"url": "https://app.publicsq.com/marketplace/1140af70-cd90-11ed-8d0c-1138304e5366?type=online"}, {"url": "https://app.publicsq.com/marketplace/categories/apparel_and_accessories/jewelry?type=online"}], "resultsLimit": 20}, "actor_id": "mscraper/publicsq-scraper"}, "kuaima/douban": {"id": 1596, "url": "https://apify.com/kuaima/douban/api/client/nodejs", "title": "Apify API and douban interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"kuaima/douban\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "kuaima/douban"}, "tomashujer/los-angeles-times-scraper": {"id": 1546, "url": "https://apify.com/tomashujer/los-angeles-times-scraper/api/client/nodejs", "title": "Apify API and Los Angeles Times Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.theguardian.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"tomashujer/los-angeles-times-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.theguardian.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "tomashujer/los-angeles-times-scraper"}, "autodomme/loyalfans-timeline-scraper": {"id": 1547, "url": "https://apify.com/autodomme/loyalfans-timeline-scraper/api/client/nodejs", "title": "Apify API and Loyalfans Timeline Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"profiles\": [\n        \"autosissy\",\n        \"goddesslivs\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"autodomme/loyalfans-timeline-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"profiles": ["autosissy", "goddesslivs"]}, "actor_id": "autodomme/loyalfans-timeline-scraper"}, "adrian_horning/the-big-z": {"id": 1548, "url": "https://apify.com/adrian_horning/the-big-z/api/client/nodejs", "title": "Apify API and The Big Z interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"Canton, Ohio\",\n    \"status\": \"sale\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"adrian_horning/the-big-z\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"search": "Canton, Ohio", "status": "sale"}, "actor_id": "adrian_horning/the-big-z"}, "natasha.lekh/nbc-news-scraper": {"id": 1549, "url": "https://apify.com/natasha.lekh/nbc-news-scraper/api/client/nodejs", "title": "Apify API and NBC news Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.nbcnews.com/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"natasha.lekh/nbc-news-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.nbcnews.com/"}], "maxArticlesPerCrawl": 100}, "actor_id": "natasha.lekh/nbc-news-scraper"}, "mnmkng/delete-named-storages": {"id": 1550, "url": "https://apify.com/mnmkng/delete-named-storages/api/client/nodejs", "title": "Apify API and Delete Named Storages interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mnmkng/delete-named-storages\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "mnmkng/delete-named-storages"}, "zuzka/huffington-post-scraper": {"id": 1551, "url": "https://apify.com/zuzka/huffington-post-scraper/api/client/nodejs", "title": "Apify API and Huffington Post Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.huffpost.com/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/huffington-post-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.huffpost.com/"}], "maxArticlesPerCrawl": 100}, "actor_id": "zuzka/huffington-post-scraper"}, "undrtkr984/softwareadviceproductnamesearch": {"id": 1552, "url": "https://apify.com/undrtkr984/softwareadviceproductnamesearch/api/client/nodejs", "title": "Apify API and SoftwareAdviceProductNameSearch interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://crawlee.dev\"\n        }\n    ],\n    \"maxRequestRetries\": 2\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"undrtkr984/softwareadviceproductnamesearch\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://crawlee.dev"}], "maxRequestRetries": 2}, "actor_id": "undrtkr984/softwareadviceproductnamesearch"}, "strajk/koloshop-koloshop-cz-scraper": {"id": 1553, "url": "https://apify.com/strajk/koloshop-koloshop-cz-scraper/api/client/nodejs", "title": "Apify API and Koloshop (koloshop.cz) scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"TEST\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"strajk/koloshop-koloshop-cz-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "TEST"}, "actor_id": "strajk/koloshop-koloshop-cz-scraper"}, "astronomical_lizard/naver-blog-scraper": {"id": 1554, "url": "https://apify.com/astronomical_lizard/naver-blog-scraper/api/client/nodejs", "title": "Apify API and Naver Blog Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"keywords\": [\n        \"testing\"\n    ],\n    \"date_start\": \"2023-01-01\",\n    \"date_end\": \"2023-06-30\",\n    \"max_pages\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"astronomical_lizard/naver-blog-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"date_end": "2023-06-30", "keywords": ["testing"], "max_pages": 1, "date_start": "2023-01-01"}, "actor_id": "astronomical_lizard/naver-blog-scraper"}, "equidem/dataset-validity-checker": {"id": 1555, "url": "https://apify.com/equidem/dataset-validity-checker/api/client/nodejs", "title": "Apify API and Dataset Validity Checker interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"equidem/dataset-validity-checker\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "equidem/dataset-validity-checker"}, "pocesar/run-webhook-digest": {"id": 1556, "url": "https://apify.com/pocesar/run-webhook-digest/api/client/nodejs", "title": "Apify API and Power Webhook Integration interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"method\": \"POST\",\n    \"statuses\": [\n        \"ACTOR.RUN.SUCCEEDED\",\n        \"ACTOR.RUN.FAILED\",\n        \"ACTOR.RUN.TIMED_OUT\"\n    ],\n    \"customData\": {},\n    \"transformEndpoint\": async ({ Apify, url, dataset, requestQueue, keyValueStore, abort, data, input: { customData } }) => {\n      return data;\n    },\n    \"triggerCondition\": async ({ Apify, dataset, requestQueue, keyValueStore, abort, data, input: { customData } }) => {\n     return true;\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"pocesar/run-webhook-digest\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"method": "POST", "statuses": ["ACTOR.RUN.SUCCEEDED", "ACTOR.RUN.FAILED", "ACTOR.RUN.TIMED_OUT"], "customData": {}}, "actor_id": "pocesar/run-webhook-digest"}, "eneiromatos/ultimate-best-buy-scraper": {"id": 1557, "url": "https://apify.com/eneiromatos/ultimate-best-buy-scraper/api/client/nodejs", "title": "Apify API and Ultimate Best Buy Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"productUrls\": [\n        \"https://www.bestbuy.com/site/hp-15-6-touch-screen-laptop-intel-core-i3-8gb-memory-256gb-ssd-natural-silver/6510528.p?skuId=6510528\",\n        \"https://www.bestbuy.com/site/beats-by-dr-dre-geek-squad-certified-refurbished-beats-studio-buds-true-wireless-noise-cancelling-earbuds-beats-red/6473909.p?skuId=6473909\"\n    ],\n    \"listingUrls\": [\n        \"https://www.bestbuy.com/site/pain-management/muscle-stimulators/pcmcat1598359185974.c?id=pcmcat1598359185974\",\n        \"https://www.bestbuy.com/site/tvs/all-flat-screen-tvs/abcat0101001.c?id=abcat0101001\"\n    ],\n    \"keywords\": [\n        \"sony headphones\"\n    ],\n    \"startPageNumber\": 1,\n    \"finalPageNumber\": 1,\n    \"minPrice\": 0,\n    \"maxPrice\": 0\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"eneiromatos/ultimate-best-buy-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"keywords": ["sony headphones"], "maxPrice": 0, "minPrice": 0, "listingUrls": ["https://www.bestbuy.com/site/pain-management/muscle-stimulators/pcmcat1598359185974.c?id=pcmcat1598359185974", "https://www.bestbuy.com/site/tvs/all-flat-screen-tvs/abcat0101001.c?id=abcat0101001"], "productUrls": ["https://www.bestbuy.com/site/hp-15-6-touch-screen-laptop-intel-core-i3-8gb-memory-256gb-ssd-natural-silver/6510528.p?skuId=6510528", "https://www.bestbuy.com/site/beats-by-dr-dre-geek-squad-certified-refurbished-beats-studio-buds-true-wireless-noise-cancelling-earbuds-beats-red/6473909.p?skuId=6473909"], "finalPageNumber": 1, "startPageNumber": 1}, "actor_id": "eneiromatos/ultimate-best-buy-scraper"}, "zhen/bloxy-tokens-contract": {"id": 1558, "url": "https://apify.com/zhen/bloxy-tokens-contract/api/client/nodejs", "title": "Apify API and Bloxy Tokens Info interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zhen/bloxy-tokens-contract\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "zhen/bloxy-tokens-contract"}, "kelvinspencer/universe-oauth": {"id": 1559, "url": "https://apify.com/kelvinspencer/universe-oauth/api/client/nodejs", "title": "Apify API and Universe Oauth interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"kelvinspencer/universe-oauth\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "kelvinspencer/universe-oauth"}, "strajk/fdf-bike-shop-fdfbikeshop-cz-scraper": {"id": 1560, "url": "https://apify.com/strajk/fdf-bike-shop-fdfbikeshop-cz-scraper/api/client/nodejs", "title": "Apify API and FDF Bike Shop (fdfbikeshop.cz) scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"TEST\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"strajk/fdf-bike-shop-fdfbikeshop-cz-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "TEST"}, "actor_id": "strajk/fdf-bike-shop-fdfbikeshop-cz-scraper"}, "hanatsai/the-sun-scraper": {"id": 1561, "url": "https://apify.com/hanatsai/the-sun-scraper/api/client/nodejs", "title": "Apify API and The Sun Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"http://the-sun.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hanatsai/the-sun-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "http://the-sun.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "hanatsai/the-sun-scraper"}, "mrfoxord/fparse": {"id": 1562, "url": "https://apify.com/mrfoxord/fparse/api/client/nodejs", "title": "Apify API and FParse interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mrfoxord/fparse\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "mrfoxord/fparse"}, "hamza.alwan/gab-explore-feeds-scraper": {"id": 1563, "url": "https://apify.com/hamza.alwan/gab-explore-feeds-scraper/api/client/nodejs", "title": "Apify API and Gab Explore & Feeds Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [],\n    \"desiredPostsCount\": 100,\n    \"desiredCommentsCount\": 0,\n    \"commentsSortBy\": \"MOST_LIKED\",\n    \"explorePostsSortBy\": \"HOT\",\n    \"photosFeedSortBy\": \"NEWEST\",\n    \"videosFeedSortBy\": \"NEWEST\",\n    \"linksFeedSortBy\": \"NEWEST\",\n    \"proFeedSortBy\": \"NEWEST\",\n    \"pollFeedPostsSortBy\": \"NEWEST\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/gab-explore-feeds-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [], "proFeedSortBy": "NEWEST", "commentsSortBy": "MOST_LIKED", "linksFeedSortBy": "NEWEST", "photosFeedSortBy": "NEWEST", "videosFeedSortBy": "NEWEST", "desiredPostsCount": 100, "explorePostsSortBy": "HOT", "pollFeedPostsSortBy": "NEWEST", "desiredCommentsCount": 0}, "actor_id": "hamza.alwan/gab-explore-feeds-scraper"}, "mshopik/bjj-fanatics-scraper": {"id": 1564, "url": "https://apify.com/mshopik/bjj-fanatics-scraper/api/client/nodejs", "title": "Apify API and BJJ Fanatics Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bjj-fanatics-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bjj-fanatics-scraper"}, "curious_coder/linkedin-group-search-scraper": {"id": 1565, "url": "https://apify.com/curious_coder/linkedin-group-search-scraper/api/client/nodejs", "title": "Apify API and Linkedin group search scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startPage\": 1,\n    \"minDelay\": 2,\n    \"maxDelay\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/linkedin-group-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDelay": 5, "minDelay": 2, "startPage": 1}, "actor_id": "curious_coder/linkedin-group-search-scraper"}, "lukass/breitbart-scraper": {"id": 1566, "url": "https://apify.com/lukass/breitbart-scraper/api/client/nodejs", "title": "Apify API and Breitbart Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.breitbart.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukass/breitbart-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.breitbart.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "lukass/breitbart-scraper"}, "alexey/vdab-be-jobs": {"id": 1567, "url": "https://apify.com/alexey/vdab-be-jobs/api/client/nodejs", "title": "Apify API and Vdab-be jobs interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.vdab.be/vindeenjob/jobs/financieel?sort=1\"\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"alexey/vdab-be-jobs\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": ["https://www.vdab.be/vindeenjob/jobs/financieel?sort=1"], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "alexey/vdab-be-jobs"}, "mscraper/feedly-scraper": {"id": 1568, "url": "https://apify.com/mscraper/feedly-scraper/api/client/nodejs", "title": "Apify API and Feedly Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"language\": \"en\",\n    \"topics\": [\n        \"apple\"\n    ],\n    \"feeds\": [\n        \"feed/http://nordicapis.com/feed/\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/feedly-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"feeds": ["feed/http://nordicapis.com/feed/"], "proxy": {"useApifyProxy": true, "apifyProxyCountry": "US"}, "topics": ["apple"], "language": "en"}, "actor_id": "mscraper/feedly-scraper"}, "younes_benallal/seloger": {"id": 1569, "url": "https://apify.com/younes_benallal/seloger/api/client/nodejs", "title": "Apify API and Seloger Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.seloger.com/list.htm?projects=2%2C5&types=2%2C1&qsVersion=1.0\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"younes_benallal/seloger\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.seloger.com/list.htm?projects=2%2C5&types=2%2C1&qsVersion=1.0"}], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "younes_benallal/seloger"}, "glaring_santoor/zillow-fastest-scraper": {"id": 1570, "url": "https://apify.com/glaring_santoor/zillow-fastest-scraper/api/client/nodejs", "title": "Apify API and Zillow Fastest Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"region\": \"Pima County\",\n    \"state\": \"AZ\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"glaring_santoor/zillow-fastest-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"state": "AZ", "region": "Pima County"}, "actor_id": "glaring_santoor/zillow-fastest-scraper"}, "tcz/trustpilot-review-crawler": {"id": 1571, "url": "https://apify.com/tcz/trustpilot-review-crawler/api/client/nodejs", "title": "Apify API and TrustPilot Review Crawler interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"trustpilot_pages\": [\n        \"https://www.trustpilot.com/review/stellasolar.se\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"tcz/trustpilot-review-crawler\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"trustpilot_pages": ["https://www.trustpilot.com/review/stellasolar.se"]}, "actor_id": "tcz/trustpilot-review-crawler"}, "curious_coder/google-lens-scraper": {"id": 1572, "url": "https://apify.com/curious_coder/google-lens-scraper/api/client/nodejs", "title": "Apify API and Google Lens Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urls\": [\n        \"https://imageio.forbes.com/specials-images/imageserve/62d700cd6094d2c180f269b9/0x0.jpg?format=jpg&crop=959,959,x0,y0,safe&height=416&width=416&fit=bounds\",\n        \"https://upload.wikimedia.org/wikipedia/commons/1/18/Mark_Zuckerberg_F8_2019_Keynote_%2832830578717%29_%28cropped%29.jpg\"\n    ],\n    \"searchType\": \"findImageSources\",\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/google-lens-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": ["https://imageio.forbes.com/specials-images/imageserve/62d700cd6094d2c180f269b9/0x0.jpg?format=jpg&crop=959,959,x0,y0,safe&height=416&width=416&fit=bounds", "https://upload.wikimedia.org/wikipedia/commons/1/18/Mark_Zuckerberg_F8_2019_Keynote_%2832830578717%29_%28cropped%29.jpg"], "proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "searchType": "findImageSources"}, "actor_id": "curious_coder/google-lens-scraper"}, "wiliamribeiro/quora-brasil": {"id": 1573, "url": "https://apify.com/wiliamribeiro/quora-brasil/api/client/nodejs", "title": "Apify API and Quora Brasil interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"queries\": [\n        \"how to drink yogurt\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"wiliamribeiro/quora-brasil\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "queries": ["how to drink yogurt"]}, "actor_id": "wiliamribeiro/quora-brasil"}, "lukas.novotny/ubuntu-images-scraper": {"id": 1574, "url": "https://apify.com/lukas.novotny/ubuntu-images-scraper/api/client/nodejs", "title": "Apify API and Ubuntu Images Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"zone\": \"us-east-1\",\n    \"name\": \"focal\",\n    \"version\": \"20.04\",\n    \"arch\": \"amd64\",\n    \"instanceType\": \"hvm-ssd\",\n    \"release\": \"Any\",\n    \"id\": \"Any\",\n    \"numberOfResults\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukas.novotny/ubuntu-images-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"id": "Any", "arch": "amd64", "name": "focal", "zone": "us-east-1", "release": "Any", "version": "20.04", "instanceType": "hvm-ssd", "numberOfResults": 1}, "actor_id": "lukas.novotny/ubuntu-images-scraper"}, "anchor/forward-dataset-webhook": {"id": 1575, "url": "https://apify.com/anchor/forward-dataset-webhook/api/client/nodejs", "title": "Apify API and Forward dataset as POST data interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"anchor/forward-dataset-webhook\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "anchor/forward-dataset-webhook"}, "zuzka/get-proper-url-from-shortened-one": {"id": 1576, "url": "https://apify.com/zuzka/get-proper-url-from-shortened-one/api/client/nodejs", "title": "Apify API and Get Proper Url From Shortened One interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://bit.ly/the_shortest_way_to_get_to_apify_store\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/get-proper-url-from-shortened-one\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://bit.ly/the_shortest_way_to_get_to_apify_store"}]}, "actor_id": "zuzka/get-proper-url-from-shortened-one"}, "aezakmi/cryptocurrency-jobs-scraper": {"id": 1577, "url": "https://apify.com/aezakmi/cryptocurrency-jobs-scraper/api/client/nodejs", "title": "Apify API and Cryptocurrency jobs scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"aezakmi/cryptocurrency-jobs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "aezakmi/cryptocurrency-jobs-scraper"}, "strajk/bike-components-bike-components-de-scraper": {"id": 1578, "url": "https://apify.com/strajk/bike-components-bike-components-de-scraper/api/client/nodejs", "title": "Apify API and Bike Components (bike-components.de) scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"TEST\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"strajk/bike-components-bike-components-de-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "TEST"}, "actor_id": "strajk/bike-components-bike-components-de-scraper"}, "mshopik/bbc-shop-us-scraper": {"id": 1579, "url": "https://apify.com/mshopik/bbc-shop-us-scraper/api/client/nodejs", "title": "Apify API and BBC Shop US Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bbc-shop-us-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bbc-shop-us-scraper"}, "binhbui/metube": {"id": 1580, "url": "https://apify.com/binhbui/metube/api/client/nodejs", "title": "Apify API and Metube interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"youtubeUrls\": [\n        \"https://www.youtube.com/watch?v=irz1XRFZA0Q\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"binhbui/metube\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"youtubeUrls": ["https://www.youtube.com/watch?v=irz1XRFZA0Q"]}, "actor_id": "binhbui/metube"}, "enchanted_clamp/my-actor-1": {"id": 1581, "url": "https://apify.com/enchanted_clamp/my-actor-1/api/client/nodejs", "title": "Apify API and My Personal Test Actor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"New York\",\n    \"maxPages\": 2,\n    \"sortBy\": \"distance_from_search\",\n    \"currency\": \"USD\",\n    \"language\": \"en-us\",\n    \"minMaxPrice\": \"0-999999\",\n    \"maxReviews\": 10,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    },\n    \"extendOutputFunction\": ($) => { return {} }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"enchanted_clamp/my-actor-1\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"search": "New York", "sortBy": "distance_from_search", "currency": "USD", "language": "en-us", "maxPages": 2, "maxReviews": 10, "minMaxPrice": "0-999999", "proxyConfig": {"useApifyProxy": true}}, "actor_id": "enchanted_clamp/my-actor-1"}, "saswave/sales-navigator-search-lists-prospects": {"id": 1604, "url": "https://apify.com/saswave/sales-navigator-search-lists-prospects/api/client/nodejs", "title": "Apify API and Sales Navigator Search Lists Prospects interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"saswave/sales-navigator-search-lists-prospects\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "saswave/sales-navigator-search-lists-prospects"}, "lexis-solutions/otto-de-scraper": {"id": 1582, "url": "https://apify.com/lexis-solutions/otto-de-scraper/api/client/nodejs", "title": "Apify API and Otto.de Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.otto.de/haushalt/trockner/waermepumpentrockner/?l=gq&o=120&c=Trockner\"\n        }\n    ],\n    \"maxItems\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lexis-solutions/otto-de-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 10, "startUrls": [{"url": "https://www.otto.de/haushalt/trockner/waermepumpentrockner/?l=gq&o=120&c=Trockner"}]}, "actor_id": "lexis-solutions/otto-de-scraper"}, "curious_coder/linkedin-comment-scraper": {"id": 1583, "url": "https://apify.com/curious_coder/linkedin-comment-scraper/api/client/nodejs", "title": "Apify API and Linkedin comment scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"postUrl\": \"https://www.linkedin.com/posts/linkedin_add-job-preferences-activity-6704397478923423745-OR6o\",\n    \"startPage\": 1,\n    \"minDelay\": 2,\n    \"maxDelay\": 7\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/linkedin-comment-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"postUrl": "https://www.linkedin.com/posts/linkedin_add-job-preferences-activity-6704397478923423745-OR6o", "maxDelay": 7, "minDelay": 2, "startPage": 1}, "actor_id": "curious_coder/linkedin-comment-scraper"}, "mscraper/vrbo-reviews-scraper": {"id": 1584, "url": "https://apify.com/mscraper/vrbo-reviews-scraper/api/client/nodejs", "title": "Apify API and Vrbo Reviews Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.vrbo.com/802504\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\",\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    },\n    \"resultsLimit\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/vrbo-reviews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "startUrls": [{"url": "https://www.vrbo.com/802504"}], "resultsLimit": 10}, "actor_id": "mscraper/vrbo-reviews-scraper"}, "babak/behance-profile-scraper": {"id": 1585, "url": "https://apify.com/babak/behance-profile-scraper/api/client/nodejs", "title": "Apify API and Behance Profile Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"profileUrl\": \"https://www.behance.net/ramotion\",\n    \"worksMaxItems\": 2,\n    \"workCommentsLimit\": 5,\n    \"moodboardsMaximumLimit\": 2,\n    \"appreciationsMaximumLimit\": 2,\n    \"servicesMaximumLimit\": 2,\n    \"nftsMaximumLimit\": 2,\n    \"maxConcurrency\": 100,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"babak/behance-profile-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"profileUrl": "https://www.behance.net/ramotion", "worksMaxItems": 2, "maxConcurrency": 100, "nftsMaximumLimit": 2, "workCommentsLimit": 5, "proxyConfiguration": {"useApifyProxy": true}, "servicesMaximumLimit": 2, "moodboardsMaximumLimit": 2, "appreciationsMaximumLimit": 2}, "actor_id": "babak/behance-profile-scraper"}, "mscraper/wizzair-scraper": {"id": 1586, "url": "https://apify.com/mscraper/wizzair-scraper/api/client/nodejs", "title": "Apify API and WizzAir Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"origin\": \"BUD\",\n    \"departureDate\": \"2023-09-20\",\n    \"returnDate\": \"\",\n    \"destination\": \"LTN\",\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"DE\",\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/wizzair-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "DE"}, "origin": "BUD", "returnDate": "", "destination": "LTN", "departureDate": "2023-09-20"}, "actor_id": "mscraper/wizzair-scraper"}, "zuzka/slack-messages-downloader": {"id": 1587, "url": "https://apify.com/zuzka/slack-messages-downloader/api/client/nodejs", "title": "Apify API and Slack Messages Downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"oldest\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/slack-messages-downloader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"oldest": 10}, "actor_id": "zuzka/slack-messages-downloader"}, "lukaskrivka/google-search-live": {"id": 1626, "url": "https://apify.com/lukaskrivka/google-search-live/api/client/nodejs", "title": "Apify API and Google Search Scraper For Live App interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/google-search-live\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukaskrivka/google-search-live"}, "sanjeta/linkedin-company-profile-scraper": {"id": 1588, "url": "https://apify.com/sanjeta/linkedin-company-profile-scraper/api/client/nodejs", "title": "Apify API and Linkedin Company Profile Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urls\": [\n        {\n            \"url\": \"https://www.linkedin.com/company/apifytech\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"sanjeta/linkedin-company-profile-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": [{"url": "https://www.linkedin.com/company/apifytech"}]}, "actor_id": "sanjeta/linkedin-company-profile-scraper"}, "strajk/team-sport-teamsport-cz-scraper": {"id": 1589, "url": "https://apify.com/strajk/team-sport-teamsport-cz-scraper/api/client/nodejs", "title": "Apify API and Team Sport (teamsport.cz) scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"TEST\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"strajk/team-sport-teamsport-cz-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "TEST"}, "actor_id": "strajk/team-sport-teamsport-cz-scraper"}, "mshopik/2modern-furniture-and-lighting-scraper": {"id": 1590, "url": "https://apify.com/mshopik/2modern-furniture-and-lighting-scraper/api/client/nodejs", "title": "Apify API and 2Modern Furniture Lighting Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/2modern-furniture-and-lighting-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/2modern-furniture-and-lighting-scraper"}, "curious_coder/github-scraper": {"id": 1591, "url": "https://apify.com/curious_coder/github-scraper/api/client/nodejs", "title": "Apify API and Github user profile scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"scrapeStars\",\n    \"scrapeStars.repoUrl\": \"https://github.com/facebook/react\",\n    \"startPage\": 1,\n    \"count\": 40,\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/github-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"count": 40, "action": "scrapeStars", "maxDelay": 3, "minDelay": 1, "startPage": 1, "scrapeStars.repoUrl": "https://github.com/facebook/react"}, "actor_id": "curious_coder/github-scraper"}, "curious_coder/ultimate-facebook-scraper": {"id": 1592, "url": "https://apify.com/curious_coder/ultimate-facebook-scraper/api/client/nodejs", "title": "Apify API and Ultimate Facebook Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"action\": \"scrapeProfiles\",\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [],\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/ultimate-facebook-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": [], "apifyProxyCountry": "US"}, "action": "scrapeProfiles", "maxDelay": 3, "minDelay": 1}, "actor_id": "curious_coder/ultimate-facebook-scraper"}, "apify/example-github-gist": {"id": 1593, "url": "https://apify.com/apify/example-github-gist/api/client/nodejs", "title": "Apify API and Example using GitHub Gist interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/example-github-gist\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "apify/example-github-gist"}, "chin/apify-factual": {"id": 1594, "url": "https://apify.com/chin/apify-factual/api/client/nodejs", "title": "Apify API and Apify Factual interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"chin/apify-factual\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "chin/apify-factual"}, "strajk/bikerboarder-biker-boarder-de-scraper": {"id": 1595, "url": "https://apify.com/strajk/bikerboarder-biker-boarder-de-scraper/api/client/nodejs", "title": "Apify API and BikerBoarder (biker-boarder.de) scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"TEST\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"strajk/bikerboarder-biker-boarder-de-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "TEST"}, "actor_id": "strajk/bikerboarder-biker-boarder-de-scraper"}, "mshopik/social-status-scraper": {"id": 1597, "url": "https://apify.com/mshopik/social-status-scraper/api/client/nodejs", "title": "Apify API and Social Status Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/social-status-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/social-status-scraper"}, "trudax/mercadolibre-scraper": {"id": 1598, "url": "https://apify.com/trudax/mercadolibre-scraper/api/client/nodejs", "title": "Apify API and Mercado Libre Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"search\": \"smartphone\",\n    \"domainCode\": \"MX\",\n    \"maxItemCount\": 5,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"trudax/mercadolibre-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "search": "smartphone", "domainCode": "MX", "maxItemCount": 5}, "actor_id": "trudax/mercadolibre-scraper"}, "bebity/welcome-to-the-jungle-jobs-scraper": {"id": 1599, "url": "https://apify.com/bebity/welcome-to-the-jungle-jobs-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udd25 Welcome To The Jungle Jobs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bebity/welcome-to-the-jungle-jobs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "bebity/welcome-to-the-jungle-jobs-scraper"}, "natasha.lekh/epoch-times-scraper": {"id": 1600, "url": "https://apify.com/natasha.lekh/epoch-times-scraper/api/client/nodejs", "title": "Apify API and Epoch Times Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.theepochtimes.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"natasha.lekh/epoch-times-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.theepochtimes.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "natasha.lekh/epoch-times-scraper"}, "mshopik/vapordna-scraper": {"id": 1601, "url": "https://apify.com/mshopik/vapordna-scraper/api/client/nodejs", "title": "Apify API and VaporDNA Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/vapordna-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/vapordna-scraper"}, "yuri/icon-burglar": {"id": 1602, "url": "https://apify.com/yuri/icon-burglar/api/client/nodejs", "title": "Apify API and Icon Burglar interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"yuri/icon-burglar\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "yuri/icon-burglar"}, "mshopik/hasbro-pulse-scraper": {"id": 1603, "url": "https://apify.com/mshopik/hasbro-pulse-scraper/api/client/nodejs", "title": "Apify API and Hasbro Pulse Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/hasbro-pulse-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/hasbro-pulse-scraper"}, "zuzka/franceinfo-scraper": {"id": 1605, "url": "https://apify.com/zuzka/franceinfo-scraper/api/client/nodejs", "title": "Apify API and Franceinfo Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.francetvinfo.fr/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/franceinfo-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.francetvinfo.fr/"}], "maxArticlesPerCrawl": 100}, "actor_id": "zuzka/franceinfo-scraper"}, "glaring_santoor/apify-store-scraper": {"id": 1606, "url": "https://apify.com/glaring_santoor/apify-store-scraper/api/client/nodejs", "title": "Apify API and Apify Store Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://apify.com\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"glaring_santoor/apify-store-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://apify.com"}]}, "actor_id": "glaring_santoor/apify-store-scraper"}, "datastorm/market-data-api": {"id": 1607, "url": "https://apify.com/datastorm/market-data-api/api/client/nodejs", "title": "Apify API and Stock Market Data Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"symbols\": [\n        \"MSFT\"\n    ],\n    \"days\": \"7\",\n    \"interval\": \"1d\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"datastorm/market-data-api\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"days": "7", "symbols": ["MSFT"], "interval": "1d"}, "actor_id": "datastorm/market-data-api"}, "mnmkng/abort-and-resurrect": {"id": 1608, "url": "https://apify.com/mnmkng/abort-and-resurrect/api/client/nodejs", "title": "Apify API and Abort And Resurrect interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"transformInputSource\": function transformInput({ input, options }) {\n         input.foo = 'bar';\n         options.memory = 2048;\n     }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mnmkng/abort-and-resurrect\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "mnmkng/abort-and-resurrect"}, "lukaskrivka/generate-people-pairs": {"id": 1609, "url": "https://apify.com/lukaskrivka/generate-people-pairs/api/client/nodejs", "title": "Apify API and Generate People Pairs interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"names\": [\n        \"Elon Musk\",\n        \"Mark Zuckerberg\",\n        \"Bill Gates\",\n        \"Steve Jobs\",\n        \"Jeff Bezos\"\n    ],\n    \"teamName\": \"My best team\",\n    \"skipEveryTimes\": 0,\n    \"slackMessage\": \"Chosen pairs are:\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/generate-people-pairs\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"names": ["Elon Musk", "Mark Zuckerberg", "Bill Gates", "Steve Jobs", "Jeff Bezos"], "teamName": "My best team", "slackMessage": "Chosen pairs are:", "skipEveryTimes": 0}, "actor_id": "lukaskrivka/generate-people-pairs"}, "mshopik/alo-yoga-scraper": {"id": 1610, "url": "https://apify.com/mshopik/alo-yoga-scraper/api/client/nodejs", "title": "Apify API and Alo Yoga Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/alo-yoga-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/alo-yoga-scraper"}, "maria.f/abc-scraper": {"id": 1611, "url": "https://apify.com/maria.f/abc-scraper/api/client/nodejs", "title": "Apify API and ABC Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.abc.net.au/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"maria.f/abc-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.abc.net.au/"}], "maxArticlesPerCrawl": 100}, "actor_id": "maria.f/abc-scraper"}, "forbit/req": {"id": 1675, "url": "https://apify.com/forbit/req/api/client/nodejs", "title": "Apify API and req interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.apify.com/\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"forbit/req\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.apify.com/"}, "actor_id": "forbit/req"}, "mshopik/true-leaf-market-scraper": {"id": 1612, "url": "https://apify.com/mshopik/true-leaf-market-scraper/api/client/nodejs", "title": "Apify API and True Leaf Market Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/true-leaf-market-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/true-leaf-market-scraper"}, "hanatsai/the-atlantic--scraper": {"id": 1613, "url": "https://apify.com/hanatsai/the-atlantic--scraper/api/client/nodejs", "title": "Apify API and The Atlantic Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"http://theatlantic.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hanatsai/the-atlantic--scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "http://theatlantic.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "hanatsai/the-atlantic--scraper"}, "mshopik/the-citizenry-scraper": {"id": 1614, "url": "https://apify.com/mshopik/the-citizenry-scraper/api/client/nodejs", "title": "Apify API and The Citizenry Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-citizenry-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/the-citizenry-scraper"}, "canadesk/sentiment-analyzer": {"id": 1615, "url": "https://apify.com/canadesk/sentiment-analyzer/api/client/nodejs", "title": "Apify API and Sentiment Analyzer interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"text\": \"I'm very happy!\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/sentiment-analyzer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"text": "I'm very happy!", "proxy": {"useApifyProxy": true}}, "actor_id": "canadesk/sentiment-analyzer"}, "web.harvester/twitter-users-ids-scraper": {"id": 1616, "url": "https://apify.com/web.harvester/twitter-users-ids-scraper/api/client/nodejs", "title": "Apify API and Twitter Users IDs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"handles\": [\n        \"elonmusk\"\n    ],\n    \"userQueries\": [],\n    \"profilesDesired\": 10,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ]\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"web.harvester/twitter-users-ids-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"handles": ["elonmusk"], "proxyConfig": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"]}, "userQueries": [], "profilesDesired": 10}, "actor_id": "web.harvester/twitter-users-ids-scraper"}, "mshopik/incasecom-scraper": {"id": 1617, "url": "https://apify.com/mshopik/incasecom-scraper/api/client/nodejs", "title": "Apify API and Incase.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/incasecom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/incasecom-scraper"}, "buseta/crypto-news": {"id": 1618, "url": "https://apify.com/buseta/crypto-news/api/client/nodejs", "title": "Apify API and Crypto News Pro Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"buseta/crypto-news\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "buseta/crypto-news"}, "lukass/idealista-crawler-italy": {"id": 1619, "url": "https://apify.com/lukass/idealista-crawler-italy/api/client/nodejs", "title": "Apify API and Idealist.com-italy interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"district\": \"Rome\",\n    \"maxItems\": 33,\n    \"startUrl\": [],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"IT\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukass/idealista-crawler-italy\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "IT"}, "district": "Rome", "maxItems": 33, "startUrl": []}, "actor_id": "lukass/idealista-crawler-italy"}, "mshopik/allbirds-scraper": {"id": 1620, "url": "https://apify.com/mshopik/allbirds-scraper/api/client/nodejs", "title": "Apify API and Allbirds Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/allbirds-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/allbirds-scraper"}, "mshopik/reservebar-scraper": {"id": 1621, "url": "https://apify.com/mshopik/reservebar-scraper/api/client/nodejs", "title": "Apify API and ReserveBar Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/reservebar-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/reservebar-scraper"}, "theo/abc-news-scraper": {"id": 1622, "url": "https://apify.com/theo/abc-news-scraper/api/client/nodejs", "title": "Apify API and ABC News Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://abcnews.go.com/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"theo/abc-news-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://abcnews.go.com/"}], "maxArticlesPerCrawl": 100}, "actor_id": "theo/abc-news-scraper"}, "mshopik/eat-cake-today-scraper": {"id": 1623, "url": "https://apify.com/mshopik/eat-cake-today-scraper/api/client/nodejs", "title": "Apify API and Eat Cake Today Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/eat-cake-today-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/eat-cake-today-scraper"}, "fearless_sharpener/hacker-news-top-sites-scraper": {"id": 1624, "url": "https://apify.com/fearless_sharpener/hacker-news-top-sites-scraper/api/client/nodejs", "title": "Apify API and Hacker News Top Sites Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"fearless_sharpener/hacker-news-top-sites-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "fearless_sharpener/hacker-news-top-sites-scraper"}, "jindrich.bar/flight-ticket-scraper": {"id": 1625, "url": "https://apify.com/jindrich.bar/flight-ticket-scraper/api/client/nodejs", "title": "Apify API and \ud83d\udeeb Flight Ticket Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"fromIATAs\": [\n        \"PRG\"\n    ],\n    \"toIATAs\": [\n        \"MUC\"\n    ],\n    \"currency\": \"USD\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jindrich.bar/flight-ticket-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"toIATAs": ["MUC"], "currency": "USD", "fromIATAs": ["PRG"]}, "actor_id": "jindrich.bar/flight-ticket-scraper"}, "canadesk/gartner": {"id": 1627, "url": "https://apify.com/canadesk/gartner/api/client/nodejs", "title": "Apify API and Gartner interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"category\": \"expense-management-software\",\n    \"website\": \"Expensify\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/gartner\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "website": "Expensify", "category": "expense-management-software"}, "actor_id": "canadesk/gartner"}, "jupri/skyscanner-hotels": {"id": 1628, "url": "https://apify.com/jupri/skyscanner-hotels/api/client/nodejs", "title": "Apify API and Skyscanner Hotels \ud83c\udfe8 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"Alaska\",\n    \"limit\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/skyscanner-hotels\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"limit": 10, "location": "Alaska"}, "actor_id": "jupri/skyscanner-hotels"}, "asadali/skybox": {"id": 1629, "url": "https://apify.com/asadali/skybox/api/client/nodejs", "title": "Apify API and skyBox-AmazonScrapper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://apify.com\"\n        }\n    ],\n    \"Proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"asadali/skybox\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"Proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://apify.com"}]}, "actor_id": "asadali/skybox"}, "mshopik/my-jewelscom-scraper": {"id": 1630, "url": "https://apify.com/mshopik/my-jewelscom-scraper/api/client/nodejs", "title": "Apify API and my-jewels.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/my-jewelscom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/my-jewelscom-scraper"}, "hamza.alwan/gab-marketplace-scraper": {"id": 1631, "url": "https://apify.com/hamza.alwan/gab-marketplace-scraper/api/client/nodejs", "title": "Apify API and Gab Marketplace Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [],\n    \"marketplaceSearchQueries\": [\n        \"Cream\"\n    ],\n    \"desiredMarketplaceListingsCount\": 100,\n    \"marketplaceSortBy\": \"NEWEST\",\n    \"marketplaceCategory\": \"ALL\",\n    \"marketplaceCondition\": \"ALL\",\n    \"marketplacePriceFrom\": 0,\n    \"marketplacePriceTo\": 99999,\n    \"marketplaceShipping\": \"ALL\",\n    \"marketplaceLocation\": \"\",\n    \"marketplaceTags\": []\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/gab-marketplace-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [], "marketplaceTags": [], "marketplaceSortBy": "NEWEST", "marketplacePriceTo": 99999, "marketplaceCategory": "ALL", "marketplaceLocation": "", "marketplaceShipping": "ALL", "marketplaceCondition": "ALL", "marketplacePriceFrom": 0, "marketplaceSearchQueries": ["Cream"], "desiredMarketplaceListingsCount": 100}, "actor_id": "hamza.alwan/gab-marketplace-scraper"}, "rikunk/my-anime-list-scraper": {"id": 1632, "url": "https://apify.com/rikunk/my-anime-list-scraper/api/client/nodejs", "title": "Apify API and MyAnimeList Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchTitles\": [\n        \"Attack on Titan\",\n        \"Spirited Away\"\n    ],\n    \"searchURLs\": [\n        \"https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood\",\n        \"https://myanimelist.net/anime/11061/Hunter_x_Hunter_2011\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"rikunk/my-anime-list-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"searchURLs": ["https://myanimelist.net/anime/5114/Fullmetal_Alchemist__Brotherhood", "https://myanimelist.net/anime/11061/Hunter_x_Hunter_2011"], "searchTitles": ["Attack on Titan", "Spirited Away"]}, "actor_id": "rikunk/my-anime-list-scraper"}, "dtrungtin/covid-si": {"id": 1633, "url": "https://apify.com/dtrungtin/covid-si/api/client/nodejs", "title": "Apify API and Coronavirus stats in Slovenia interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtrungtin/covid-si\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "dtrungtin/covid-si"}, "vaclavrut/executionid-to-xls-and-send": {"id": 1634, "url": "https://apify.com/vaclavrut/executionid-to-xls-and-send/api/client/nodejs", "title": "Apify API and Execution ID To XLS Send interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"vaclavrut/executionid-to-xls-and-send\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "vaclavrut/executionid-to-xls-and-send"}, "aaron/dropbox-upload-actor": {"id": 1635, "url": "https://apify.com/aaron/dropbox-upload-actor/api/client/nodejs", "title": "Apify API and Dropbox Upload Actor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"aaron/dropbox-upload-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "aaron/dropbox-upload-actor"}, "krakorj/covid-netherland": {"id": 1636, "url": "https://apify.com/krakorj/covid-netherland/api/client/nodejs", "title": "Apify API and Coronavirus stats in the Netherlands interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"krakorj/covid-netherland\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "krakorj/covid-netherland"}, "petr_cermak/ftp-upload": {"id": 1637, "url": "https://apify.com/petr_cermak/ftp-upload/api/client/nodejs", "title": "Apify API and FTP file uploader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"port\": 21,\n    \"folder\": \"/\",\n    \"fileUrls\": []\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/ftp-upload\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"port": 21, "folder": "/", "fileUrls": []}, "actor_id": "petr_cermak/ftp-upload"}, "drobnikj/keboola-input-mapping": {"id": 1638, "url": "https://apify.com/drobnikj/keboola-input-mapping/api/client/nodejs", "title": "Apify API and Keboola Input Mapping interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"inputMapping\": async function inputMappingFunction({ currentInput, parsedInputTableCsv }) {\n        console.log(parsedInputTableCsv);\n        const targetInput = {\n            startUrls: parsedInputTableCsv.map((line) => line['URL']),\n        }\n        return targetInput;\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"drobnikj/keboola-input-mapping\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "drobnikj/keboola-input-mapping"}, "medh/humblebundle": {"id": 1639, "url": "https://apify.com/medh/humblebundle/api/client/nodejs", "title": "Apify API and HumbleBundle Deals interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"medh/humblebundle\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "medh/humblebundle"}, "juansgaitan/crawler-orchestra": {"id": 1640, "url": "https://apify.com/juansgaitan/crawler-orchestra/api/client/nodejs", "title": "Apify API and Crawler Orchestra interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"juansgaitan/crawler-orchestra\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "juansgaitan/crawler-orchestra"}, "apify/actor-readme-generator": {"id": 1641, "url": "https://apify.com/apify/actor-readme-generator/api/client/nodejs", "title": "Apify API and Actor Readme Generator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"readmeExamples\": [\n        \"https://apify.com/misceres/indeed-scraper\",\n        \"https://apify.com/lukaskrivka/google-maps-with-contact-details\",\n        \"https://apify.com/apify/facebook-events-scraper\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"apify/actor-readme-generator\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"readmeExamples": ["https://apify.com/misceres/indeed-scraper", "https://apify.com/lukaskrivka/google-maps-with-contact-details", "https://apify.com/apify/facebook-events-scraper"]}, "actor_id": "apify/actor-readme-generator"}, "hreal/my-actor": {"id": 1642, "url": "https://apify.com/hreal/my-actor/api/client/nodejs", "title": "Apify API and 959395053th covid api interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hreal/my-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "hreal/my-actor"}, "maria.f/new-york-post-scraper": {"id": 1643, "url": "https://apify.com/maria.f/new-york-post-scraper/api/client/nodejs", "title": "Apify API and New York Post Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://nypost.com/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"maria.f/new-york-post-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://nypost.com/"}], "maxArticlesPerCrawl": 100}, "actor_id": "maria.f/new-york-post-scraper"}, "strajk/wiggle-wiggle-com-scraper": {"id": 1644, "url": "https://apify.com/strajk/wiggle-wiggle-com-scraper/api/client/nodejs", "title": "Apify API and Wiggle (wiggle.com) scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"TEST\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"strajk/wiggle-wiggle-com-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "TEST"}, "actor_id": "strajk/wiggle-wiggle-com-scraper"}, "mtrunkatova/sfgate-scraper": {"id": 1645, "url": "https://apify.com/mtrunkatova/sfgate-scraper/api/client/nodejs", "title": "Apify API and SFGate Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.sfgate.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mtrunkatova/sfgate-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.sfgate.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "mtrunkatova/sfgate-scraper"}, "mshopik/unique-vintage-scraper": {"id": 1646, "url": "https://apify.com/mshopik/unique-vintage-scraper/api/client/nodejs", "title": "Apify API and Unique Vintage Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/unique-vintage-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/unique-vintage-scraper"}, "mtrunkatova/us-magazine-scraper": {"id": 1647, "url": "https://apify.com/mtrunkatova/us-magazine-scraper/api/client/nodejs", "title": "Apify API and US Magazine Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.usmagazine.com/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mtrunkatova/us-magazine-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.usmagazine.com/"}], "maxArticlesPerCrawl": 100}, "actor_id": "mtrunkatova/us-magazine-scraper"}, "curious_coder/linkedin-group-members-scraper": {"id": 1648, "url": "https://apify.com/curious_coder/linkedin-group-members-scraper/api/client/nodejs", "title": "Apify API and Linkedin group members scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.linkedin.com/groups/100358\",\n    \"startPage\": 1,\n    \"endPage\": 2,\n    \"minDelay\": 2,\n    \"maxDelay\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/linkedin-group-members-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.linkedin.com/groups/100358", "endPage": 2, "maxDelay": 5, "minDelay": 2, "startPage": 1}, "actor_id": "curious_coder/linkedin-group-members-scraper"}, "mshopik/hafelehome-scraper": {"id": 1649, "url": "https://apify.com/mshopik/hafelehome-scraper/api/client/nodejs", "title": "Apify API and H\u00e4feleHome Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/hafelehome-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/hafelehome-scraper"}, "mshopik/thursday-boot-company-scraper": {"id": 1650, "url": "https://apify.com/mshopik/thursday-boot-company-scraper/api/client/nodejs", "title": "Apify API and Thursday Boot Company Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/thursday-boot-company-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/thursday-boot-company-scraper"}, "social-analytics/rumble-channel-analytics": {"id": 1651, "url": "https://apify.com/social-analytics/rumble-channel-analytics/api/client/nodejs", "title": "Apify API and Rumble Channel Analytics API interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"channel_names\": [\n        \"UFC\",\n        \"powerslap\",\n        \"russellbrand\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"social-analytics/rumble-channel-analytics\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"channel_names": ["UFC", "powerslap", "russellbrand"]}, "actor_id": "social-analytics/rumble-channel-analytics"}, "jancurn/probe-resources-plus-webhook": {"id": 1652, "url": "https://apify.com/jancurn/probe-resources-plus-webhook/api/client/nodejs", "title": "Apify API and Probe Resources Plus Webhook interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jancurn/probe-resources-plus-webhook\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jancurn/probe-resources-plus-webhook"}, "mshopik/brodo-store-scraper": {"id": 1653, "url": "https://apify.com/mshopik/brodo-store-scraper/api/client/nodejs", "title": "Apify API and BRODO Store Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/brodo-store-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/brodo-store-scraper"}, "mshopik/beyond-yoga-scraper": {"id": 1654, "url": "https://apify.com/mshopik/beyond-yoga-scraper/api/client/nodejs", "title": "Apify API and Beyond Yoga Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/beyond-yoga-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/beyond-yoga-scraper"}, "mshopik/nine-west-scraper": {"id": 1655, "url": "https://apify.com/mshopik/nine-west-scraper/api/client/nodejs", "title": "Apify API and Nine West Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/nine-west-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/nine-west-scraper"}, "thibault/extract-signatures-chambre-depute-luxembourg": {"id": 1690, "url": "https://apify.com/thibault/extract-signatures-chambre-depute-luxembourg/api/client/nodejs", "title": "Apify API and Extract names and city from petition.lu website. interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"thibault/extract-signatures-chambre-depute-luxembourg\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "thibault/extract-signatures-chambre-depute-luxembourg"}, "mshopik/only-ny-scraper": {"id": 1656, "url": "https://apify.com/mshopik/only-ny-scraper/api/client/nodejs", "title": "Apify API and Only NY Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/only-ny-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/only-ny-scraper"}, "mshopik/urban-excess-scraper": {"id": 1657, "url": "https://apify.com/mshopik/urban-excess-scraper/api/client/nodejs", "title": "Apify API and URBAN EXCESS Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/urban-excess-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/urban-excess-scraper"}, "mshopik/cymaticsfm-scraper": {"id": 1658, "url": "https://apify.com/mshopik/cymaticsfm-scraper/api/client/nodejs", "title": "Apify API and Cymatics.fm Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/cymaticsfm-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/cymaticsfm-scraper"}, "mshopik/the-spice-house-scraper": {"id": 1659, "url": "https://apify.com/mshopik/the-spice-house-scraper/api/client/nodejs", "title": "Apify API and The Spice House Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-spice-house-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/the-spice-house-scraper"}, "mshopik/harney-and-sons-fine-scraper": {"id": 1660, "url": "https://apify.com/mshopik/harney-and-sons-fine-scraper/api/client/nodejs", "title": "Apify API and Harney Sons Fine Teas Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/harney-and-sons-fine-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/harney-and-sons-fine-scraper"}, "jupri/faceswapify": {"id": 1661, "url": "https://apify.com/jupri/faceswapify/api/client/nodejs", "title": "Apify API and DeepFakes Faceswap AI interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"face\": \"https://hips.hearstapps.com/hmg-prod/images/gettyimages-1229892983-square.jpg\",\n    \"base\": \"https://musicart.xboxlive.com/7/d7350600-0000-0000-0000-000000000002/504/image.jpg\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/faceswapify\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"base": "https://musicart.xboxlive.com/7/d7350600-0000-0000-0000-000000000002/504/image.jpg", "face": "https://hips.hearstapps.com/hmg-prod/images/gettyimages-1229892983-square.jpg"}, "actor_id": "jupri/faceswapify"}, "mshopik/frankies-bikinis-scraper": {"id": 1662, "url": "https://apify.com/mshopik/frankies-bikinis-scraper/api/client/nodejs", "title": "Apify API and Frankies Bikinis Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/frankies-bikinis-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/frankies-bikinis-scraper"}, "mshopik/nixplay-scraper": {"id": 1663, "url": "https://apify.com/mshopik/nixplay-scraper/api/client/nodejs", "title": "Apify API and Nixplay Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/nixplay-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/nixplay-scraper"}, "jupri/markdown-test": {"id": 1664, "url": "https://apify.com/jupri/markdown-test/api/client/nodejs", "title": "Apify API and markdown-test interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/markdown-test\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jupri/markdown-test"}, "mshopik/misfits-market-scraper": {"id": 1665, "url": "https://apify.com/mshopik/misfits-market-scraper/api/client/nodejs", "title": "Apify API and Misfits Market Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/misfits-market-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/misfits-market-scraper"}, "mshopik/motherhood-maternity-scraper": {"id": 1666, "url": "https://apify.com/mshopik/motherhood-maternity-scraper/api/client/nodejs", "title": "Apify API and Motherhood Maternity Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/motherhood-maternity-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/motherhood-maternity-scraper"}, "mshopik/troy-lee-designs-scraper": {"id": 1667, "url": "https://apify.com/mshopik/troy-lee-designs-scraper/api/client/nodejs", "title": "Apify API and Troy Lee Designs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/troy-lee-designs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/troy-lee-designs-scraper"}, "lukass/covid-est": {"id": 1710, "url": "https://apify.com/lukass/covid-est/api/client/nodejs", "title": "Apify API and Coronavirus stats in Estonia interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukass/covid-est\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukass/covid-est"}, "mscraper/airasia-scraper": {"id": 1668, "url": "https://apify.com/mscraper/airasia-scraper/api/client/nodejs", "title": "Apify API and AirAsia Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"origin\": \"WAW\",\n    \"departureDate\": \"24/12/2023\",\n    \"returnDate\": \"\",\n    \"destination\": \"AUH\",\n    \"countryCode\": \"PL\",\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"groups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"PL\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/airasia-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"groups": ["RESIDENTIAL"], "useApifyProxy": true, "apifyProxyCountry": "PL"}, "origin": "WAW", "returnDate": "", "countryCode": "PL", "destination": "AUH", "departureDate": "24/12/2023"}, "actor_id": "mscraper/airasia-scraper"}, "mshopik/electro-threads-scraper": {"id": 1669, "url": "https://apify.com/mshopik/electro-threads-scraper/api/client/nodejs", "title": "Apify API and Electro Threads Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/electro-threads-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/electro-threads-scraper"}, "mshopik/cotopaxi-scraper": {"id": 1670, "url": "https://apify.com/mshopik/cotopaxi-scraper/api/client/nodejs", "title": "Apify API and Cotopaxi Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/cotopaxi-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/cotopaxi-scraper"}, "natasha.lekh/cbs-news-scraper": {"id": 1671, "url": "https://apify.com/natasha.lekh/cbs-news-scraper/api/client/nodejs", "title": "Apify API and CBS News Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.cbsnews.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"natasha.lekh/cbs-news-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.cbsnews.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "natasha.lekh/cbs-news-scraper"}, "mshopik/interiors-online-scraper": {"id": 1672, "url": "https://apify.com/mshopik/interiors-online-scraper/api/client/nodejs", "title": "Apify API and Interiors Online Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/interiors-online-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/interiors-online-scraper"}, "zuzka/covid-hr": {"id": 1673, "url": "https://apify.com/zuzka/covid-hr/api/client/nodejs", "title": "Apify API and Coronavirus stats in Croatia interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"email\": \"zuzka@apify.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/covid-hr\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"email": "zuzka@apify.com"}, "actor_id": "zuzka/covid-hr"}, "hamza.alwan/reverso-translator": {"id": 1674, "url": "https://apify.com/hamza.alwan/reverso-translator/api/client/nodejs", "title": "Apify API and Reverso Translator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"text\": \"I want to translate this text\",\n    \"sourceLanguage\": \"eng\",\n    \"targetLanguage\": \"spa\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/reverso-translator\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"text": "I want to translate this text", "sourceLanguage": "eng", "targetLanguage": "spa"}, "actor_id": "hamza.alwan/reverso-translator"}, "mshopik/la-crosse-technology-scraper": {"id": 1676, "url": "https://apify.com/mshopik/la-crosse-technology-scraper/api/client/nodejs", "title": "Apify API and La Crosse Technology Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/la-crosse-technology-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/la-crosse-technology-scraper"}, "mshopik/radioshack-scraper": {"id": 1677, "url": "https://apify.com/mshopik/radioshack-scraper/api/client/nodejs", "title": "Apify API and RadioShack Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/radioshack-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/radioshack-scraper"}, "barry8schneider/task-metrics": {"id": 1678, "url": "https://apify.com/barry8schneider/task-metrics/api/client/nodejs", "title": "Apify API and Task Metrics interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"barry8schneider/task-metrics\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "barry8schneider/task-metrics"}, "lukaskrivka/sort-dataset-items": {"id": 1679, "url": "https://apify.com/lukaskrivka/sort-dataset-items/api/client/nodejs", "title": "Apify API and Sort Dataset Items interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/sort-dataset-items\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukaskrivka/sort-dataset-items"}, "mshopik/sneaker-politics-scraper": {"id": 1680, "url": "https://apify.com/mshopik/sneaker-politics-scraper/api/client/nodejs", "title": "Apify API and Sneaker Politics Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/sneaker-politics-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/sneaker-politics-scraper"}, "yeyo/screenshot": {"id": 1681, "url": "https://apify.com/yeyo/screenshot/api/client/nodejs", "title": "Apify API and Screenshot: Website Image and Video Capture interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://apify.com\",\n    \"device\": \"Laptop (1920x1080)\",\n    \"output\": \"jpeg\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"yeyo/screenshot\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://apify.com", "device": "Laptop (1920x1080)", "output": "jpeg"}, "actor_id": "yeyo/screenshot"}, "deeper/youtube-comment-scrapper": {"id": 1682, "url": "https://apify.com/deeper/youtube-comment-scrapper/api/client/nodejs", "title": "Apify API and Youtube Comment Scrapper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": [\n        {\n            \"url\": \"https://www.youtube.com/watch?v=3xGwtP97FXI\"\n        }\n    ],\n    \"max_comments\": 100,\n    \"proxySettings\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"deeper/youtube-comment-scrapper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"start_urls": [{"url": "https://www.youtube.com/watch?v=3xGwtP97FXI"}], "max_comments": 100, "proxySettings": {"useApifyProxy": false}}, "actor_id": "deeper/youtube-comment-scrapper"}, "mshopik/dtlr-scraper": {"id": 1683, "url": "https://apify.com/mshopik/dtlr-scraper/api/client/nodejs", "title": "Apify API and DTLR Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/dtlr-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/dtlr-scraper"}, "santa77/cashreceiptsk": {"id": 1684, "url": "https://apify.com/santa77/cashreceiptsk/api/client/nodejs", "title": "Apify API and Cash Receipt SK scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"receiptId\": \"O-7AF0CFF74D45483FB0CFF74D45283FA1\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"santa77/cashreceiptsk\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"receiptId": "O-7AF0CFF74D45483FB0CFF74D45283FA1"}, "actor_id": "santa77/cashreceiptsk"}, "adrian_horning/mclennan-county-tax-assessor-api": {"id": 1685, "url": "https://apify.com/adrian_horning/mclennan-county-tax-assessor-api/api/client/nodejs", "title": "Apify API and McLennan County Tax Assessor API interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"address\": \"1918 Sanger Ave\",\n    \"accountId\": \"00000577729000000\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"adrian_horning/mclennan-county-tax-assessor-api\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"address": "1918 Sanger Ave", "accountId": "00000577729000000"}, "actor_id": "adrian_horning/mclennan-county-tax-assessor-api"}, "mshopik/los-angeles-apparel-scraper": {"id": 1686, "url": "https://apify.com/mshopik/los-angeles-apparel-scraper/api/client/nodejs", "title": "Apify API and Los Angeles Apparel Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/los-angeles-apparel-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/los-angeles-apparel-scraper"}, "runtime/vimeo": {"id": 1687, "url": "https://apify.com/runtime/vimeo/api/client/nodejs", "title": "Apify API and Vimeo interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.vimeo.com/wearepixelartworks\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"runtime/vimeo\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.vimeo.com/wearepixelartworks"}, "actor_id": "runtime/vimeo"}, "mshopik/shady-raysr-scraper": {"id": 1688, "url": "https://apify.com/mshopik/shady-raysr-scraper/api/client/nodejs", "title": "Apify API and Shady Rays\u00ae Polarized Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/shady-raysr-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/shady-raysr-scraper"}, "novotnyj/uk-vat-number-checker": {"id": 1689, "url": "https://apify.com/novotnyj/uk-vat-number-checker/api/client/nodejs", "title": "Apify API and UK VAT number checker interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"vatIds\": [\n        \"GB123456789\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"novotnyj/uk-vat-number-checker\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"vatIds": ["GB123456789"]}, "actor_id": "novotnyj/uk-vat-number-checker"}, "mshopik/your-super-scraper": {"id": 1691, "url": "https://apify.com/mshopik/your-super-scraper/api/client/nodejs", "title": "Apify API and Your Super Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/your-super-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/your-super-scraper"}, "mshopik/crown-and-caliber-scraper": {"id": 1692, "url": "https://apify.com/mshopik/crown-and-caliber-scraper/api/client/nodejs", "title": "Apify API and Crown Caliber Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/crown-and-caliber-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/crown-and-caliber-scraper"}, "mshopik/culture-kings-scraper": {"id": 1693, "url": "https://apify.com/mshopik/culture-kings-scraper/api/client/nodejs", "title": "Apify API and Culture Kings Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/culture-kings-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/culture-kings-scraper"}, "mshopik/g-fuel-scraper": {"id": 1694, "url": "https://apify.com/mshopik/g-fuel-scraper/api/client/nodejs", "title": "Apify API and FUEL Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/g-fuel-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/g-fuel-scraper"}, "canadesk/zoho": {"id": 1695, "url": "https://apify.com/canadesk/zoho/api/client/nodejs", "title": "Apify API and Zoho Help Center interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"subdomain\": \"engineerica\",\n    \"lang\": \"en\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/zoho\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"lang": "en", "proxy": {"useApifyProxy": true}, "subdomain": "engineerica"}, "actor_id": "canadesk/zoho"}, "mshopik/primitive-skateboarding-scraper": {"id": 1696, "url": "https://apify.com/mshopik/primitive-skateboarding-scraper/api/client/nodejs", "title": "Apify API and Primitive Skateboarding Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/primitive-skateboarding-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/primitive-skateboarding-scraper"}, "binhbui/imdb-extractor": {"id": 1697, "url": "https://apify.com/binhbui/imdb-extractor/api/client/nodejs", "title": "Apify API and Imdb Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.imdb.com/search/title/?title_type=feature&user_rating=9.5\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"maxItems\": 50,\n    \"extendOutputFunction\": ($, item) => { return {} }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"binhbui/imdb-extractor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 50, "startUrls": [{"url": "https://www.imdb.com/search/title/?title_type=feature&user_rating=9.5"}], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "binhbui/imdb-extractor"}, "mshopik/leons-scraper": {"id": 1698, "url": "https://apify.com/mshopik/leons-scraper/api/client/nodejs", "title": "Apify API and Leon's Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/leons-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/leons-scraper"}, "boneto/new-york-times-search-scraper": {"id": 1699, "url": "https://apify.com/boneto/new-york-times-search-scraper/api/client/nodejs", "title": "Apify API and New York Times Search Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchPhrase\": \"coronavirus\",\n    \"sortType\": \"newest\",\n    \"maxItems\": 10\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"boneto/new-york-times-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 10, "sortType": "newest", "searchPhrase": "coronavirus"}, "actor_id": "boneto/new-york-times-search-scraper"}, "mshopik/wigscom-scraper": {"id": 1700, "url": "https://apify.com/mshopik/wigscom-scraper/api/client/nodejs", "title": "Apify API and Wigs.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/wigscom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/wigscom-scraper"}, "mshopik/baggu-scraper": {"id": 1701, "url": "https://apify.com/mshopik/baggu-scraper/api/client/nodejs", "title": "Apify API and BAGGU Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/baggu-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/baggu-scraper"}, "mshopik/pat-mcgrath-labs-scraper": {"id": 1702, "url": "https://apify.com/mshopik/pat-mcgrath-labs-scraper/api/client/nodejs", "title": "Apify API and PAT McGRATH LABS Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/pat-mcgrath-labs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/pat-mcgrath-labs-scraper"}, "opesicka/figma-count-files": {"id": 1737, "url": "https://apify.com/opesicka/figma-count-files/api/client/nodejs", "title": "Apify API and Figma file counter interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"opesicka/figma-count-files\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "opesicka/figma-count-files"}, "mshopik/cupshe-scraper": {"id": 1703, "url": "https://apify.com/mshopik/cupshe-scraper/api/client/nodejs", "title": "Apify API and Cupshe Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/cupshe-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/cupshe-scraper"}, "mshopik/untilgonecom-scraper": {"id": 1704, "url": "https://apify.com/mshopik/untilgonecom-scraper/api/client/nodejs", "title": "Apify API and UntilGone.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/untilgonecom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/untilgonecom-scraper"}, "datastorm/cryptoprices-api": {"id": 1705, "url": "https://apify.com/datastorm/cryptoprices-api/api/client/nodejs", "title": "Apify API and Crypto Prices Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"symbols\": [\n        \"BTC\",\n        \"ETH\",\n        \"LTC\"\n    ],\n    \"days\": \"7\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"datastorm/cryptoprices-api\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"days": "7", "symbols": ["BTC", "ETH", "LTC"]}, "actor_id": "datastorm/cryptoprices-api"}, "mshopik/giulio-fashion-scraper": {"id": 1706, "url": "https://apify.com/mshopik/giulio-fashion-scraper/api/client/nodejs", "title": "Apify API and Giulio Fashion Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/giulio-fashion-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/giulio-fashion-scraper"}, "mshopik/cosmetics-by-kylie-scraper": {"id": 1707, "url": "https://apify.com/mshopik/cosmetics-by-kylie-scraper/api/client/nodejs", "title": "Apify API and Kylie Cosmetics by Kylie Jenner Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/cosmetics-by-kylie-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/cosmetics-by-kylie-scraper"}, "jirimoravcik/julia-actor-example": {"id": 1708, "url": "https://apify.com/jirimoravcik/julia-actor-example/api/client/nodejs", "title": "Apify API and Actor in Julia example interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://example.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jirimoravcik/julia-actor-example\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://example.com"}, "actor_id": "jirimoravcik/julia-actor-example"}, "azzouzana/bienici-com-search-pages-scraper": {"id": 1709, "url": "https://apify.com/azzouzana/bienici-com-search-pages-scraper/api/client/nodejs", "title": "Apify API and bienici.com search pages scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrl\": \"https://www.bienici.com/recherche/achat/mayotte-976\",\n    \"maxPages\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"azzouzana/bienici-com-search-pages-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxPages": 1, "startUrl": "https://www.bienici.com/recherche/achat/mayotte-976"}, "actor_id": "azzouzana/bienici-com-search-pages-scraper"}, "service-paradis/bandcamp-crawler": {"id": 1711, "url": "https://apify.com/service-paradis/bandcamp-crawler/api/client/nodejs", "title": "Apify API and Bandcamp Crawler interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://fivefingerdeathpunch.bandcamp.com/music\"\n        }\n    ],\n    \"maxPagesToSearch\": 6,\n    \"proxy\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"service-paradis/bandcamp-crawler\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": false}, "startUrls": [{"url": "https://fivefingerdeathpunch.bandcamp.com/music"}], "maxPagesToSearch": 6}, "actor_id": "service-paradis/bandcamp-crawler"}, "mshopik/couture-candy-scraper": {"id": 1712, "url": "https://apify.com/mshopik/couture-candy-scraper/api/client/nodejs", "title": "Apify API and Couture Candy Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/couture-candy-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/couture-candy-scraper"}, "mscraper/mubi-list-scraper": {"id": 1713, "url": "https://apify.com/mscraper/mubi-list-scraper/api/client/nodejs", "title": "Apify API and Mubi Film Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://mubi.com/lists/i-love-sci-fi\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"resultsLimit\": 20\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/mubi-list-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyCountry": "US"}, "startUrls": [{"url": "https://mubi.com/lists/i-love-sci-fi"}], "resultsLimit": 20}, "actor_id": "mscraper/mubi-list-scraper"}, "mshopik/bluemercury-scraper": {"id": 1714, "url": "https://apify.com/mshopik/bluemercury-scraper/api/client/nodejs", "title": "Apify API and bluemercury Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bluemercury-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bluemercury-scraper"}, "mshopik/vanquish-fitness-scraper": {"id": 1715, "url": "https://apify.com/mshopik/vanquish-fitness-scraper/api/client/nodejs", "title": "Apify API and Vanquish Fitness Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/vanquish-fitness-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/vanquish-fitness-scraper"}, "enco/covid-cl": {"id": 1716, "url": "https://apify.com/enco/covid-cl/api/client/nodejs", "title": "Apify API and Coronavirus stats in Chile interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"enco/covid-cl\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "enco/covid-cl"}, "fjvs0283/actor-batch-builder": {"id": 1717, "url": "https://apify.com/fjvs0283/actor-batch-builder/api/client/nodejs", "title": "Apify API and Actor Build Starter interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"actorNameContains\": \"my-awesome\",\n    \"actorIds\": [\n        \"abc123\",\n        \"abc123\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"fjvs0283/actor-batch-builder\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"actorIds": ["abc123", "abc123"], "actorNameContains": "my-awesome"}, "actor_id": "fjvs0283/actor-batch-builder"}, "mshopik/kick-game-scraper": {"id": 1718, "url": "https://apify.com/mshopik/kick-game-scraper/api/client/nodejs", "title": "Apify API and Kick Game Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/kick-game-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/kick-game-scraper"}, "valek.josef/utils-pipe": {"id": 1719, "url": "https://apify.com/valek.josef/utils-pipe/api/client/nodejs", "title": "Apify API and Utils/Pipe interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"transformFunction\": async ({ payload, apifyClient, log }) => {\n    return {};\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"valek.josef/utils-pipe\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "valek.josef/utils-pipe"}, "mshopik/knix-scraper": {"id": 1720, "url": "https://apify.com/mshopik/knix-scraper/api/client/nodejs", "title": "Apify API and Knix Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/knix-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/knix-scraper"}, "mshopik/campmor-scraper": {"id": 1721, "url": "https://apify.com/mshopik/campmor-scraper/api/client/nodejs", "title": "Apify API and Campmor Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/campmor-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/campmor-scraper"}, "mshopik/the-couture-club-scraper": {"id": 1722, "url": "https://apify.com/mshopik/the-couture-club-scraper/api/client/nodejs", "title": "Apify API and The Couture Club Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-couture-club-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/the-couture-club-scraper"}, "mshopik/zippocom-scraper": {"id": 1723, "url": "https://apify.com/mshopik/zippocom-scraper/api/client/nodejs", "title": "Apify API and Zippo.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/zippocom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/zippocom-scraper"}, "strajk/fitanu-fitanu-com-scraper": {"id": 1759, "url": "https://apify.com/strajk/fitanu-fitanu-com-scraper/api/client/nodejs", "title": "Apify API and Fitanu (fitanu.com) scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"TEST\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"strajk/fitanu-fitanu-com-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "TEST"}, "actor_id": "strajk/fitanu-fitanu-com-scraper"}, "mshopik/wholesome-culture-scraper": {"id": 1724, "url": "https://apify.com/mshopik/wholesome-culture-scraper/api/client/nodejs", "title": "Apify API and Wholesome Culture Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/wholesome-culture-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/wholesome-culture-scraper"}, "mshopik/whole-latte-love-scraper": {"id": 1725, "url": "https://apify.com/mshopik/whole-latte-love-scraper/api/client/nodejs", "title": "Apify API and Whole Latte Love Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/whole-latte-love-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/whole-latte-love-scraper"}, "drobnikj/dataset-processor-python": {"id": 1726, "url": "https://apify.com/drobnikj/dataset-processor-python/api/client/nodejs", "title": "Apify API and Dataset Processor in Python interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"dataset\": \"test-dataset\",\n    \"rowFnc\": \"def process_row(row):\\n    row[\\\"process\\\"] = True\\n    print(f'Row was procesed {row}')\\n    return row\\n\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"drobnikj/dataset-processor-python\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"rowFnc": "def process_row(row):\n    row[\"process\"] = True\n    print(f'Row was procesed {row}')\n    return row\n", "dataset": "test-dataset"}, "actor_id": "drobnikj/dataset-processor-python"}, "jupri/remotezip-extractor": {"id": 1727, "url": "https://apify.com/jupri/remotezip-extractor/api/client/nodejs", "title": "Apify API and Remote ZIP File Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"limit\": 10000\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/remotezip-extractor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"limit": 10000}, "actor_id": "jupri/remotezip-extractor"}, "lukaskrivka/resurrect-on-out-of-memory": {"id": 1728, "url": "https://apify.com/lukaskrivka/resurrect-on-out-of-memory/api/client/nodejs", "title": "Apify API and Resurrect run on Out of memory interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/resurrect-on-out-of-memory\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukaskrivka/resurrect-on-out-of-memory"}, "virgileb/my-actor": {"id": 1729, "url": "https://apify.com/virgileb/my-actor/api/client/nodejs", "title": "Apify API and ZDA interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"virgileb/my-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "virgileb/my-actor"}, "bebich/sreality-scraper": {"id": 1730, "url": "https://apify.com/bebich/sreality-scraper/api/client/nodejs", "title": "Apify API and Sreality scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"AREA\": [\n        \"\"\n    ],\n    \"CITY\": [\n        \"\u010cesk\u00e9 Bud\u011bjovice\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bebich/sreality-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"AREA": [""], "CITY": ["\u010cesk\u00e9 Bud\u011bjovice"]}, "actor_id": "bebich/sreality-scraper"}, "mtrunkatova/newsweek-scraper": {"id": 1758, "url": "https://apify.com/mtrunkatova/newsweek-scraper/api/client/nodejs", "title": "Apify API and Newsweek Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.newsweek.com/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mtrunkatova/newsweek-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.newsweek.com/"}], "maxArticlesPerCrawl": 100}, "actor_id": "mtrunkatova/newsweek-scraper"}, "mshopik/a-bathing-ape-scraper": {"id": 1731, "url": "https://apify.com/mshopik/a-bathing-ape-scraper/api/client/nodejs", "title": "Apify API and *A BATHING APE BAPE ltd Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/a-bathing-ape-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/a-bathing-ape-scraper"}, "mshopik/shoe-city-scraper": {"id": 1732, "url": "https://apify.com/mshopik/shoe-city-scraper/api/client/nodejs", "title": "Apify API and Shoe City Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/shoe-city-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/shoe-city-scraper"}, "mshopik/france-and-son-scraper": {"id": 1733, "url": "https://apify.com/mshopik/france-and-son-scraper/api/client/nodejs", "title": "Apify API and France Son Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/france-and-son-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/france-and-son-scraper"}, "mshopik/teabox-scraper": {"id": 1734, "url": "https://apify.com/mshopik/teabox-scraper/api/client/nodejs", "title": "Apify API and Teabox Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/teabox-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/teabox-scraper"}, "mshopik/lekker-home-scraper": {"id": 1735, "url": "https://apify.com/mshopik/lekker-home-scraper/api/client/nodejs", "title": "Apify API and Lekker Home Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/lekker-home-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/lekker-home-scraper"}, "maria.f/news-au-scraper": {"id": 1736, "url": "https://apify.com/maria.f/news-au-scraper/api/client/nodejs", "title": "Apify API and News AU Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.news.com.au/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"maria.f/news-au-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.news.com.au/"}], "maxArticlesPerCrawl": 100}, "actor_id": "maria.f/news-au-scraper"}, "azzouzana/canadiantire-ca-search-page-products-scraper": {"id": 1738, "url": "https://apify.com/azzouzana/canadiantire-ca-search-page-products-scraper/api/client/nodejs", "title": "Apify API and Canadiantire.ca Search Page Products Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrl\": \"https://www.canadiantire.ca/en/search-results.html?q=shirtsmen;store=216\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"azzouzana/canadiantire-ca-search-page-products-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrl": "https://www.canadiantire.ca/en/search-results.html?q=shirtsmen;store=216"}, "actor_id": "azzouzana/canadiantire-ca-search-page-products-scraper"}, "decisive_pipa/analyzer-ts": {"id": 1739, "url": "https://apify.com/decisive_pipa/analyzer-ts/api/client/nodejs", "title": "Apify API and Analyzer Ts interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\",\n    \"keywords\": [\n        \"A Light in the Attic\",\n        \"\u00a351.77\",\n        \" In stock (22 available) \",\n        \"a897fe39b1053632\",\n        \"It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\"\n    ],\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"decisive_pipa/analyzer-ts\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html", "keywords": ["A Light in the Attic", "\u00a351.77", " In stock (22 available) ", "a897fe39b1053632", "It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more"], "proxyConfig": {"useApifyProxy": true}}, "actor_id": "decisive_pipa/analyzer-ts"}, "mnmkng/rebuilder": {"id": 1740, "url": "https://apify.com/mnmkng/rebuilder/api/client/nodejs", "title": "Apify API and Rebuilder interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mnmkng/rebuilder\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "mnmkng/rebuilder"}, "mshopik/the-brick-scraper": {"id": 1741, "url": "https://apify.com/mshopik/the-brick-scraper/api/client/nodejs", "title": "Apify API and The Brick Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-brick-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/the-brick-scraper"}, "mshopik/machines-online-store-scraper": {"id": 1742, "url": "https://apify.com/mshopik/machines-online-store-scraper/api/client/nodejs", "title": "Apify API and Machines Online Store Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/machines-online-store-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/machines-online-store-scraper"}, "inquisitive_sarangi/bulk-address-parser": {"id": 1743, "url": "https://apify.com/inquisitive_sarangi/bulk-address-parser/api/client/nodejs", "title": "Apify API and Bulk Address Parser interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"addresses\": [\n        \"257 Fireweed Ln Ketchikan Alaska 99901 USA\"\n    ],\n    \"batchSize\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"inquisitive_sarangi/bulk-address-parser\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"addresses": ["257 Fireweed Ln Ketchikan Alaska 99901 USA"], "batchSize": 1}, "actor_id": "inquisitive_sarangi/bulk-address-parser"}, "mvolfik/yet-another-dataset-translator": {"id": 1744, "url": "https://apify.com/mvolfik/yet-another-dataset-translator/api/client/nodejs", "title": "Apify API and Yet Another Dataset Translator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"target_language\": \"en\",\n    \"detect_language_threshold\": \"0.7\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mvolfik/yet-another-dataset-translator\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"target_language": "en", "detect_language_threshold": "0.7"}, "actor_id": "mvolfik/yet-another-dataset-translator"}, "mshopik/wwwmaryruthorganicscom-scraper": {"id": 1745, "url": "https://apify.com/mshopik/wwwmaryruthorganicscom-scraper/api/client/nodejs", "title": "Apify API and www.maryruthorganics.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/wwwmaryruthorganicscom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/wwwmaryruthorganicscom-scraper"}, "mshopik/shop-shashi-scraper": {"id": 1746, "url": "https://apify.com/mshopik/shop-shashi-scraper/api/client/nodejs", "title": "Apify API and SHOP SHASHI Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/shop-shashi-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/shop-shashi-scraper"}, "jefersonflus/infojobs-scrapper": {"id": 1747, "url": "https://apify.com/jefersonflus/infojobs-scrapper/api/client/nodejs", "title": "Apify API and Infojobs-scrapper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://crawlee.dev\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jefersonflus/infojobs-scrapper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://crawlee.dev"}]}, "actor_id": "jefersonflus/infojobs-scrapper"}, "mshopik/outerknown-scraper": {"id": 1748, "url": "https://apify.com/mshopik/outerknown-scraper/api/client/nodejs", "title": "Apify API and Outerknown Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/outerknown-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/outerknown-scraper"}, "mscraper/appgallery-scraper": {"id": 1749, "url": "https://apify.com/mscraper/appgallery-scraper/api/client/nodejs", "title": "Apify API and AppGallery Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"groups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"GB\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/appgallery-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"groups": ["RESIDENTIAL"], "useApifyProxy": true, "apifyProxyCountry": "GB"}}, "actor_id": "mscraper/appgallery-scraper"}, "fearless_sharpener/bubble-io-plugin-scraper": {"id": 1750, "url": "https://apify.com/fearless_sharpener/bubble-io-plugin-scraper/api/client/nodejs", "title": "Apify API and Bubble.io Plugin Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"fearless_sharpener/bubble-io-plugin-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "fearless_sharpener/bubble-io-plugin-scraper"}, "mshopik/balr-scraper": {"id": 1751, "url": "https://apify.com/mshopik/balr-scraper/api/client/nodejs", "title": "Apify API and BALR. Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/balr-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/balr-scraper"}, "mshopik/onthelist-scraper": {"id": 1752, "url": "https://apify.com/mshopik/onthelist-scraper/api/client/nodejs", "title": "Apify API and OnTheList Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/onthelist-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/onthelist-scraper"}, "mshopik/blissy-scraper": {"id": 1753, "url": "https://apify.com/mshopik/blissy-scraper/api/client/nodejs", "title": "Apify API and Blissy Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/blissy-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/blissy-scraper"}, "mshopik/coconut-lane-scraper": {"id": 1754, "url": "https://apify.com/mshopik/coconut-lane-scraper/api/client/nodejs", "title": "Apify API and Coconut Lane Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/coconut-lane-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/coconut-lane-scraper"}, "mshopik/prettylitter-scraper": {"id": 1755, "url": "https://apify.com/mshopik/prettylitter-scraper/api/client/nodejs", "title": "Apify API and PrettyLitter Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/prettylitter-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/prettylitter-scraper"}, "mshopik/solestage-scraper": {"id": 1756, "url": "https://apify.com/mshopik/solestage-scraper/api/client/nodejs", "title": "Apify API and Solestage Solestage Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/solestage-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/solestage-scraper"}, "zuzka/onet-scraper": {"id": 1757, "url": "https://apify.com/zuzka/onet-scraper/api/client/nodejs", "title": "Apify API and Onet Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.onet.pl/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/onet-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.onet.pl/"}], "maxArticlesPerCrawl": 100}, "actor_id": "zuzka/onet-scraper"}, "lukass/covid-az": {"id": 1828, "url": "https://apify.com/lukass/covid-az/api/client/nodejs", "title": "Apify API and Coronavirus stats in Azerbaijan interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukass/covid-az\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "lukass/covid-az"}, "mshopik/bulksupplementscom-scraper": {"id": 1760, "url": "https://apify.com/mshopik/bulksupplementscom-scraper/api/client/nodejs", "title": "Apify API and BulkSupplements.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bulksupplementscom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bulksupplementscom-scraper"}, "mscraper/ollies-store-location-scraper": {"id": 1761, "url": "https://apify.com/mscraper/ollies-store-location-scraper/api/client/nodejs", "title": "Apify API and Ollies Store Location Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/ollies-store-location-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyCountry": "US"}}, "actor_id": "mscraper/ollies-store-location-scraper"}, "mshopik/lumin-scraper": {"id": 1762, "url": "https://apify.com/mshopik/lumin-scraper/api/client/nodejs", "title": "Apify API and Lumin Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/lumin-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/lumin-scraper"}, "mshopik/killstar-uk-scraper": {"id": 1763, "url": "https://apify.com/mshopik/killstar-uk-scraper/api/client/nodejs", "title": "Apify API and KILLSTAR UK Store Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/killstar-uk-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/killstar-uk-scraper"}, "mshopik/rhone-scraper": {"id": 1764, "url": "https://apify.com/mshopik/rhone-scraper/api/client/nodejs", "title": "Apify API and Rhone Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/rhone-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/rhone-scraper"}, "mshopik/the-flex-company-scraper": {"id": 1765, "url": "https://apify.com/mshopik/the-flex-company-scraper/api/client/nodejs", "title": "Apify API and The Flex Company Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 100,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    },\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-flex-company-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "proxyConfig": {"useApifyProxy": true}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 100}, "actor_id": "mshopik/the-flex-company-scraper"}, "mshopik/omaze-scraper": {"id": 1766, "url": "https://apify.com/mshopik/omaze-scraper/api/client/nodejs", "title": "Apify API and Omaze Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/omaze-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/omaze-scraper"}, "mshopik/unif-scraper": {"id": 1767, "url": "https://apify.com/mshopik/unif-scraper/api/client/nodejs", "title": "Apify API and UNIF Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/unif-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/unif-scraper"}, "mtrunkat/actor-task-copier": {"id": 1768, "url": "https://apify.com/mtrunkat/actor-task-copier/api/client/nodejs", "title": "Apify API and Actor Task Copier interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mtrunkat/actor-task-copier\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "mtrunkat/actor-task-copier"}, "mshopik/the-happy-planner-scraper": {"id": 1769, "url": "https://apify.com/mshopik/the-happy-planner-scraper/api/client/nodejs", "title": "Apify API and The Happy Planner Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-happy-planner-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/the-happy-planner-scraper"}, "mshopik/olly-public-benefit-corporation-scraper": {"id": 1770, "url": "https://apify.com/mshopik/olly-public-benefit-corporation-scraper/api/client/nodejs", "title": "Apify API and OLLY Public Benefit Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/olly-public-benefit-corporation-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/olly-public-benefit-corporation-scraper"}, "mshopik/succulents-box-scraper": {"id": 1771, "url": "https://apify.com/mshopik/succulents-box-scraper/api/client/nodejs", "title": "Apify API and Succulents Box Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/succulents-box-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/succulents-box-scraper"}, "jupri/actor-readme": {"id": 1895, "url": "https://apify.com/jupri/actor-readme/api/client/nodejs", "title": "Apify API and README Creator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urls\": [\n        \"https://apify.com/apify/web-scraper\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/actor-readme\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": ["https://apify.com/apify/web-scraper"]}, "actor_id": "jupri/actor-readme"}, "mshopik/design-public-scraper": {"id": 1772, "url": "https://apify.com/mshopik/design-public-scraper/api/client/nodejs", "title": "Apify API and Design Public Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/design-public-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/design-public-scraper"}, "mshopik/mikes-bikes-scraper": {"id": 1773, "url": "https://apify.com/mshopik/mikes-bikes-scraper/api/client/nodejs", "title": "Apify API and Mike's Bikes Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/mikes-bikes-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/mikes-bikes-scraper"}, "mshopik/knockaround-scraper": {"id": 1774, "url": "https://apify.com/mshopik/knockaround-scraper/api/client/nodejs", "title": "Apify API and Knockaround Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/knockaround-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/knockaround-scraper"}, "mshopik/factoryoutletstorecom-scraper": {"id": 1775, "url": "https://apify.com/mshopik/factoryoutletstorecom-scraper/api/client/nodejs", "title": "Apify API and FactoryOutletStore.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/factoryoutletstorecom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/factoryoutletstorecom-scraper"}, "mshopik/thuma-scraper": {"id": 1776, "url": "https://apify.com/mshopik/thuma-scraper/api/client/nodejs", "title": "Apify API and Thuma Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/thuma-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/thuma-scraper"}, "flamboyant_marimba/my-actor": {"id": 1777, "url": "https://apify.com/flamboyant_marimba/my-actor/api/client/nodejs", "title": "Apify API and My Actor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://apify.com\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"flamboyant_marimba/my-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://apify.com"}]}, "actor_id": "flamboyant_marimba/my-actor"}, "kuaima/douban-book-pro": {"id": 1928, "url": "https://apify.com/kuaima/douban-book-pro/api/client/nodejs", "title": "Apify API and douban book pro interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"kuaima/douban-book-pro\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "kuaima/douban-book-pro"}, "mshopik/christy-dawn-scraper": {"id": 1778, "url": "https://apify.com/mshopik/christy-dawn-scraper/api/client/nodejs", "title": "Apify API and Christy Dawn Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/christy-dawn-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/christy-dawn-scraper"}, "canadesk/bing-microsoft-translator": {"id": 1779, "url": "https://apify.com/canadesk/bing-microsoft-translator/api/client/nodejs", "title": "Apify API and Bing Microsoft Translator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"text\": \"Let's do this!\",\n    \"agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/bing-microsoft-translator\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"text": "Let's do this!", "agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36", "proxy": {"useApifyProxy": true}}, "actor_id": "canadesk/bing-microsoft-translator"}, "mshopik/olive-piper-scraper": {"id": 1780, "url": "https://apify.com/mshopik/olive-piper-scraper/api/client/nodejs", "title": "Apify API and Olive Piper Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/olive-piper-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/olive-piper-scraper"}, "mshopik/grunt-style-llc-scraper": {"id": 1781, "url": "https://apify.com/mshopik/grunt-style-llc-scraper/api/client/nodejs", "title": "Apify API and Grunt Style, LLC Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/grunt-style-llc-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/grunt-style-llc-scraper"}, "mshopik/the-chivery-scraper": {"id": 1782, "url": "https://apify.com/mshopik/the-chivery-scraper/api/client/nodejs", "title": "Apify API and The Chivery Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-chivery-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/the-chivery-scraper"}, "mshopik/sengled-usa-scraper": {"id": 1783, "url": "https://apify.com/mshopik/sengled-usa-scraper/api/client/nodejs", "title": "Apify API and Sengled USA Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/sengled-usa-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/sengled-usa-scraper"}, "mshopik/asrv-scraper": {"id": 1784, "url": "https://apify.com/mshopik/asrv-scraper/api/client/nodejs", "title": "Apify API and ASRV Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/asrv-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/asrv-scraper"}, "mshopik/astr-the-label-scraper": {"id": 1785, "url": "https://apify.com/mshopik/astr-the-label-scraper/api/client/nodejs", "title": "Apify API and ASTR The Label Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/astr-the-label-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/astr-the-label-scraper"}, "lukass/cbs-local-scraper": {"id": 1786, "url": "https://apify.com/lukass/cbs-local-scraper/api/client/nodejs", "title": "Apify API and CBS Local Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://cbslocal.com/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukass/cbs-local-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://cbslocal.com/"}], "maxArticlesPerCrawl": 100}, "actor_id": "lukass/cbs-local-scraper"}, "mshopik/vici-scraper": {"id": 1787, "url": "https://apify.com/mshopik/vici-scraper/api/client/nodejs", "title": "Apify API and VICI Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/vici-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/vici-scraper"}, "mshopik/august-home-scraper": {"id": 1788, "url": "https://apify.com/mshopik/august-home-scraper/api/client/nodejs", "title": "Apify API and August Home Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/august-home-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/august-home-scraper"}, "mshopik/gorjana-scraper": {"id": 1789, "url": "https://apify.com/mshopik/gorjana-scraper/api/client/nodejs", "title": "Apify API and gorjana Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/gorjana-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/gorjana-scraper"}, "jupri/pdf-extractor": {"id": 1956, "url": "https://apify.com/jupri/pdf-extractor/api/client/nodejs", "title": "Apify API and PDF Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": []\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/pdf-extractor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": []}, "actor_id": "jupri/pdf-extractor"}, "mshopik/beachsissi-scraper": {"id": 1790, "url": "https://apify.com/mshopik/beachsissi-scraper/api/client/nodejs", "title": "Apify API and Beachsissi beachsissi Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/beachsissi-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/beachsissi-scraper"}, "lukaskrivka/input-analyzer": {"id": 1791, "url": "https://apify.com/lukaskrivka/input-analyzer/api/client/nodejs", "title": "Apify API and Input Analyzer interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"runIds\": [\n        \"IlwEbDd4iy2vPfvbw\",\n        \"ZRT5Z0jDzM49zZAaG\",\n        \"Kxed8O2MCLkbj4RMo\"\n    ],\n    \"fieldsOnlyCountPresent\": [\n        \"startUrls\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/input-analyzer\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"runIds": ["IlwEbDd4iy2vPfvbw", "ZRT5Z0jDzM49zZAaG", "Kxed8O2MCLkbj4RMo"], "fieldsOnlyCountPresent": ["startUrls"]}, "actor_id": "lukaskrivka/input-analyzer"}, "mshopik/figs-scraper": {"id": 1792, "url": "https://apify.com/mshopik/figs-scraper/api/client/nodejs", "title": "Apify API and FIGS Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/figs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/figs-scraper"}, "mshopik/sugar-and-cotton-scraper": {"id": 1793, "url": "https://apify.com/mshopik/sugar-and-cotton-scraper/api/client/nodejs", "title": "Apify API and Sugar Cotton Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/sugar-and-cotton-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/sugar-and-cotton-scraper"}, "mshopik/o2-industries-scraper": {"id": 1794, "url": "https://apify.com/mshopik/o2-industries-scraper/api/client/nodejs", "title": "Apify API and O2 Industries Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/o2-industries-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/o2-industries-scraper"}, "mshopik/jaded-london-scraper": {"id": 1795, "url": "https://apify.com/mshopik/jaded-london-scraper/api/client/nodejs", "title": "Apify API and Jaded London Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/jaded-london-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/jaded-london-scraper"}, "mshopik/brave-new-look-scraper": {"id": 1796, "url": "https://apify.com/mshopik/brave-new-look-scraper/api/client/nodejs", "title": "Apify API and Brave New Look Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/brave-new-look-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/brave-new-look-scraper"}, "mshopik/untuckit-scraper": {"id": 1797, "url": "https://apify.com/mshopik/untuckit-scraper/api/client/nodejs", "title": "Apify API and UNTUCKit Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/untuckit-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/untuckit-scraper"}, "mshopik/snow-peak-scraper": {"id": 1798, "url": "https://apify.com/mshopik/snow-peak-scraper/api/client/nodejs", "title": "Apify API and Snow Peak Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/snow-peak-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/snow-peak-scraper"}, "jupri/vrbo-scraper-two": {"id": 1799, "url": "https://apify.com/jupri/vrbo-scraper-two/api/client/nodejs", "title": "Apify API and VRBO Scraper 2.0 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"New York\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/vrbo-scraper-two\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"location": "New York"}, "actor_id": "jupri/vrbo-scraper-two"}, "rikunk/1960tips-scraper": {"id": 1800, "url": "https://apify.com/rikunk/1960tips-scraper/api/client/nodejs", "title": "Apify API and 1960tips Bet Prediction Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"rikunk/1960tips-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "rikunk/1960tips-scraper"}, "mshopik/charming-charlie-scraper": {"id": 1801, "url": "https://apify.com/mshopik/charming-charlie-scraper/api/client/nodejs", "title": "Apify API and Charming Charlie Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/charming-charlie-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/charming-charlie-scraper"}, "mshopik/grill-hey-scraper": {"id": 1802, "url": "https://apify.com/mshopik/grill-hey-scraper/api/client/nodejs", "title": "Apify API and Hey Grill, Hey Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/grill-hey-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/grill-hey-scraper"}, "mshopik/i-saw-it-first-scraper": {"id": 1803, "url": "https://apify.com/mshopik/i-saw-it-first-scraper/api/client/nodejs", "title": "Apify API and SAW IT FIRST Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/i-saw-it-first-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/i-saw-it-first-scraper"}, "mshopik/sabo-skirt-scraper": {"id": 1804, "url": "https://apify.com/mshopik/sabo-skirt-scraper/api/client/nodejs", "title": "Apify API and SABO SKIRT Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/sabo-skirt-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/sabo-skirt-scraper"}, "mshopik/pax-scraper": {"id": 1805, "url": "https://apify.com/mshopik/pax-scraper/api/client/nodejs", "title": "Apify API and PAX Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/pax-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/pax-scraper"}, "mshopik/urban-industry-scraper": {"id": 1806, "url": "https://apify.com/mshopik/urban-industry-scraper/api/client/nodejs", "title": "Apify API and Urban Industry Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/urban-industry-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/urban-industry-scraper"}, "mshopik/blenders-eyewear-scraper": {"id": 1807, "url": "https://apify.com/mshopik/blenders-eyewear-scraper/api/client/nodejs", "title": "Apify API and Blenders Eyewear Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/blenders-eyewear-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/blenders-eyewear-scraper"}, "mshopik/my-childhood-treasures-scraper": {"id": 1808, "url": "https://apify.com/mshopik/my-childhood-treasures-scraper/api/client/nodejs", "title": "Apify API and My Childhood Treasures Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/my-childhood-treasures-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/my-childhood-treasures-scraper"}, "mshopik/stio-scraper": {"id": 1809, "url": "https://apify.com/mshopik/stio-scraper/api/client/nodejs", "title": "Apify API and Stio Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/stio-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/stio-scraper"}, "mshopik/royal-design-studio-stencils-scraper": {"id": 1810, "url": "https://apify.com/mshopik/royal-design-studio-stencils-scraper/api/client/nodejs", "title": "Apify API and Royal Design Studio Stencils Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/royal-design-studio-stencils-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/royal-design-studio-stencils-scraper"}, "mshopik/undefeated-scraper": {"id": 1811, "url": "https://apify.com/mshopik/undefeated-scraper/api/client/nodejs", "title": "Apify API and Undefeated Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/undefeated-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/undefeated-scraper"}, "mshopik/paynes-gray-scraper": {"id": 1812, "url": "https://apify.com/mshopik/paynes-gray-scraper/api/client/nodejs", "title": "Apify API and Paynes Gray Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/paynes-gray-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/paynes-gray-scraper"}, "mshopik/american-giant-scraper": {"id": 1813, "url": "https://apify.com/mshopik/american-giant-scraper/api/client/nodejs", "title": "Apify API and American Giant Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/american-giant-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/american-giant-scraper"}, "mshopik/inkboxtm-scraper": {"id": 1814, "url": "https://apify.com/mshopik/inkboxtm-scraper/api/client/nodejs", "title": "Apify API and Inkbox\u2122 Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/inkboxtm-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/inkboxtm-scraper"}, "mshopik/fertilizer-grow-organic-scraper": {"id": 1815, "url": "https://apify.com/mshopik/fertilizer-grow-organic-scraper/api/client/nodejs", "title": "Apify API and Organic Fertilizer\u2013 Grow Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/fertilizer-grow-organic-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/fertilizer-grow-organic-scraper"}, "svpetrenko/snowflake-uploader": {"id": 1816, "url": "https://apify.com/svpetrenko/snowflake-uploader/api/client/nodejs", "title": "Apify API and Snowflake Uploader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"svpetrenko/snowflake-uploader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "svpetrenko/snowflake-uploader"}, "mshopik/efavormartcom-scraper": {"id": 1817, "url": "https://apify.com/mshopik/efavormartcom-scraper/api/client/nodejs", "title": "Apify API and efavormart.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/efavormartcom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/efavormartcom-scraper"}, "mshopik/bohme-scraper": {"id": 1818, "url": "https://apify.com/mshopik/bohme-scraper/api/client/nodejs", "title": "Apify API and b\u00f6hme Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bohme-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bohme-scraper"}, "mshopik/jlab-audio-scraper": {"id": 1819, "url": "https://apify.com/mshopik/jlab-audio-scraper/api/client/nodejs", "title": "Apify API and JLab Audio Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/jlab-audio-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/jlab-audio-scraper"}, "jupri/homesnap": {"id": 1820, "url": "https://apify.com/jupri/homesnap/api/client/nodejs", "title": "Apify API and Homenap.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"location\": \"New Jersey\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/homesnap\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"location": "New Jersey"}, "actor_id": "jupri/homesnap"}, "mshopik/stussy-europe-scraper": {"id": 1821, "url": "https://apify.com/mshopik/stussy-europe-scraper/api/client/nodejs", "title": "Apify API and Stussy Europe Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/stussy-europe-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/stussy-europe-scraper"}, "mshopik/limelightpk-welcome-scraper": {"id": 1822, "url": "https://apify.com/mshopik/limelightpk-welcome-scraper/api/client/nodejs", "title": "Apify API and Limelightpk Welcome Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/limelightpk-welcome-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/limelightpk-welcome-scraper"}, "mshopik/sock-fancy-scraper": {"id": 1823, "url": "https://apify.com/mshopik/sock-fancy-scraper/api/client/nodejs", "title": "Apify API and Sock Fancy Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/sock-fancy-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/sock-fancy-scraper"}, "mshopik/goli-scraper": {"id": 1824, "url": "https://apify.com/mshopik/goli-scraper/api/client/nodejs", "title": "Apify API and Goli Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/goli-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/goli-scraper"}, "mshopik/princess-polly-usa-scraper": {"id": 1825, "url": "https://apify.com/mshopik/princess-polly-usa-scraper/api/client/nodejs", "title": "Apify API and Princess Polly USA Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/princess-polly-usa-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/princess-polly-usa-scraper"}, "mshopik/ruggable-scraper": {"id": 1826, "url": "https://apify.com/mshopik/ruggable-scraper/api/client/nodejs", "title": "Apify API and Ruggable Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/ruggable-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/ruggable-scraper"}, "mshopik/the-sill-scraper": {"id": 1827, "url": "https://apify.com/mshopik/the-sill-scraper/api/client/nodejs", "title": "Apify API and The Sill Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-sill-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/the-sill-scraper"}, "canadesk/intercom": {"id": 1829, "url": "https://apify.com/canadesk/intercom/api/client/nodejs", "title": "Apify API and Intercom Help Center interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"subdomain\": \"intercom\",\n    \"lang\": \"en\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/intercom\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"lang": "en", "proxy": {"useApifyProxy": true}, "subdomain": "intercom"}, "actor_id": "canadesk/intercom"}, "mshopik/naiise-scraper": {"id": 1830, "url": "https://apify.com/mshopik/naiise-scraper/api/client/nodejs", "title": "Apify API and Naiise Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/naiise-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/naiise-scraper"}, "mshopik/the-music-zoo-scraper": {"id": 1831, "url": "https://apify.com/mshopik/the-music-zoo-scraper/api/client/nodejs", "title": "Apify API and The Music Zoo Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-music-zoo-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/the-music-zoo-scraper"}, "mshopik/laird-superfood-scraper": {"id": 1832, "url": "https://apify.com/mshopik/laird-superfood-scraper/api/client/nodejs", "title": "Apify API and Laird Superfood Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/laird-superfood-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/laird-superfood-scraper"}, "perci/gateway-pundit-scraper": {"id": 1833, "url": "https://apify.com/perci/gateway-pundit-scraper/api/client/nodejs", "title": "Apify API and Gateway Pundit Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.thegatewaypundit.com/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"perci/gateway-pundit-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.thegatewaypundit.com/"}], "maxArticlesPerCrawl": 100}, "actor_id": "perci/gateway-pundit-scraper"}, "natasha.lekh/ksl-scraper": {"id": 1834, "url": "https://apify.com/natasha.lekh/ksl-scraper/api/client/nodejs", "title": "Apify API and KSL Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.ksl.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"natasha.lekh/ksl-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.ksl.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "natasha.lekh/ksl-scraper"}, "mshopik/wwwarcherandolivecom-scraper": {"id": 1835, "url": "https://apify.com/mshopik/wwwarcherandolivecom-scraper/api/client/nodejs", "title": "Apify API and www.archerandolive.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/wwwarcherandolivecom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/wwwarcherandolivecom-scraper"}, "mshopik/banza-scraper": {"id": 1836, "url": "https://apify.com/mshopik/banza-scraper/api/client/nodejs", "title": "Apify API and Banza Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/banza-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/banza-scraper"}, "mshopik/only-natural-pet-scraper": {"id": 1837, "url": "https://apify.com/mshopik/only-natural-pet-scraper/api/client/nodejs", "title": "Apify API and Only Natural Pet Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/only-natural-pet-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/only-natural-pet-scraper"}, "mshopik/ec-scraper": {"id": 1838, "url": "https://apify.com/mshopik/ec-scraper/api/client/nodejs", "title": "Apify API and EC Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/ec-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/ec-scraper"}, "mshopik/biotrust-scraper": {"id": 1839, "url": "https://apify.com/mshopik/biotrust-scraper/api/client/nodejs", "title": "Apify API and BioTRUST Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/biotrust-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/biotrust-scraper"}, "strajk-old/cesky-raj-ceskyraj-com-scraper": {"id": 1840, "url": "https://apify.com/strajk-old/cesky-raj-ceskyraj-com-scraper/api/client/nodejs", "title": "Apify API and \u010cesk\u00fd r\u00e1j (ceskyraj.com) scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"TEST\",\n    \"country\": \"CZ\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"strajk-old/cesky-raj-ceskyraj-com-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "TEST", "country": "CZ"}, "actor_id": "strajk-old/cesky-raj-ceskyraj-com-scraper"}, "hamza.alwan/gab-groups-scraper": {"id": 1841, "url": "https://apify.com/hamza.alwan/gab-groups-scraper/api/client/nodejs", "title": "Apify API and Gab Groups Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [],\n    \"groupsIds\": [\n        \"150\"\n    ],\n    \"desiredPostsCount\": 100,\n    \"desiredCommentsCount\": 0,\n    \"groupPostsSortBy\": \"NEWEST\",\n    \"commentsSortBy\": \"MOST_LIKED\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/gab-groups-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"groupsIds": ["150"], "startUrls": [], "commentsSortBy": "MOST_LIKED", "groupPostsSortBy": "NEWEST", "desiredPostsCount": 100, "desiredCommentsCount": 0}, "actor_id": "hamza.alwan/gab-groups-scraper"}, "hamza.alwan/google-dataset-items-translator": {"id": 1936, "url": "https://apify.com/hamza.alwan/google-dataset-items-translator/api/client/nodejs", "title": "Apify API and Google Dataset Items Translator interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"sourceLanguage\": \"auto\",\n    \"targetLanguage\": \"es\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/google-dataset-items-translator\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"sourceLanguage": "auto", "targetLanguage": "es"}, "actor_id": "hamza.alwan/google-dataset-items-translator"}, "mshopik/neighborhood-scraper": {"id": 1842, "url": "https://apify.com/mshopik/neighborhood-scraper/api/client/nodejs", "title": "Apify API and NEIGHBORHOOD Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/neighborhood-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/neighborhood-scraper"}, "mshopik/osear-malibu-scraper": {"id": 1843, "url": "https://apify.com/mshopik/osear-malibu-scraper/api/client/nodejs", "title": "Apify API and OSEA\u00ae Malibu Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/osear-malibu-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/osear-malibu-scraper"}, "perci/daily-wire-scraper": {"id": 1844, "url": "https://apify.com/perci/daily-wire-scraper/api/client/nodejs", "title": "Apify API and Daily Wire Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.dailywire.com/read\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"perci/daily-wire-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.dailywire.com/read"}], "maxArticlesPerCrawl": 100}, "actor_id": "perci/daily-wire-scraper"}, "maria.f/cheat-sheet-scraper": {"id": 1845, "url": "https://apify.com/maria.f/cheat-sheet-scraper/api/client/nodejs", "title": "Apify API and Cheat Sheet Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.cheatsheet.com/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"maria.f/cheat-sheet-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.cheatsheet.com/"}], "maxArticlesPerCrawl": 100}, "actor_id": "maria.f/cheat-sheet-scraper"}, "mshopik/1st-phorm-scraper": {"id": 1846, "url": "https://apify.com/mshopik/1st-phorm-scraper/api/client/nodejs", "title": "Apify API and 1st Phorm Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/1st-phorm-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/1st-phorm-scraper"}, "mshopik/northern-brewer-scraper": {"id": 1847, "url": "https://apify.com/mshopik/northern-brewer-scraper/api/client/nodejs", "title": "Apify API and Northern Brewer Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/northern-brewer-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/northern-brewer-scraper"}, "canadesk/spell-checker": {"id": 1954, "url": "https://apify.com/canadesk/spell-checker/api/client/nodejs", "title": "Apify API and Spell Checker interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"text\": \"I don't like this cats!\\nDoes you?\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/spell-checker\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"text": "I don't like this cats!\nDoes you?", "proxy": {"useApifyProxy": true}}, "actor_id": "canadesk/spell-checker"}, "useful-tools/dynamic-input": {"id": 1848, "url": "https://apify.com/useful-tools/dynamic-input/api/client/nodejs", "title": "Apify API and Dynamic Input interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"input\": \"const waitTime = Math.ceil(5 + Math.random() * 10);\\n\\nreturn {\\n    minWaitTime: waitTime,\\n    maxWaitTime: waitTime + 2_000\\n}\",\n    \"targetId\": \"useful-tools/wait-and-finish\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"useful-tools/dynamic-input\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"input": "const waitTime = Math.ceil(5 + Math.random() * 10);\n\nreturn {\n    minWaitTime: waitTime,\n    maxWaitTime: waitTime + 2_000\n}", "targetId": "useful-tools/wait-and-finish"}, "actor_id": "useful-tools/dynamic-input"}, "useful-tools/storage-cleaner": {"id": 1849, "url": "https://apify.com/useful-tools/storage-cleaner/api/client/nodejs", "title": "Apify API and Storage Cleaner interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"deleteAfterDays\": 90,\n    \"storageType\": \"ALL\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"useful-tools/storage-cleaner\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"storageType": "ALL", "deleteAfterDays": 90}, "actor_id": "useful-tools/storage-cleaner"}, "mshopik/republic-wireless-scraper": {"id": 1850, "url": "https://apify.com/mshopik/republic-wireless-scraper/api/client/nodejs", "title": "Apify API and Republic Wireless Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/republic-wireless-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/republic-wireless-scraper"}, "strajk/r2-bike-r2-bike-com-scraper": {"id": 1851, "url": "https://apify.com/strajk/r2-bike-r2-bike-com-scraper/api/client/nodejs", "title": "Apify API and r2-bike (r2-bike.com) scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"TEST\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"strajk/r2-bike-r2-bike-com-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "TEST"}, "actor_id": "strajk/r2-bike-r2-bike-com-scraper"}, "zuzka/t-online-scraper": {"id": 1852, "url": "https://apify.com/zuzka/t-online-scraper/api/client/nodejs", "title": "Apify API and T-online Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.t-online.de/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/t-online-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.t-online.de/"}], "maxArticlesPerCrawl": 100}, "actor_id": "zuzka/t-online-scraper"}, "canadesk/spyfu-bulk-urls": {"id": 1853, "url": "https://apify.com/canadesk/spyfu-bulk-urls/api/client/nodejs", "title": "Apify API and Spyfu (Bulk URLs) interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.apify.com\",\n        \"https://www.expensify.com\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/spyfu-bulk-urls\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": ["https://www.apify.com", "https://www.expensify.com"]}, "actor_id": "canadesk/spyfu-bulk-urls"}, "mshopik/colourpop-scraper": {"id": 1854, "url": "https://apify.com/mshopik/colourpop-scraper/api/client/nodejs", "title": "Apify API and ColourPop Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/colourpop-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/colourpop-scraper"}, "autofacts/lululemon-scraper": {"id": 1855, "url": "https://apify.com/autofacts/lululemon-scraper/api/client/nodejs", "title": "Apify API and Lululemon Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://shop.lululemon.com/c/mens-jackets-and-outerwear/_/N-8rm\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    },\n    \"maxConcurrency\": 5\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"autofacts/lululemon-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "startUrls": [{"url": "https://shop.lululemon.com/c/mens-jackets-and-outerwear/_/N-8rm"}], "maxConcurrency": 5}, "actor_id": "autofacts/lululemon-scraper"}, "mshopik/organifi-scraper": {"id": 1856, "url": "https://apify.com/mshopik/organifi-scraper/api/client/nodejs", "title": "Apify API and Organifi Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/organifi-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/organifi-scraper"}, "mshopik/wwwvaporcom-scraper": {"id": 1857, "url": "https://apify.com/mshopik/wwwvaporcom-scraper/api/client/nodejs", "title": "Apify API and www.vapor.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/wwwvaporcom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/wwwvaporcom-scraper"}, "wraythezw/zw-rates-scraper": {"id": 1858, "url": "https://apify.com/wraythezw/zw-rates-scraper/api/client/nodejs", "title": "Apify API and Zimbabwe Rates Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": [\n        {\n            \"url\": \"https://apify.com\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"wraythezw/zw-rates-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"start_urls": [{"url": "https://apify.com"}]}, "actor_id": "wraythezw/zw-rates-scraper"}, "jupri/priceline-hotel-scraper": {"id": 1859, "url": "https://apify.com/jupri/priceline-hotel-scraper/api/client/nodejs", "title": "Apify API and Priceline Hotel Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/priceline-hotel-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jupri/priceline-hotel-scraper"}, "fayzul_sam/e-commerce-scraper": {"id": 1860, "url": "https://apify.com/fayzul_sam/e-commerce-scraper/api/client/nodejs", "title": "Apify API and E-Commerce Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"product_urls\": [\n        {\n            \"url\": \"https://www.amazon.com/OtterBox-SYMMETRY-Antimicrobial-MagSafe-iPhone/dp/B09LP9Q8FB/?_encoding=UTF8&_ref=dlx_gate_sd_dcl_tlt_703c82f9_dt&pd_rd_w=7vVIe&content-id=amzn1.sym.108e0d4d-c023-4aaa-8b8a-69f88c793aa4&pf_rd_p=108e0d4d-c023-4aaa-8b8a-69f88c793aa4&pf_rd_r=83BV0CJXVV7MP32AA3KP&pd_rd_wg=JPVPw&pd_rd_r=79e14a26-8b55-4081-acd4-3f2556c5e781&ref_=pd_gw_unk\"\n        }\n    ],\n    \"proxySettings\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"fayzul_sam/e-commerce-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"product_urls": [{"url": "https://www.amazon.com/OtterBox-SYMMETRY-Antimicrobial-MagSafe-iPhone/dp/B09LP9Q8FB/?_encoding=UTF8&_ref=dlx_gate_sd_dcl_tlt_703c82f9_dt&pd_rd_w=7vVIe&content-id=amzn1.sym.108e0d4d-c023-4aaa-8b8a-69f88c793aa4&pf_rd_p=108e0d4d-c023-4aaa-8b8a-69f88c793aa4&pf_rd_r=83BV0CJXVV7MP32AA3KP&pd_rd_wg=JPVPw&pd_rd_r=79e14a26-8b55-4081-acd4-3f2556c5e781&ref_=pd_gw_unk"}], "proxySettings": {"useApifyProxy": true}}, "actor_id": "fayzul_sam/e-commerce-scraper"}, "flexible_cypress/medium-stats-scraper": {"id": 1957, "url": "https://apify.com/flexible_cypress/medium-stats-scraper/api/client/nodejs", "title": "Apify API and Medium Earnings Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"flexible_cypress/medium-stats-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "flexible_cypress/medium-stats-scraper"}, "zuzka/tableau-refresher": {"id": 1861, "url": "https://apify.com/zuzka/tableau-refresher/api/client/nodejs", "title": "Apify API and Tableau Dashboard Refresher interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://public.tableau.com/app/profile/pablolgomez/viz/HappinessintheWorld_16245341657220/WHR\",\n    \"email\": \"my.email@example.com\",\n    \"password\": \"my_password_to_tableau\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/tableau-refresher\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://public.tableau.com/app/profile/pablolgomez/viz/HappinessintheWorld_16245341657220/WHR", "email": "my.email@example.com", "password": "my_password_to_tableau"}, "actor_id": "zuzka/tableau-refresher"}, "mshopik/fangamer-scraper": {"id": 1862, "url": "https://apify.com/mshopik/fangamer-scraper/api/client/nodejs", "title": "Apify API and Fangamer Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/fangamer-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/fangamer-scraper"}, "mshopik/la-colombe-coffee-roasters-scraper": {"id": 1863, "url": "https://apify.com/mshopik/la-colombe-coffee-roasters-scraper/api/client/nodejs", "title": "Apify API and La Colombe Coffee Roasters Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/la-colombe-coffee-roasters-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/la-colombe-coffee-roasters-scraper"}, "canadesk/trustradius": {"id": 1864, "url": "https://apify.com/canadesk/trustradius/api/client/nodejs", "title": "Apify API and TrustRadius interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"website\": \"Apify\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/trustradius\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "website": "Apify"}, "actor_id": "canadesk/trustradius"}, "saswave/avis-verifier": {"id": 1865, "url": "https://apify.com/saswave/avis-verifier/api/client/nodejs", "title": "Apify API and Avis Verifier scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"domains\": [\n        \"www.google.com\",\n        \"www.youtube.com\"\n    ],\n    \"proxies\": \"{'http': 'http://proxy.example.com:8080','https': 'http://secureproxy.example.com:8090'}\",\n    \"str_domains\": \"www.google.com,www.youtube.com,apple.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"saswave/avis-verifier\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"domains": ["www.google.com", "www.youtube.com"], "proxies": "{'http': 'http://proxy.example.com:8080','https': 'http://secureproxy.example.com:8090'}", "str_domains": "www.google.com,www.youtube.com,apple.com"}, "actor_id": "saswave/avis-verifier"}, "mshopik/az-swagg-sauce-scraper": {"id": 1866, "url": "https://apify.com/mshopik/az-swagg-sauce-scraper/api/client/nodejs", "title": "Apify API and AZ Swagg Sauce Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/az-swagg-sauce-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/az-swagg-sauce-scraper"}, "petr_cermak/executions-wait-finish": {"id": 1991, "url": "https://apify.com/petr_cermak/executions-wait-finish/api/client/nodejs", "title": "Apify API and Executions Wait Finish interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/executions-wait-finish\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petr_cermak/executions-wait-finish"}, "jupri/tripadvisor": {"id": 1867, "url": "https://apify.com/jupri/tripadvisor/api/client/nodejs", "title": "Apify API and TripAdvisor Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": `location/302122\n        location/6897800\n        location/13965515\n        location/301415\n        location/3844449`\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/tripadvisor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "location/302122\n        location/6897800\n        location/13965515\n        location/301415\n        location/3844449"}, "actor_id": "jupri/tripadvisor"}, "mtrunkatova/patch-scraper": {"id": 1868, "url": "https://apify.com/mtrunkatova/patch-scraper/api/client/nodejs", "title": "Apify API and Patch Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.patch.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mtrunkatova/patch-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.patch.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "mtrunkatova/patch-scraper"}, "michael_b/whoscored-scraper": {"id": 1869, "url": "https://apify.com/michael_b/whoscored-scraper/api/client/nodejs", "title": "Apify API and WhoScored Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"Leagues\": [\n        {\n            \"url\": \"https://www.whoscored.com/Regions/252/Tournaments/2/England-Premier-League\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"michael_b/whoscored-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"Leagues": [{"url": "https://www.whoscored.com/Regions/252/Tournaments/2/England-Premier-League"}]}, "actor_id": "michael_b/whoscored-scraper"}, "mshopik/perfect-snacks-scraper": {"id": 1870, "url": "https://apify.com/mshopik/perfect-snacks-scraper/api/client/nodejs", "title": "Apify API and Perfect Snacks Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/perfect-snacks-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/perfect-snacks-scraper"}, "jirimoravcik/go-actor-example": {"id": 1871, "url": "https://apify.com/jirimoravcik/go-actor-example/api/client/nodejs", "title": "Apify API and Actor in Go example interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jirimoravcik/go-actor-example\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jirimoravcik/go-actor-example"}, "mshopik/merchology-scraper": {"id": 1872, "url": "https://apify.com/mshopik/merchology-scraper/api/client/nodejs", "title": "Apify API and Merchology Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/merchology-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/merchology-scraper"}, "epctex/dailymotion-video-downloader": {"id": 1873, "url": "https://apify.com/epctex/dailymotion-video-downloader/api/client/nodejs", "title": "Apify API and Dailymotion Video Downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.dailymotion.com/video/x13dhx3\"\n    ],\n    \"quality\": \"medium\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/dailymotion-video-downloader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "quality": "medium", "startUrls": ["https://www.dailymotion.com/video/x13dhx3"]}, "actor_id": "epctex/dailymotion-video-downloader"}, "natanielsantos/lowe-s-scraper": {"id": 1874, "url": "https://apify.com/natanielsantos/lowe-s-scraper/api/client/nodejs", "title": "Apify API and Lowes Web Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.lowes.com/search?searchTerm=refrigerator\"\n        }\n    ],\n    \"maxItems\": 50,\n    \"proxySettings\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"natanielsantos/lowe-s-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 50, "startUrls": [{"url": "https://www.lowes.com/search?searchTerm=refrigerator"}], "proxySettings": {"useApifyProxy": true}}, "actor_id": "natanielsantos/lowe-s-scraper"}, "mshopik/skims-scraper": {"id": 1875, "url": "https://apify.com/mshopik/skims-scraper/api/client/nodejs", "title": "Apify API and SKIMS Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/skims-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/skims-scraper"}, "vincario/vincario-vin-decoder-api-services": {"id": 1876, "url": "https://apify.com/vincario/vincario-vin-decoder-api-services/api/client/nodejs", "title": "Apify API and Vincario VIN Decoder API Services interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"vincario/vincario-vin-decoder-api-services\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "vincario/vincario-vin-decoder-api-services"}, "bossula/biletix-crawler": {"id": 1877, "url": "https://apify.com/bossula/biletix-crawler/api/client/nodejs", "title": "Apify API and Biletix Crawler interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bossula/biletix-crawler\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "bossula/biletix-crawler"}, "shazux/trustpilot-reviews-scraper": {"id": 1878, "url": "https://apify.com/shazux/trustpilot-reviews-scraper/api/client/nodejs", "title": "Apify API and Trustpilot Reviews Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"businessUrls\": [\n        {\n            \"url\": \"https://www.trustpilot.com/review/beschuetzerbox.de\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"shazux/trustpilot-reviews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "businessUrls": [{"url": "https://www.trustpilot.com/review/beschuetzerbox.de"}]}, "actor_id": "shazux/trustpilot-reviews-scraper"}, "adrian_horning/bell-county-tax-assessor-api": {"id": 1879, "url": "https://apify.com/adrian_horning/bell-county-tax-assessor-api/api/client/nodejs", "title": "Apify API and Bell County Tax Assessor API interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"streetNumber\": \"1219\",\n    \"streetName\": \"7\",\n    \"propertyId\": \"73512\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"adrian_horning/bell-county-tax-assessor-api\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"propertyId": "73512", "streetName": "7", "streetNumber": "1219"}, "actor_id": "adrian_horning/bell-county-tax-assessor-api"}, "nguyendan/startupjobs-to-jazzhr": {"id": 1880, "url": "https://apify.com/nguyendan/startupjobs-to-jazzhr/api/client/nodejs", "title": "Apify API and StartupJobs and JazzHR integration interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"nguyendan/startupjobs-to-jazzhr\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "nguyendan/startupjobs-to-jazzhr"}, "hanatsai/atlanta-black-star-scraper": {"id": 1887, "url": "https://apify.com/hanatsai/atlanta-black-star-scraper/api/client/nodejs", "title": "Apify API and Atlanta Black Star Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"http://atlantablackstar.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hanatsai/atlanta-black-star-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "http://atlantablackstar.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "hanatsai/atlanta-black-star-scraper"}, "mshopik/horne-scraper": {"id": 1881, "url": "https://apify.com/mshopik/horne-scraper/api/client/nodejs", "title": "Apify API and HORNE Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/horne-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/horne-scraper"}, "scrapingxpert/redfin-agent-reviews-scraper": {"id": 1882, "url": "https://apify.com/scrapingxpert/redfin-agent-reviews-scraper/api/client/nodejs", "title": "Apify API and Redfin agent reviews scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": [\n        {\n            \"url\": \"https://www.redfin.com/real-estate-agents/brentjon/reviews\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"scrapingxpert/redfin-agent-reviews-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"start_urls": [{"url": "https://www.redfin.com/real-estate-agents/brentjon/reviews"}]}, "actor_id": "scrapingxpert/redfin-agent-reviews-scraper"}, "hamza.alwan/gab-accounts-scraper": {"id": 1883, "url": "https://apify.com/hamza.alwan/gab-accounts-scraper/api/client/nodejs", "title": "Apify API and Gab Accounts Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [],\n    \"accounts\": [\n        \"RealCharlieTheCat\"\n    ],\n    \"desiredPostsCount\": 100,\n    \"desiredCommentsCount\": 0,\n    \"accountPostsSortBy\": \"NEWEST\",\n    \"commentsSortBy\": \"MOST_LIKED\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/gab-accounts-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"accounts": ["RealCharlieTheCat"], "startUrls": [], "commentsSortBy": "MOST_LIKED", "desiredPostsCount": 100, "accountPostsSortBy": "NEWEST", "desiredCommentsCount": 0}, "actor_id": "hamza.alwan/gab-accounts-scraper"}, "mshopik/gaiam-scraper": {"id": 1884, "url": "https://apify.com/mshopik/gaiam-scraper/api/client/nodejs", "title": "Apify API and Gaiam Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/gaiam-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/gaiam-scraper"}, "mshopik/onewheel-future-motion-scraper": {"id": 1885, "url": "https://apify.com/mshopik/onewheel-future-motion-scraper/api/client/nodejs", "title": "Apify API and Onewheel // Future Motion Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/onewheel-future-motion-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/onewheel-future-motion-scraper"}, "mshopik/brentwood-home-scraper": {"id": 1886, "url": "https://apify.com/mshopik/brentwood-home-scraper/api/client/nodejs", "title": "Apify API and Brentwood Home Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/brentwood-home-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/brentwood-home-scraper"}, "mshopik/topo-designs-scraper": {"id": 1888, "url": "https://apify.com/mshopik/topo-designs-scraper/api/client/nodejs", "title": "Apify API and Topo Designs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/topo-designs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/topo-designs-scraper"}, "mshopik/mcgee-and-co-scraper": {"id": 1889, "url": "https://apify.com/mshopik/mcgee-and-co-scraper/api/client/nodejs", "title": "Apify API and McGee Co. Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/mcgee-and-co-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/mcgee-and-co-scraper"}, "mshopik/kbdfanscom-scraper": {"id": 1890, "url": "https://apify.com/mshopik/kbdfanscom-scraper/api/client/nodejs", "title": "Apify API and kbdfans.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/kbdfanscom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/kbdfanscom-scraper"}, "mtrunkat/24-hour-stats": {"id": 1891, "url": "https://apify.com/mtrunkat/24-hour-stats/api/client/nodejs", "title": "Apify API and 24 Hour Stats interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mtrunkat/24-hour-stats\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "mtrunkat/24-hour-stats"}, "service-paradis/w3c-html-reporter": {"id": 1892, "url": "https://apify.com/service-paradis/w3c-html-reporter/api/client/nodejs", "title": "Apify API and W3C Html Reporter interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://apify.com\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"service-paradis/w3c-html-reporter\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": false}, "startUrls": [{"url": "https://apify.com"}]}, "actor_id": "service-paradis/w3c-html-reporter"}, "strajk/axit-axit-cz-scraper": {"id": 1893, "url": "https://apify.com/strajk/axit-axit-cz-scraper/api/client/nodejs", "title": "Apify API and Axit (axit.cz) scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"TEST\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"strajk/axit-axit-cz-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "TEST"}, "actor_id": "strajk/axit-axit-cz-scraper"}, "canadesk/cms-checker": {"id": 1894, "url": "https://apify.com/canadesk/cms-checker/api/client/nodejs", "title": "Apify API and CMS Checker interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"apify.com\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/cms-checker\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "apify.com", "proxy": {"useApifyProxy": true}}, "actor_id": "canadesk/cms-checker"}, "mshopik/luxe-bedding-bed-threads-scraper": {"id": 1896, "url": "https://apify.com/mshopik/luxe-bedding-bed-threads-scraper/api/client/nodejs", "title": "Apify API and Luxe Bedding\u2013 Bed Threads Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/luxe-bedding-bed-threads-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/luxe-bedding-bed-threads-scraper"}, "datastorm/weaviate-integration": {"id": 1897, "url": "https://apify.com/datastorm/weaviate-integration/api/client/nodejs", "title": "Apify API and Weaviate Integration interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"weaviateClass\": \"XXX\",\n    \"weaviateHost\": \"myindex.weaviate.network\",\n    \"fieldMappings\": [\n        {\n            \"datasetColumnName\": \"url\",\n            \"dbPropertyName\": \"key\",\n            \"isKeyColumn\": true,\n            \"isMetadata\": true,\n            \"dataType\": [\n                \"text\"\n            ]\n        },\n        {\n            \"datasetColumnName\": \"pageTitle\",\n            \"dbPropertyName\": \"pageTitle\",\n            \"isKeyColumn\": false,\n            \"isMetadata\": false,\n            \"dataType\": [\n                \"text\"\n            ]\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"datastorm/weaviate-integration\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"weaviateHost": "myindex.weaviate.network", "fieldMappings": [{"dataType": ["text"], "isMetadata": true, "isKeyColumn": true, "dbPropertyName": "key", "datasetColumnName": "url"}, {"dataType": ["text"], "isMetadata": false, "isKeyColumn": false, "dbPropertyName": "pageTitle", "datasetColumnName": "pageTitle"}], "weaviateClass": "XXX"}, "actor_id": "datastorm/weaviate-integration"}, "mshopik/itech-deals-scraper": {"id": 1898, "url": "https://apify.com/mshopik/itech-deals-scraper/api/client/nodejs", "title": "Apify API and iTech Deals Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/itech-deals-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/itech-deals-scraper"}, "mshopik/milk-bar-scraper": {"id": 1899, "url": "https://apify.com/mshopik/milk-bar-scraper/api/client/nodejs", "title": "Apify API and Milk Bar Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/milk-bar-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/milk-bar-scraper"}, "mvolfik/azure-dataset-uploader": {"id": 1900, "url": "https://apify.com/mvolfik/azure-dataset-uploader/api/client/nodejs", "title": "Apify API and Azure dataset uploader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mvolfik/azure-dataset-uploader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "mvolfik/azure-dataset-uploader"}, "lhotanok/fler-scraper": {"id": 1901, "url": "https://apify.com/lhotanok/fler-scraper/api/client/nodejs", "title": "Apify API and Fler Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"sellerUrls\": [\n        \"https://www.fler.cz/shop/findings\"\n    ],\n    \"maxItems\": 20,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lhotanok/fler-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 20, "sellerUrls": ["https://www.fler.cz/shop/findings"], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "lhotanok/fler-scraper"}, "mshopik/dr-squatch-scraper": {"id": 1902, "url": "https://apify.com/mshopik/dr-squatch-scraper/api/client/nodejs", "title": "Apify API and Dr. Squatch Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/dr-squatch-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/dr-squatch-scraper"}, "mshopik/the-pros-closet-scraper": {"id": 1903, "url": "https://apify.com/mshopik/the-pros-closet-scraper/api/client/nodejs", "title": "Apify API and The Pro's Closet Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-pros-closet-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/the-pros-closet-scraper"}, "mshopik/huel-scraper": {"id": 1904, "url": "https://apify.com/mshopik/huel-scraper/api/client/nodejs", "title": "Apify API and Huel Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/huel-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/huel-scraper"}, "mshopik/rocketbook-scraper": {"id": 1905, "url": "https://apify.com/mshopik/rocketbook-scraper/api/client/nodejs", "title": "Apify API and Rocketbook Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/rocketbook-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/rocketbook-scraper"}, "mshopik/sapphire-online-scraper": {"id": 1906, "url": "https://apify.com/mshopik/sapphire-online-scraper/api/client/nodejs", "title": "Apify API and Sapphire Online Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/sapphire-online-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/sapphire-online-scraper"}, "binhbui/rotten-extractor": {"id": 1907, "url": "https://apify.com/binhbui/rotten-extractor/api/client/nodejs", "title": "Apify API and Rotten Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.rottentomatoes.com/browse/movies_in_theaters/ratings:nc_17,pg,pg_13,r~sort:popular\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    },\n    \"maxItems\": 1000,\n    \"extendOutputFunction\": ($, item) => { return {} }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"binhbui/rotten-extractor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 1000, "startUrls": [{"url": "https://www.rottentomatoes.com/browse/movies_in_theaters/ratings:nc_17,pg,pg_13,r~sort:popular"}], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "binhbui/rotten-extractor"}, "mshopik/collins-scraper": {"id": 1908, "url": "https://apify.com/mshopik/collins-scraper/api/client/nodejs", "title": "Apify API and Collins Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/collins-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/collins-scraper"}, "mshopik/enjoy-life-foods-scraper": {"id": 1909, "url": "https://apify.com/mshopik/enjoy-life-foods-scraper/api/client/nodejs", "title": "Apify API and Enjoy Life Foods Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/enjoy-life-foods-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/enjoy-life-foods-scraper"}, "mshopik/orgain-scraper": {"id": 1910, "url": "https://apify.com/mshopik/orgain-scraper/api/client/nodejs", "title": "Apify API and Orgain Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/orgain-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/orgain-scraper"}, "adrian_horning/dallas-county-tax-assessor-api": {"id": 1911, "url": "https://apify.com/adrian_horning/dallas-county-tax-assessor-api/api/client/nodejs", "title": "Apify API and Dallas County Tax Assessor API interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"streetNumber\": \"5706\",\n    \"streetName\": \"Boca Raton Dr\",\n    \"accountId\": \"00000577729000000\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"adrian_horning/dallas-county-tax-assessor-api\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"accountId": "00000577729000000", "streetName": "Boca Raton Dr", "streetNumber": "5706"}, "actor_id": "adrian_horning/dallas-county-tax-assessor-api"}, "canadesk/apple-app-store": {"id": 1912, "url": "https://apify.com/canadesk/apple-app-store/api/client/nodejs", "title": "Apify API and Apple App Store interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"lang\": \"en\",\n    \"country\": \"us\",\n    \"appId\": \"1514844618\",\n    \"developerId\": \"Apple\",\n    \"keyword\": \"translate\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/apple-app-store\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"lang": "en", "appId": "1514844618", "proxy": {"useApifyProxy": true}, "country": "us", "keyword": "translate", "developerId": "Apple"}, "actor_id": "canadesk/apple-app-store"}, "scrabber/beagle-boys": {"id": 1913, "url": "https://apify.com/scrabber/beagle-boys/api/client/nodejs", "title": "Apify API and Beagle Boys (Skroutz Scraper) interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"urls\": [\n        \"https://www.skroutz.gr/s/40208718/Lenovo-V15-G2-ALC-15-6-FHD-Ryzen-3-5300U-8GB-256GB-SSD-W11-Home-Black-US-Keyboard.html\",\n        \"https://www.skroutz.gr/s/38571219/Lenovo-IdeaPad-3-17IAU7-17-3-IPS-FHD-i5-1235U-8GB-512GB-SSD-W11-S-Arctic-Grey-GR-Keyboard.html\",\n        \"https://www.skroutz.gr/s/36923496/Huawei-MateBook-D16-16-IPS-FHD-i7-12700H-16GB-512GB-SSD-W11-Home-US-Keyboard.html\",\n        \"https://www.skroutz.gr/s/37725758/Lenovo-IdeaPad-3-15ALC6-15-6-IPS-FHD-Ryzen-5-5500U-8GB-512GB-SSD-W11-Home-Arctic-Grey-US-Keyboard.html\",\n        \"https://www.skroutz.gr/s/37214807/HP-Victus-15-fb0007nv-15-6-FHD-Ryzen-5-5600H-8GB-512GB-SSD-GeForce-GTX-1650-W11-Home-GR-Keyboard.html\",\n        \"https://www.skroutz.gr/s/42302804/Asus-Vivobook-Go-15-E1504FA-BQ521W-15-6-FHD-Ryzen-5-7520U-16GB-512GB-SSD-W11-Home-Green-Grey-GR-Keyboard.html\",\n        \"https://www.skroutz.gr/s/38735377/Lenovo-IdeaPad-3-15IAU7-15-6-IPS-FHD-i7-1255U-16GB-512GB-SSD-W11-S-Arctic-Grey-GR-Keyboard.html\",\n        \"https://www.skroutz.gr/s/39217438/Lenovo-IdeaPad-3-15ITL6-15-6-FHD-i3-1115G4-12GB-512GB-SSD-No-OS-Arctic-Grey-US-Keyboard.html\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"scrabber/beagle-boys\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"urls": ["https://www.skroutz.gr/s/40208718/Lenovo-V15-G2-ALC-15-6-FHD-Ryzen-3-5300U-8GB-256GB-SSD-W11-Home-Black-US-Keyboard.html", "https://www.skroutz.gr/s/38571219/Lenovo-IdeaPad-3-17IAU7-17-3-IPS-FHD-i5-1235U-8GB-512GB-SSD-W11-S-Arctic-Grey-GR-Keyboard.html", "https://www.skroutz.gr/s/36923496/Huawei-MateBook-D16-16-IPS-FHD-i7-12700H-16GB-512GB-SSD-W11-Home-US-Keyboard.html", "https://www.skroutz.gr/s/37725758/Lenovo-IdeaPad-3-15ALC6-15-6-IPS-FHD-Ryzen-5-5500U-8GB-512GB-SSD-W11-Home-Arctic-Grey-US-Keyboard.html", "https://www.skroutz.gr/s/37214807/HP-Victus-15-fb0007nv-15-6-FHD-Ryzen-5-5600H-8GB-512GB-SSD-GeForce-GTX-1650-W11-Home-GR-Keyboard.html", "https://www.skroutz.gr/s/42302804/Asus-Vivobook-Go-15-E1504FA-BQ521W-15-6-FHD-Ryzen-5-7520U-16GB-512GB-SSD-W11-Home-Green-Grey-GR-Keyboard.html", "https://www.skroutz.gr/s/38735377/Lenovo-IdeaPad-3-15IAU7-15-6-IPS-FHD-i7-1255U-16GB-512GB-SSD-W11-S-Arctic-Grey-GR-Keyboard.html", "https://www.skroutz.gr/s/39217438/Lenovo-IdeaPad-3-15ITL6-15-6-FHD-i3-1115G4-12GB-512GB-SSD-No-OS-Arctic-Grey-US-Keyboard.html"]}, "actor_id": "scrabber/beagle-boys"}, "lhotanok/zoot-scraper": {"id": 1914, "url": "https://apify.com/lhotanok/zoot-scraper/api/client/nodejs", "title": "Apify API and Zoot Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://www.zoot.cz/katalog/17504/zeny\",\n        \"https://www.zoot.cz/katalog/17572/muzi\",\n        \"https://www.zoot.cz/vse-pro-deti\"\n    ],\n    \"maxItems\": 100,\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lhotanok/zoot-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxItems": 100, "startUrls": ["https://www.zoot.cz/katalog/17504/zeny", "https://www.zoot.cz/katalog/17572/muzi", "https://www.zoot.cz/vse-pro-deti"], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "lhotanok/zoot-scraper"}, "mscraper/creativemarket-scraper": {"id": 1915, "url": "https://apify.com/mscraper/creativemarket-scraper/api/client/nodejs", "title": "Apify API and CreativeMarket Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://creativemarket.com/brushes-more/palettes\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"groups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/creativemarket-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"groups": ["RESIDENTIAL"], "useApifyProxy": true, "apifyProxyCountry": "US"}, "startUrls": [{"url": "https://creativemarket.com/brushes-more/palettes"}]}, "actor_id": "mscraper/creativemarket-scraper"}, "petrpatek/get-debug-items-from-dataset": {"id": 1916, "url": "https://apify.com/petrpatek/get-debug-items-from-dataset/api/client/nodejs", "title": "Apify API and Error Messages Deduplication interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petrpatek/get-debug-items-from-dataset\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petrpatek/get-debug-items-from-dataset"}, "mshopik/hint-inc-scraper": {"id": 1917, "url": "https://apify.com/mshopik/hint-inc-scraper/api/client/nodejs", "title": "Apify API and Hint Inc. Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/hint-inc-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/hint-inc-scraper"}, "lukaskrivka/rust-actor-example": {"id": 1918, "url": "https://apify.com/lukaskrivka/rust-actor-example/api/client/nodejs", "title": "Apify API and Actor in Rust Example interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://apify.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lukaskrivka/rust-actor-example\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://apify.com"}, "actor_id": "lukaskrivka/rust-actor-example"}, "mshopik/ufc-store-scraper": {"id": 1919, "url": "https://apify.com/mshopik/ufc-store-scraper/api/client/nodejs", "title": "Apify API and UFC Store Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/ufc-store-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/ufc-store-scraper"}, "mshopik/secretlab-scraper": {"id": 1920, "url": "https://apify.com/mshopik/secretlab-scraper/api/client/nodejs", "title": "Apify API and Secretlab Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/secretlab-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/secretlab-scraper"}, "mshopik/vinyl-me-please-scraper": {"id": 1921, "url": "https://apify.com/mshopik/vinyl-me-please-scraper/api/client/nodejs", "title": "Apify API and Vinyl Me, Please Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/vinyl-me-please-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/vinyl-me-please-scraper"}, "mshopik/sunwarrior-scraper": {"id": 1922, "url": "https://apify.com/mshopik/sunwarrior-scraper/api/client/nodejs", "title": "Apify API and Sunwarrior Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/sunwarrior-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/sunwarrior-scraper"}, "mshopik/petdoorscom-scraper": {"id": 1923, "url": "https://apify.com/mshopik/petdoorscom-scraper/api/client/nodejs", "title": "Apify API and PetDoors.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/petdoorscom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/petdoorscom-scraper"}, "strajk/cesky-raj-ceskyraj-com-scraper": {"id": 1924, "url": "https://apify.com/strajk/cesky-raj-ceskyraj-com-scraper/api/client/nodejs", "title": "Apify API and \u010cesk\u00fd r\u00e1j (ceskyraj.com) scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"TEST\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"strajk/cesky-raj-ceskyraj-com-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "TEST"}, "actor_id": "strajk/cesky-raj-ceskyraj-com-scraper"}, "david.lukac/le-figaro-scraper": {"id": 1925, "url": "https://apify.com/david.lukac/le-figaro-scraper/api/client/nodejs", "title": "Apify API and Le Figaro Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.lefigaro.fr/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"david.lukac/le-figaro-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.lefigaro.fr/"}], "maxArticlesPerCrawl": 100}, "actor_id": "david.lukac/le-figaro-scraper"}, "dtziugomez/covidyucatan": {"id": 1926, "url": "https://apify.com/dtziugomez/covidyucatan/api/client/nodejs", "title": "Apify API and Covid Yucatan interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"dtziugomez/covidyucatan\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "dtziugomez/covidyucatan"}, "lhotanok/erasmus-plus-organisation-scraper": {"id": 1927, "url": "https://apify.com/lhotanok/erasmus-plus-organisation-scraper/api/client/nodejs", "title": "Apify API and Erasmus+ Organisation Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"countryCode\": \"BE\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lhotanok/erasmus-plus-organisation-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"countryCode": "BE", "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "lhotanok/erasmus-plus-organisation-scraper"}, "mshopik/soylent-scraper": {"id": 1929, "url": "https://apify.com/mshopik/soylent-scraper/api/client/nodejs", "title": "Apify API and Soylent Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/soylent-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/soylent-scraper"}, "maria.f/cbc-scraper": {"id": 1930, "url": "https://apify.com/maria.f/cbc-scraper/api/client/nodejs", "title": "Apify API and CBC Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.cbc.ca/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"maria.f/cbc-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.cbc.ca/"}], "maxArticlesPerCrawl": 100}, "actor_id": "maria.f/cbc-scraper"}, "fayzul_sam/capterra-scraper": {"id": 1931, "url": "https://apify.com/fayzul_sam/capterra-scraper/api/client/nodejs", "title": "Apify API and Capterra Software Data Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"CategoryURL\": [\n        {\n            \"url\": \"https://www.capterra.com/360-degree-feedback-software/\"\n        }\n    ],\n    \"proxySettings\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"fayzul_sam/capterra-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"CategoryURL": [{"url": "https://www.capterra.com/360-degree-feedback-software/"}], "proxySettings": {"useApifyProxy": true}}, "actor_id": "fayzul_sam/capterra-scraper"}, "mshopik/310-nutrition-scraper": {"id": 1932, "url": "https://apify.com/mshopik/310-nutrition-scraper/api/client/nodejs", "title": "Apify API and 310 Nutrition Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/310-nutrition-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/310-nutrition-scraper"}, "florcorvalan/boletin-oficial": {"id": 1933, "url": "https://apify.com/florcorvalan/boletin-oficial/api/client/nodejs", "title": "Apify API and Bolet\u00edn Oficial interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"fechaDesdeInput\": \"\",\n    \"fechaHastaInput\": \"\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"florcorvalan/boletin-oficial\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"fechaDesdeInput": "", "fechaHastaInput": ""}, "actor_id": "florcorvalan/boletin-oficial"}, "strajk/bergfreunde-bergfreunde-eu-scraper": {"id": 1934, "url": "https://apify.com/strajk/bergfreunde-bergfreunde-eu-scraper/api/client/nodejs", "title": "Apify API and Bergfreunde (bergfreunde.eu) scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"mode\": \"TEST\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"strajk/bergfreunde-bergfreunde-eu-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"mode": "TEST"}, "actor_id": "strajk/bergfreunde-bergfreunde-eu-scraper"}, "hanatsai/the-hill-scraper": {"id": 1935, "url": "https://apify.com/hanatsai/the-hill-scraper/api/client/nodejs", "title": "Apify API and The Hill Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"http://thehill.com\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hanatsai/the-hill-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "http://thehill.com"}], "maxArticlesPerCrawl": 100}, "actor_id": "hanatsai/the-hill-scraper"}, "mshopik/salt-and-straw-scraper": {"id": 1937, "url": "https://apify.com/mshopik/salt-and-straw-scraper/api/client/nodejs", "title": "Apify API and Salt Straw Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/salt-and-straw-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/salt-and-straw-scraper"}, "mshopik/evil-bikes-usa-scraper": {"id": 1938, "url": "https://apify.com/mshopik/evil-bikes-usa-scraper/api/client/nodejs", "title": "Apify API and Evil Bikes USA Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/evil-bikes-usa-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/evil-bikes-usa-scraper"}, "lhotanok/github-issues-to-slack": {"id": 1939, "url": "https://apify.com/lhotanok/github-issues-to-slack/api/client/nodejs", "title": "Apify API and Sends Slack notification about newly created / closed issues interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"repositories\": [\n        \"apifytech/apify-js\"\n    ],\n    \"channel\": \"#general\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lhotanok/github-issues-to-slack\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"channel": "#general", "repositories": ["apifytech/apify-js"]}, "actor_id": "lhotanok/github-issues-to-slack"}, "mshopik/wwwbricksworldcom-scraper": {"id": 1940, "url": "https://apify.com/mshopik/wwwbricksworldcom-scraper/api/client/nodejs", "title": "Apify API and www.bricksworld.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/wwwbricksworldcom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/wwwbricksworldcom-scraper"}, "mshopik/iandi-sports-supply-co-scraper": {"id": 1941, "url": "https://apify.com/mshopik/iandi-sports-supply-co-scraper/api/client/nodejs", "title": "Apify API and I&I Sports Supply Co., Inc. Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/iandi-sports-supply-co-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/iandi-sports-supply-co-scraper"}, "jindrich.bar/free-flight-ticket-scraper": {"id": 1942, "url": "https://apify.com/jindrich.bar/free-flight-ticket-scraper/api/client/nodejs", "title": "Apify API and Free Flight Ticket Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"fromIATAs\": [\n        \"PRG\"\n    ],\n    \"toIATAs\": [\n        \"MUC\"\n    ],\n    \"currency\": \"USD\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jindrich.bar/free-flight-ticket-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"toIATAs": ["MUC"], "currency": "USD", "fromIATAs": ["PRG"]}, "actor_id": "jindrich.bar/free-flight-ticket-scraper"}, "danielwebr/firebase-firestore-import": {"id": 1992, "url": "https://apify.com/danielwebr/firebase-firestore-import/api/client/nodejs", "title": "Apify API and Firebase Firestore Import interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"danielwebr/firebase-firestore-import\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "danielwebr/firebase-firestore-import"}, "mshopik/sister-jane-japan-scraper": {"id": 1943, "url": "https://apify.com/mshopik/sister-jane-japan-scraper/api/client/nodejs", "title": "Apify API and Sister Jane Japan Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/sister-jane-japan-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/sister-jane-japan-scraper"}, "mshopik/hanacure-scraper": {"id": 1944, "url": "https://apify.com/mshopik/hanacure-scraper/api/client/nodejs", "title": "Apify API and Hanacure Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/hanacure-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/hanacure-scraper"}, "mshopik/sounds-true-scraper": {"id": 1945, "url": "https://apify.com/mshopik/sounds-true-scraper/api/client/nodejs", "title": "Apify API and Sounds True Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/sounds-true-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/sounds-true-scraper"}, "mshopik/passion-planner-scraper": {"id": 1946, "url": "https://apify.com/mshopik/passion-planner-scraper/api/client/nodejs", "title": "Apify API and Passion Planner Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/passion-planner-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/passion-planner-scraper"}, "mshopik/primally-pure-scraper": {"id": 1947, "url": "https://apify.com/mshopik/primally-pure-scraper/api/client/nodejs", "title": "Apify API and Primally Pure Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/primally-pure-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/primally-pure-scraper"}, "mshopik/luxy-hair-scraper": {"id": 1955, "url": "https://apify.com/mshopik/luxy-hair-scraper/api/client/nodejs", "title": "Apify API and Luxy Hair Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/luxy-hair-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/luxy-hair-scraper"}, "useful-tools/batch-runner": {"id": 1948, "url": "https://apify.com/useful-tools/batch-runner/api/client/nodejs", "title": "Apify API and Batch Runner interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"batch\": [\n        {\n            \"actorId\": \"useful-tools/wait-and-finish\",\n            \"input\": {\n                \"minWaitTime\": 30,\n                \"dataset\": {\n                    \"items\": [\n                        {\n                            \"key\": \"value1\"\n                        }\n                    ]\n                }\n            }\n        },\n        {\n            \"actorId\": \"useful-tools/wait-and-finish\",\n            \"input\": {\n                \"dataset\": {\n                    \"items\": [\n                        {\n                            \"key\": \"value1\"\n                        },\n                        {\n                            \"key\": \"value2\"\n                        }\n                    ]\n                }\n            }\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"useful-tools/batch-runner\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"batch": [{"input": {"dataset": {"items": [{"key": "value1"}]}, "minWaitTime": 30}, "actorId": "useful-tools/wait-and-finish"}, {"input": {"dataset": {"items": [{"key": "value1"}, {"key": "value2"}]}}, "actorId": "useful-tools/wait-and-finish"}]}, "actor_id": "useful-tools/batch-runner"}, "mshopik/lovevery-scraper": {"id": 1949, "url": "https://apify.com/mshopik/lovevery-scraper/api/client/nodejs", "title": "Apify API and Lovevery Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/lovevery-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/lovevery-scraper"}, "mshopik/lovepop-scraper": {"id": 1950, "url": "https://apify.com/mshopik/lovepop-scraper/api/client/nodejs", "title": "Apify API and Lovepop Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/lovepop-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/lovepop-scraper"}, "zuzka/ctv-news-scraper": {"id": 1951, "url": "https://apify.com/zuzka/ctv-news-scraper/api/client/nodejs", "title": "Apify API and CTV News Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.ctvnews.ca/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"zuzka/ctv-news-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.ctvnews.ca/"}], "maxArticlesPerCrawl": 100}, "actor_id": "zuzka/ctv-news-scraper"}, "mshopik/real-watersports-scraper": {"id": 1952, "url": "https://apify.com/mshopik/real-watersports-scraper/api/client/nodejs", "title": "Apify API and REAL Watersports Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/real-watersports-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/real-watersports-scraper"}, "iharivijay/livabl-scraper": {"id": 1953, "url": "https://apify.com/iharivijay/livabl-scraper/api/client/nodejs", "title": "Apify API and Livabl Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.livabl.com/new-york-ny/new-homes\"\n        }\n    ],\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"iharivijay/livabl-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.livabl.com/new-york-ny/new-homes"}], "proxyConfiguration": {"useApifyProxy": true}}, "actor_id": "iharivijay/livabl-scraper"}, "jupri/html-reader-mode": {"id": 1958, "url": "https://apify.com/jupri/html-reader-mode/api/client/nodejs", "title": "Apify API and HTML Reader Mode interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": [\n        \"https://crawlee.dev/docs/introduction\",\n        \"https://en.wikipedia.org/wiki/Python_(programming_language)\",\n        \"https://www.makeuseof.com/how-to-use-reader-view-in-safari/\",\n        \"https://www.nytimes.com/2023/10/06/us/berkeley-indigenous-peoples-day.html\"\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/html-reader-mode\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": ["https://crawlee.dev/docs/introduction", "https://en.wikipedia.org/wiki/Python_(programming_language)", "https://www.makeuseof.com/how-to-use-reader-view-in-safari/", "https://www.nytimes.com/2023/10/06/us/berkeley-indigenous-peoples-day.html"]}, "actor_id": "jupri/html-reader-mode"}, "bares43/kaktus-dobijecka": {"id": 1959, "url": "https://apify.com/bares43/kaktus-dobijecka/api/client/nodejs", "title": "Apify API and Kaktus dob\u00edje\u010dka interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"email\": [\n        {\n            \"to\": \"your@email.com\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"bares43/kaktus-dobijecka\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"email": [{"to": "your@email.com"}]}, "actor_id": "bares43/kaktus-dobijecka"}, "gabrielm/binance-nft-collection-scraper": {"id": 1960, "url": "https://apify.com/gabrielm/binance-nft-collection-scraper/api/client/nodejs", "title": "Apify API and Binance NFT Collection Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"adv_proxy_usage\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"gabrielm/binance-nft-collection-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"adv_proxy_usage": {"useApifyProxy": true}}, "actor_id": "gabrielm/binance-nft-collection-scraper"}, "mshopik/morphe-uk-scraper": {"id": 1961, "url": "https://apify.com/mshopik/morphe-uk-scraper/api/client/nodejs", "title": "Apify API and Morphe UK Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/morphe-uk-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/morphe-uk-scraper"}, "mshopik/hulkapps-scraper": {"id": 1962, "url": "https://apify.com/mshopik/hulkapps-scraper/api/client/nodejs", "title": "Apify API and HulkApps Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/hulkapps-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/hulkapps-scraper"}, "inquisitive_sarangi/autoscout24-com-scraper": {"id": 1963, "url": "https://apify.com/inquisitive_sarangi/autoscout24-com-scraper/api/client/nodejs", "title": "Apify API and Autoscout24.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchUrls\": [\n        \"https://www.autoscout24.com/lst/bmw?atype=C&cy=D%2CA%2CB%2CE%2CF%2CI%2CL%2CNL&damaged_listing=exclude&desc=0&powertype=kw&search_id=2e6emxmcu9b&sort=standard&source=homepage_search-mask&ustate=N%2CU\"\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [\n            \"RESIDENTIAL\"\n        ],\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"maxPage\": 1\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"inquisitive_sarangi/autoscout24-com-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyGroups": ["RESIDENTIAL"], "apifyProxyCountry": "US"}, "maxPage": 1, "searchUrls": ["https://www.autoscout24.com/lst/bmw?atype=C&cy=D%2CA%2CB%2CE%2CF%2CI%2CL%2CNL&damaged_listing=exclude&desc=0&powertype=kw&search_id=2e6emxmcu9b&sort=standard&source=homepage_search-mask&ustate=N%2CU"]}, "actor_id": "inquisitive_sarangi/autoscout24-com-scraper"}, "equidem/ai-price-extractor": {"id": 1964, "url": "https://apify.com/equidem/ai-price-extractor/api/client/nodejs", "title": "Apify API and AI Price Extractor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"proxyConfig\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyGroups\": [],\n        \"apifyProxyCountry\": \"DE\"\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"equidem/ai-price-extractor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxyConfig": {"useApifyProxy": true, "apifyProxyGroups": [], "apifyProxyCountry": "DE"}}, "actor_id": "equidem/ai-price-extractor"}, "petr_cermak/execution-to-knack": {"id": 1965, "url": "https://apify.com/petr_cermak/execution-to-knack/api/client/nodejs", "title": "Apify API and Execution To Knack interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"petr_cermak/execution-to-knack\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "petr_cermak/execution-to-knack"}, "mshopik/fungi-perfecti-scraper": {"id": 1966, "url": "https://apify.com/mshopik/fungi-perfecti-scraper/api/client/nodejs", "title": "Apify API and Fungi Perfecti Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/fungi-perfecti-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/fungi-perfecti-scraper"}, "mshopik/swee-lee-singapore-scraper": {"id": 1967, "url": "https://apify.com/mshopik/swee-lee-singapore-scraper/api/client/nodejs", "title": "Apify API and Swee Lee Singapore Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/swee-lee-singapore-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/swee-lee-singapore-scraper"}, "jupri/icims-scraper": {"id": 1968, "url": "https://apify.com/jupri/icims-scraper/api/client/nodejs", "title": "Apify API and ICIMS Jobs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://careers-winco.icims.com\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/icims-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://careers-winco.icims.com"}, "actor_id": "jupri/icims-scraper"}, "mshopik/catch-surfr-usa-scraper": {"id": 1969, "url": "https://apify.com/mshopik/catch-surfr-usa-scraper/api/client/nodejs", "title": "Apify API and Catch Surf\u00ae USA Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/catch-surfr-usa-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/catch-surfr-usa-scraper"}, "mshopik/vital-proteins-scraper": {"id": 1970, "url": "https://apify.com/mshopik/vital-proteins-scraper/api/client/nodejs", "title": "Apify API and Vital Proteins Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/vital-proteins-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/vital-proteins-scraper"}, "mshopik/prismplussg-scraper": {"id": 1971, "url": "https://apify.com/mshopik/prismplussg-scraper/api/client/nodejs", "title": "Apify API and prismplus.sg Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/prismplussg-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/prismplussg-scraper"}, "mshopik/hunter-fan-scraper": {"id": 1972, "url": "https://apify.com/mshopik/hunter-fan-scraper/api/client/nodejs", "title": "Apify API and Hunter Fan Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/hunter-fan-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/hunter-fan-scraper"}, "mshopik/ihealth-labs-scraper": {"id": 1973, "url": "https://apify.com/mshopik/ihealth-labs-scraper/api/client/nodejs", "title": "Apify API and iHealth Labs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/ihealth-labs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/ihealth-labs-scraper"}, "rikunk/safertip-scraper": {"id": 1974, "url": "https://apify.com/rikunk/safertip-scraper/api/client/nodejs", "title": "Apify API and Safertip Bet Prediction Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"rikunk/safertip-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "rikunk/safertip-scraper"}, "mshopik/alpkit-scraper": {"id": 1975, "url": "https://apify.com/mshopik/alpkit-scraper/api/client/nodejs", "title": "Apify API and Alpkit Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/alpkit-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/alpkit-scraper"}, "mshopik/raycon-scraper": {"id": 1976, "url": "https://apify.com/mshopik/raycon-scraper/api/client/nodejs", "title": "Apify API and Raycon Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/raycon-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/raycon-scraper"}, "evision_kr/kr-ssg-scraper": {"id": 2005, "url": "https://apify.com/evision_kr/kr-ssg-scraper/api/client/nodejs", "title": "Apify API and KR SSG Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startPageNumber\": 0,\n    \"finalPageNumber\": 0,\n    \"minPrice\": 0,\n    \"maxPrice\": 0\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"evision_kr/kr-ssg-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxPrice": 0, "minPrice": 0, "finalPageNumber": 0, "startPageNumber": 0}, "actor_id": "evision_kr/kr-ssg-scraper"}, "mshopik/linjer-scraper": {"id": 1977, "url": "https://apify.com/mshopik/linjer-scraper/api/client/nodejs", "title": "Apify API and Linjer Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/linjer-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/linjer-scraper"}, "mshopik/harris-farm-markets-scraper": {"id": 1978, "url": "https://apify.com/mshopik/harris-farm-markets-scraper/api/client/nodejs", "title": "Apify API and Harris Farm Markets Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/harris-farm-markets-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/harris-farm-markets-scraper"}, "jupri/bhinneka-scraper": {"id": 2006, "url": "https://apify.com/jupri/bhinneka-scraper/api/client/nodejs", "title": "Apify API and Bhinneka Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jupri/bhinneka-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "jupri/bhinneka-scraper"}, "mshopik/fresh-water-systems-scraper": {"id": 2007, "url": "https://apify.com/mshopik/fresh-water-systems-scraper/api/client/nodejs", "title": "Apify API and Fresh Water Systems Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/fresh-water-systems-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/fresh-water-systems-scraper"}, "mshopik/miansai-scraper": {"id": 2008, "url": "https://apify.com/mshopik/miansai-scraper/api/client/nodejs", "title": "Apify API and Miansai Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/miansai-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/miansai-scraper"}, "mshopik/danielle-nicole-inc-scraper": {"id": 2009, "url": "https://apify.com/mshopik/danielle-nicole-inc-scraper/api/client/nodejs", "title": "Apify API and Danielle Nicole, Inc. Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/danielle-nicole-inc-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/danielle-nicole-inc-scraper"}, "mshopik/black-rifle-coffee-company-scraper": {"id": 1979, "url": "https://apify.com/mshopik/black-rifle-coffee-company-scraper/api/client/nodejs", "title": "Apify API and Black Rifle Coffee Company Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/black-rifle-coffee-company-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/black-rifle-coffee-company-scraper"}, "curious_coder/apollo-company-scraper": {"id": 1980, "url": "https://apify.com/curious_coder/apollo-company-scraper/api/client/nodejs", "title": "Apify API and Apollo company search scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchUrl\": \"https://app.apollo.io/#/companies?finderViewId=5b6dfc5a73f47568b2e5f11c&page=1&notOrganizationIds[]=5f2a39cb77a7440112460cf5&organizationNumEmployeesRanges[]=1%2C10&organizationIndustryTagIds[]=5567cd467369644d39040000\",\n    \"startPage\": 1,\n    \"count\": 25,\n    \"minDelay\": 2,\n    \"maxDelay\": 7\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/apollo-company-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"count": 25, "maxDelay": 7, "minDelay": 2, "searchUrl": "https://app.apollo.io/#/companies?finderViewId=5b6dfc5a73f47568b2e5f11c&page=1&notOrganizationIds[]=5f2a39cb77a7440112460cf5&organizationNumEmployeesRanges[]=1%2C10&organizationIndustryTagIds[]=5567cd467369644d39040000", "startPage": 1}, "actor_id": "curious_coder/apollo-company-scraper"}, "mshopik/heatpressnation-scraper": {"id": 1981, "url": "https://apify.com/mshopik/heatpressnation-scraper/api/client/nodejs", "title": "Apify API and HeatPressNation Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/heatpressnation-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/heatpressnation-scraper"}, "mshopik/stasher-scraper": {"id": 1982, "url": "https://apify.com/mshopik/stasher-scraper/api/client/nodejs", "title": "Apify API and Stasher Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/stasher-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/stasher-scraper"}, "mshopik/crown-and-paw-scraper": {"id": 1983, "url": "https://apify.com/mshopik/crown-and-paw-scraper/api/client/nodejs", "title": "Apify API and Crown Paw Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/crown-and-paw-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/crown-and-paw-scraper"}, "mshopik/rokform-scraper": {"id": 1984, "url": "https://apify.com/mshopik/rokform-scraper/api/client/nodejs", "title": "Apify API and Rokform Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/rokform-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/rokform-scraper"}, "mshopik/garden-goods-direct-scraper": {"id": 1985, "url": "https://apify.com/mshopik/garden-goods-direct-scraper/api/client/nodejs", "title": "Apify API and Garden Goods Direct Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/garden-goods-direct-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/garden-goods-direct-scraper"}, "datastorm/pii-scraper": {"id": 1986, "url": "https://apify.com/datastorm/pii-scraper/api/client/nodejs", "title": "Apify API and PII and Anonymization Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"items\": [\n        \"My name is Dusty Mayron and I work as the voice of The Panda Radio Station\",\n        \"Overall I am satisfied with my shopping online at Walmart\"\n    ],\n    \"datasets\": [\n        {\n            \"id\": \"serTsUnXrLOVXjPtd\",\n            \"fields\": [\n                \"value\"\n            ]\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"datastorm/pii-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"items": ["My name is Dusty Mayron and I work as the voice of The Panda Radio Station", "Overall I am satisfied with my shopping online at Walmart"], "datasets": [{"id": "serTsUnXrLOVXjPtd", "fields": ["value"]}]}, "actor_id": "datastorm/pii-scraper"}, "mshopik/novelkeys-llc-scraper": {"id": 1987, "url": "https://apify.com/mshopik/novelkeys-llc-scraper/api/client/nodejs", "title": "Apify API and NovelKeys, LLC Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/novelkeys-llc-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/novelkeys-llc-scraper"}, "mshopik/leatherup-usa-scraper": {"id": 1988, "url": "https://apify.com/mshopik/leatherup-usa-scraper/api/client/nodejs", "title": "Apify API and LeatherUp USA Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/leatherup-usa-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/leatherup-usa-scraper"}, "useful-tools/do-request": {"id": 1989, "url": "https://apify.com/useful-tools/do-request/api/client/nodejs", "title": "Apify API and Do Request interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"http://example.com/\",\n    \"headers\": {\n        \"Accept\": \"text/html, application/xhtml+xml, application/xml;q=0.9, image/webp, */*;q=0.8\"\n    },\n    \"secretHeaderAuthorization\": \"\",\n    \"secretHeaderCookie\": \"\",\n    \"proxyConfiguration\": {\n        \"useApifyProxy\": false\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"useful-tools/do-request\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "http://example.com/", "headers": {"Accept": "text/html, application/xhtml+xml, application/xml;q=0.9, image/webp, */*;q=0.8"}, "proxyConfiguration": {"useApifyProxy": false}, "secretHeaderCookie": "", "secretHeaderAuthorization": ""}, "actor_id": "useful-tools/do-request"}, "jyaba_tech/amh-crawler": {"id": 1990, "url": "https://apify.com/jyaba_tech/amh-crawler/api/client/nodejs", "title": "Apify API and Amh Crawler interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": [\n        {\n            \"url\": \"https://apify.com\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jyaba_tech/amh-crawler\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"start_urls": [{"url": "https://apify.com"}]}, "actor_id": "jyaba_tech/amh-crawler"}, "mshopik/azuro-republic-scraper": {"id": 1993, "url": "https://apify.com/mshopik/azuro-republic-scraper/api/client/nodejs", "title": "Apify API and Azuro Republic Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/azuro-republic-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/azuro-republic-scraper"}, "mshopik/dormify-scraper": {"id": 1994, "url": "https://apify.com/mshopik/dormify-scraper/api/client/nodejs", "title": "Apify API and Dormify Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/dormify-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/dormify-scraper"}, "mshopik/lets-make-art-scraper": {"id": 1995, "url": "https://apify.com/mshopik/lets-make-art-scraper/api/client/nodejs", "title": "Apify API and Let's Make Art Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/lets-make-art-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/lets-make-art-scraper"}, "mshopik/thebigblackfriday-scraper": {"id": 1996, "url": "https://apify.com/mshopik/thebigblackfriday-scraper/api/client/nodejs", "title": "Apify API and thebigblackfriday Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 100,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    },\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/thebigblackfriday-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "proxyConfig": {"useApifyProxy": true}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 100}, "actor_id": "mshopik/thebigblackfriday-scraper"}, "mshopik/mnml-scraper": {"id": 1997, "url": "https://apify.com/mshopik/mnml-scraper/api/client/nodejs", "title": "Apify API and MNML Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/mnml-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/mnml-scraper"}, "mshopik/remodelaholic-scraper": {"id": 1998, "url": "https://apify.com/mshopik/remodelaholic-scraper/api/client/nodejs", "title": "Apify API and Remodelaholic Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/remodelaholic-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/remodelaholic-scraper"}, "mshopik/mission-scraper": {"id": 1999, "url": "https://apify.com/mshopik/mission-scraper/api/client/nodejs", "title": "Apify API and MISSION Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/mission-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/mission-scraper"}, "mshopik/francfranc-scraper": {"id": 2000, "url": "https://apify.com/mshopik/francfranc-scraper/api/client/nodejs", "title": "Apify API and Francfranc\uff08\u30d5\u30e9\u30f3\u30d5\u30e9\u30f3\uff09\u516c\u5f0f\u901a\u8ca9 Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/francfranc-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/francfranc-scraper"}, "mshopik/manscaped-scraper": {"id": 2001, "url": "https://apify.com/mshopik/manscaped-scraper/api/client/nodejs", "title": "Apify API and Manscaped Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/manscaped-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/manscaped-scraper"}, "distinct_desk/yt-channel-details": {"id": 2002, "url": "https://apify.com/distinct_desk/yt-channel-details/api/client/nodejs", "title": "Apify API and Yt Channel Details interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.youtube.com/@apify\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"distinct_desk/yt-channel-details\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.youtube.com/@apify"}]}, "actor_id": "distinct_desk/yt-channel-details"}, "mshopik/european-wax-center-scraper": {"id": 2003, "url": "https://apify.com/mshopik/european-wax-center-scraper/api/client/nodejs", "title": "Apify API and European Wax Center Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/european-wax-center-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/european-wax-center-scraper"}, "mshopik/marine-layer-scraper": {"id": 2004, "url": "https://apify.com/mshopik/marine-layer-scraper/api/client/nodejs", "title": "Apify API and Marine Layer Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/marine-layer-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/marine-layer-scraper"}, "mshopik/wonkette-scraper": {"id": 2010, "url": "https://apify.com/mshopik/wonkette-scraper/api/client/nodejs", "title": "Apify API and Wonkette Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/wonkette-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/wonkette-scraper"}, "mshopik/dose-of-colors-scraper": {"id": 2011, "url": "https://apify.com/mshopik/dose-of-colors-scraper/api/client/nodejs", "title": "Apify API and Dose of Colors Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/dose-of-colors-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/dose-of-colors-scraper"}, "mshopik/architects-journal-scraper": {"id": 2012, "url": "https://apify.com/mshopik/architects-journal-scraper/api/client/nodejs", "title": "Apify API and Architects\u2019 Journal Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/architects-journal-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/architects-journal-scraper"}, "mshopik/helix-sleep-scraper": {"id": 2013, "url": "https://apify.com/mshopik/helix-sleep-scraper/api/client/nodejs", "title": "Apify API and Helix Sleep Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/helix-sleep-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/helix-sleep-scraper"}, "mshopik/altitude-sports-scraper": {"id": 2014, "url": "https://apify.com/mshopik/altitude-sports-scraper/api/client/nodejs", "title": "Apify API and Altitude Sports Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/altitude-sports-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/altitude-sports-scraper"}, "mshopik/klymit-scraper": {"id": 2015, "url": "https://apify.com/mshopik/klymit-scraper/api/client/nodejs", "title": "Apify API and Klymit Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/klymit-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/klymit-scraper"}, "mshopik/ethnic-by-outfitters-scraper": {"id": 2016, "url": "https://apify.com/mshopik/ethnic-by-outfitters-scraper/api/client/nodejs", "title": "Apify API and Ethnic by Outfitters Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/ethnic-by-outfitters-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/ethnic-by-outfitters-scraper"}, "mshopik/kindred-bravely-scraper": {"id": 2017, "url": "https://apify.com/mshopik/kindred-bravely-scraper/api/client/nodejs", "title": "Apify API and Kindred Bravely Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/kindred-bravely-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/kindred-bravely-scraper"}, "mshopik/buykud-scraper": {"id": 2018, "url": "https://apify.com/mshopik/buykud-scraper/api/client/nodejs", "title": "Apify API and Buykud Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/buykud-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/buykud-scraper"}, "mshopik/thinx-scraper": {"id": 2019, "url": "https://apify.com/mshopik/thinx-scraper/api/client/nodejs", "title": "Apify API and Thinx Thinx (BTWN) Speax Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/thinx-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/thinx-scraper"}, "mshopik/citiesocial-scraper": {"id": 2020, "url": "https://apify.com/mshopik/citiesocial-scraper/api/client/nodejs", "title": "Apify API and citiesocial Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/citiesocial-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/citiesocial-scraper"}, "mshopik/mirascreencom-scraper": {"id": 2021, "url": "https://apify.com/mshopik/mirascreencom-scraper/api/client/nodejs", "title": "Apify API and mirascreen.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/mirascreencom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/mirascreencom-scraper"}, "mshopik/jolyn-scraper": {"id": 2022, "url": "https://apify.com/mshopik/jolyn-scraper/api/client/nodejs", "title": "Apify API and JOLYN Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/jolyn-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/jolyn-scraper"}, "mshopik/wwwmodishstorecom-scraper": {"id": 2023, "url": "https://apify.com/mshopik/wwwmodishstorecom-scraper/api/client/nodejs", "title": "Apify API and www.modishstore.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/wwwmodishstorecom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/wwwmodishstorecom-scraper"}, "mshopik/desmond-and-dempsey-scraper": {"id": 2024, "url": "https://apify.com/mshopik/desmond-and-dempsey-scraper/api/client/nodejs", "title": "Apify API and Desmond Dempsey Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/desmond-and-dempsey-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/desmond-and-dempsey-scraper"}, "mshopik/biossance-scraper": {"id": 2025, "url": "https://apify.com/mshopik/biossance-scraper/api/client/nodejs", "title": "Apify API and Biossance Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/biossance-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/biossance-scraper"}, "mshopik/47-scraper": {"id": 2026, "url": "https://apify.com/mshopik/47-scraper/api/client/nodejs", "title": "Apify API and 47 Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/47-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/47-scraper"}, "mshopik/olukai-scraper": {"id": 2027, "url": "https://apify.com/mshopik/olukai-scraper/api/client/nodejs", "title": "Apify API and OluKai Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/olukai-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/olukai-scraper"}, "mshopik/fray-scraper": {"id": 2028, "url": "https://apify.com/mshopik/fray-scraper/api/client/nodejs", "title": "Apify API and Fray Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/fray-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/fray-scraper"}, "mshopik/creality3dr-printers-scraper": {"id": 2029, "url": "https://apify.com/mshopik/creality3dr-printers-scraper/api/client/nodejs", "title": "Apify API and Creality3D\u00ae Printers Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/creality3dr-printers-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/creality3dr-printers-scraper"}, "mshopik/mack-weldon-scraper": {"id": 2030, "url": "https://apify.com/mshopik/mack-weldon-scraper/api/client/nodejs", "title": "Apify API and Mack Weldon Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/mack-weldon-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/mack-weldon-scraper"}, "mshopik/dailysale-scraper": {"id": 2031, "url": "https://apify.com/mshopik/dailysale-scraper/api/client/nodejs", "title": "Apify API and DailySale Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/dailysale-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/dailysale-scraper"}, "mshopik/jw-pei-scraper": {"id": 2032, "url": "https://apify.com/mshopik/jw-pei-scraper/api/client/nodejs", "title": "Apify API and JW PEI Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/jw-pei-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/jw-pei-scraper"}, "mshopik/brumate-scraper": {"id": 2033, "url": "https://apify.com/mshopik/brumate-scraper/api/client/nodejs", "title": "Apify API and Br\u00fcMate Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/brumate-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/brumate-scraper"}, "mshopik/smallwoods-scraper": {"id": 2034, "url": "https://apify.com/mshopik/smallwoods-scraper/api/client/nodejs", "title": "Apify API and Smallwoods Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/smallwoods-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/smallwoods-scraper"}, "mshopik/dope-scraper": {"id": 2035, "url": "https://apify.com/mshopik/dope-scraper/api/client/nodejs", "title": "Apify API and DOPE Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/dope-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/dope-scraper"}, "mshopik/sports-basement-scraper": {"id": 2036, "url": "https://apify.com/mshopik/sports-basement-scraper/api/client/nodejs", "title": "Apify API and Sports Basement Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/sports-basement-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/sports-basement-scraper"}, "mshopik/electric-scraper": {"id": 2037, "url": "https://apify.com/mshopik/electric-scraper/api/client/nodejs", "title": "Apify API and Electric Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/electric-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/electric-scraper"}, "mshopik/wwwshopmissacom-scraper": {"id": 2038, "url": "https://apify.com/mshopik/wwwshopmissacom-scraper/api/client/nodejs", "title": "Apify API and www.shopmissa.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/wwwshopmissacom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/wwwshopmissacom-scraper"}, "mshopik/usatuancom-scraper": {"id": 2039, "url": "https://apify.com/mshopik/usatuancom-scraper/api/client/nodejs", "title": "Apify API and \u7f8e\u56fd\u56e2\u8d2d\u7f51 \u7f8e\u56fd\u56e2\u8d2d\u7f51USAtuan.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/usatuancom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/usatuancom-scraper"}, "mshopik/aday-scraper": {"id": 2040, "url": "https://apify.com/mshopik/aday-scraper/api/client/nodejs", "title": "Apify API and ADAY Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/aday-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/aday-scraper"}, "mshopik/first-lite-scraper": {"id": 2041, "url": "https://apify.com/mshopik/first-lite-scraper/api/client/nodejs", "title": "Apify API and First Lite Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/first-lite-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/first-lite-scraper"}, "mshopik/holabird-sports-scraper": {"id": 2042, "url": "https://apify.com/mshopik/holabird-sports-scraper/api/client/nodejs", "title": "Apify API and Holabird Sports Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/holabird-sports-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/holabird-sports-scraper"}, "mshopik/thrive-causemetics-scraper": {"id": 2043, "url": "https://apify.com/mshopik/thrive-causemetics-scraper/api/client/nodejs", "title": "Apify API and Thrive Causemetics Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/thrive-causemetics-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/thrive-causemetics-scraper"}, "mshopik/vitaly-us-scraper": {"id": 2044, "url": "https://apify.com/mshopik/vitaly-us-scraper/api/client/nodejs", "title": "Apify API and Vitaly US Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/vitaly-us-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/vitaly-us-scraper"}, "mshopik/misen-scraper": {"id": 2045, "url": "https://apify.com/mshopik/misen-scraper/api/client/nodejs", "title": "Apify API and Misen Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/misen-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/misen-scraper"}, "mshopik/timbuk2-scraper": {"id": 2046, "url": "https://apify.com/mshopik/timbuk2-scraper/api/client/nodejs", "title": "Apify API and Timbuk2 Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/timbuk2-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/timbuk2-scraper"}, "mshopik/billini-scraper": {"id": 2047, "url": "https://apify.com/mshopik/billini-scraper/api/client/nodejs", "title": "Apify API and Billini Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/billini-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/billini-scraper"}, "mshopik/botkier-scraper": {"id": 2048, "url": "https://apify.com/mshopik/botkier-scraper/api/client/nodejs", "title": "Apify API and Botkier Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/botkier-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/botkier-scraper"}, "mshopik/sinners-attire-scraper": {"id": 2049, "url": "https://apify.com/mshopik/sinners-attire-scraper/api/client/nodejs", "title": "Apify API and SINNERS ATTIRE Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/sinners-attire-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/sinners-attire-scraper"}, "hamza.alwan/array-to-excel": {"id": 2050, "url": "https://apify.com/hamza.alwan/array-to-excel/api/client/nodejs", "title": "Apify API and Array to Excel interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/array-to-excel\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "hamza.alwan/array-to-excel"}, "mshopik/outfitters-official-online-store-scraper": {"id": 2051, "url": "https://apify.com/mshopik/outfitters-official-online-store-scraper/api/client/nodejs", "title": "Apify API and Outfitters Official Online Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/outfitters-official-online-store-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/outfitters-official-online-store-scraper"}, "rikunk/forebet-scraper": {"id": 2140, "url": "https://apify.com/rikunk/forebet-scraper/api/client/nodejs", "title": "Apify API and Forebet Bet Prediction Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"rikunk/forebet-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "rikunk/forebet-scraper"}, "mshopik/dynaquest-pc-scraper": {"id": 2052, "url": "https://apify.com/mshopik/dynaquest-pc-scraper/api/client/nodejs", "title": "Apify API and DynaQuest PC Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/dynaquest-pc-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/dynaquest-pc-scraper"}, "mshopik/lifxcom-lifx-us-scraper": {"id": 2053, "url": "https://apify.com/mshopik/lifxcom-lifx-us-scraper/api/client/nodejs", "title": "Apify API and LIFX.com\u2013 LIFX US Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/lifxcom-lifx-us-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/lifxcom-lifx-us-scraper"}, "mshopik/tigerfitnesscom-scraper": {"id": 2054, "url": "https://apify.com/mshopik/tigerfitnesscom-scraper/api/client/nodejs", "title": "Apify API and TigerFitness.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/tigerfitnesscom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/tigerfitnesscom-scraper"}, "mshopik/blacktailor-scraper": {"id": 2055, "url": "https://apify.com/mshopik/blacktailor-scraper/api/client/nodejs", "title": "Apify API and BLACKTAILOR Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/blacktailor-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/blacktailor-scraper"}, "defozo/dietly-data-scraper": {"id": 2056, "url": "https://apify.com/defozo/dietly-data-scraper/api/client/nodejs", "title": "Apify API and Dietly Data Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"defozo/dietly-data-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "defozo/dietly-data-scraper"}, "mshopik/stealth-angel-survival-scraper": {"id": 2057, "url": "https://apify.com/mshopik/stealth-angel-survival-scraper/api/client/nodejs", "title": "Apify API and Stealth Angel Survival Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/stealth-angel-survival-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/stealth-angel-survival-scraper"}, "rikunk/predictz-scraper": {"id": 2226, "url": "https://apify.com/rikunk/predictz-scraper/api/client/nodejs", "title": "Apify API and PredictZ Bet Prediction Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"rikunk/predictz-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "rikunk/predictz-scraper"}, "mshopik/the-goulet-pen-company-scraper": {"id": 2058, "url": "https://apify.com/mshopik/the-goulet-pen-company-scraper/api/client/nodejs", "title": "Apify API and The Goulet Pen Company Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-goulet-pen-company-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/the-goulet-pen-company-scraper"}, "mshopik/foco-scraper": {"id": 2059, "url": "https://apify.com/mshopik/foco-scraper/api/client/nodejs", "title": "Apify API and FOCO Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/foco-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/foco-scraper"}, "mshopik/portland-leather-goods-scraper": {"id": 2060, "url": "https://apify.com/mshopik/portland-leather-goods-scraper/api/client/nodejs", "title": "Apify API and Portland Leather Goods Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/portland-leather-goods-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/portland-leather-goods-scraper"}, "mshopik/ruffwear-scraper": {"id": 2061, "url": "https://apify.com/mshopik/ruffwear-scraper/api/client/nodejs", "title": "Apify API and Ruffwear Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/ruffwear-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/ruffwear-scraper"}, "mshopik/inh-hair-scraper": {"id": 2062, "url": "https://apify.com/mshopik/inh-hair-scraper/api/client/nodejs", "title": "Apify API and INH Hair Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/inh-hair-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/inh-hair-scraper"}, "mshopik/gymshark-row-scraper": {"id": 2063, "url": "https://apify.com/mshopik/gymshark-row-scraper/api/client/nodejs", "title": "Apify API and Gymshark ROW Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/gymshark-row-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/gymshark-row-scraper"}, "mshopik/semihandmade-scraper": {"id": 2064, "url": "https://apify.com/mshopik/semihandmade-scraper/api/client/nodejs", "title": "Apify API and Semihandmade Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/semihandmade-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/semihandmade-scraper"}, "mshopik/nobull-scraper": {"id": 2065, "url": "https://apify.com/mshopik/nobull-scraper/api/client/nodejs", "title": "Apify API and NOBULL Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/nobull-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/nobull-scraper"}, "mshopik/4patriots-scraper": {"id": 2066, "url": "https://apify.com/mshopik/4patriots-scraper/api/client/nodejs", "title": "Apify API and 4Patriots Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/4patriots-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/4patriots-scraper"}, "mshopik/tushy-scraper": {"id": 2067, "url": "https://apify.com/mshopik/tushy-scraper/api/client/nodejs", "title": "Apify API and TUSHY Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/tushy-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/tushy-scraper"}, "mshopik/printfresh-scraper": {"id": 2068, "url": "https://apify.com/mshopik/printfresh-scraper/api/client/nodejs", "title": "Apify API and PRINTFRESH Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/printfresh-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/printfresh-scraper"}, "mshopik/bee-inspired-clothing-scraper": {"id": 2069, "url": "https://apify.com/mshopik/bee-inspired-clothing-scraper/api/client/nodejs", "title": "Apify API and Bee Inspired Clothing Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bee-inspired-clothing-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bee-inspired-clothing-scraper"}, "mshopik/slice-1-scraper": {"id": 2070, "url": "https://apify.com/mshopik/slice-1-scraper/api/client/nodejs", "title": "Apify API and Slice Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/slice-1-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/slice-1-scraper"}, "mshopik/diff-eyewear-scraper": {"id": 2071, "url": "https://apify.com/mshopik/diff-eyewear-scraper/api/client/nodejs", "title": "Apify API and DIFF Eyewear Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/diff-eyewear-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/diff-eyewear-scraper"}, "cyberfly/actor-probe": {"id": 2072, "url": "https://apify.com/cyberfly/actor-probe/api/client/nodejs", "title": "Apify API and Actor Probe interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"cyberfly/actor-probe\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "cyberfly/actor-probe"}, "useful-tools/details-of-runs": {"id": 2073, "url": "https://apify.com/useful-tools/details-of-runs/api/client/nodejs", "title": "Apify API and Details of runs interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"actorIds\": [\n        \"Y8pYbKBLR3SNv6Gxd\"\n    ],\n    \"limit\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"useful-tools/details-of-runs\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"limit": 100, "actorIds": ["Y8pYbKBLR3SNv6Gxd"]}, "actor_id": "useful-tools/details-of-runs"}, "mshopik/ctrlaltdel-comic-scraper": {"id": 2074, "url": "https://apify.com/mshopik/ctrlaltdel-comic-scraper/api/client/nodejs", "title": "Apify API and Ctrl+Alt+Del Comic Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/ctrlaltdel-comic-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/ctrlaltdel-comic-scraper"}, "mshopik/crane-and-canopy-scraper": {"id": 2075, "url": "https://apify.com/mshopik/crane-and-canopy-scraper/api/client/nodejs", "title": "Apify API and Crane Canopy Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/crane-and-canopy-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/crane-and-canopy-scraper"}, "mshopik/volcom-us-scraper": {"id": 2076, "url": "https://apify.com/mshopik/volcom-us-scraper/api/client/nodejs", "title": "Apify API and Volcom US Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/volcom-us-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/volcom-us-scraper"}, "mshopik/bleusalt-scraper": {"id": 2077, "url": "https://apify.com/mshopik/bleusalt-scraper/api/client/nodejs", "title": "Apify API and Bleusalt Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bleusalt-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bleusalt-scraper"}, "mshopik/lisa-says-gah-scraper": {"id": 2078, "url": "https://apify.com/mshopik/lisa-says-gah-scraper/api/client/nodejs", "title": "Apify API and Lisa Says Gah Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/lisa-says-gah-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/lisa-says-gah-scraper"}, "mshopik/umbra-scraper": {"id": 2079, "url": "https://apify.com/mshopik/umbra-scraper/api/client/nodejs", "title": "Apify API and Umbra Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/umbra-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/umbra-scraper"}, "mshopik/turntablelabcom-scraper": {"id": 2080, "url": "https://apify.com/mshopik/turntablelabcom-scraper/api/client/nodejs", "title": "Apify API and TurntableLab.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/turntablelabcom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/turntablelabcom-scraper"}, "mshopik/bajaaocom-scraper": {"id": 2081, "url": "https://apify.com/mshopik/bajaaocom-scraper/api/client/nodejs", "title": "Apify API and BAJAAO.COM Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bajaaocom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bajaaocom-scraper"}, "mshopik/necessaire-scraper": {"id": 2082, "url": "https://apify.com/mshopik/necessaire-scraper/api/client/nodejs", "title": "Apify API and N\u00e9cessaire Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/necessaire-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/necessaire-scraper"}, "mshopik/the-ridge-wallet-scraper": {"id": 2083, "url": "https://apify.com/mshopik/the-ridge-wallet-scraper/api/client/nodejs", "title": "Apify API and The Ridge Wallet Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-ridge-wallet-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/the-ridge-wallet-scraper"}, "mshopik/alternative-press-magazine-scraper": {"id": 2084, "url": "https://apify.com/mshopik/alternative-press-magazine-scraper/api/client/nodejs", "title": "Apify API and Alternative Press Magazine Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/alternative-press-magazine-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/alternative-press-magazine-scraper"}, "mshopik/schoolhouse-scraper": {"id": 2085, "url": "https://apify.com/mshopik/schoolhouse-scraper/api/client/nodejs", "title": "Apify API and Schoolhouse Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/schoolhouse-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/schoolhouse-scraper"}, "mshopik/bando-scraper": {"id": 2086, "url": "https://apify.com/mshopik/bando-scraper/api/client/nodejs", "title": "Apify API and ban.do Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bando-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bando-scraper"}, "mscraper/jsonpath": {"id": 2087, "url": "https://apify.com/mscraper/jsonpath/api/client/nodejs", "title": "Apify API and JSONPath interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://be.wizzair.com/17.7.0/Api/asset/map?languageCode=en-gb\"\n        }\n    ],\n    \"jsonPath\": \"$.cities[?(@.currencyCode=='CZK')].iata\",\n    \"transformFunction\": async ({ result }) => result.map((name) => `https://www.iata.org/en/publications/directories/code-search/?airport.search=${name}`)\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/jsonpath\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"jsonPath": "$.cities[?(@.currencyCode=='CZK')].iata", "startUrls": [{"url": "https://be.wizzair.com/17.7.0/Api/asset/map?languageCode=en-gb"}]}, "actor_id": "mscraper/jsonpath"}, "mshopik/pelagic-fishing-gear-scraper": {"id": 2088, "url": "https://apify.com/mshopik/pelagic-fishing-gear-scraper/api/client/nodejs", "title": "Apify API and PELAGIC Fishing Gear Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/pelagic-fishing-gear-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/pelagic-fishing-gear-scraper"}, "mshopik/sphero-scraper": {"id": 2089, "url": "https://apify.com/mshopik/sphero-scraper/api/client/nodejs", "title": "Apify API and Sphero Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/sphero-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/sphero-scraper"}, "mshopik/tula-skincare-scraper": {"id": 2090, "url": "https://apify.com/mshopik/tula-skincare-scraper/api/client/nodejs", "title": "Apify API and TULA Skincare Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/tula-skincare-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/tula-skincare-scraper"}, "mshopik/frye-since-1863-scraper": {"id": 2091, "url": "https://apify.com/mshopik/frye-since-1863-scraper/api/client/nodejs", "title": "Apify API and FRYE Since 1863 Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/frye-since-1863-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/frye-since-1863-scraper"}, "mshopik/bluefly-scraper": {"id": 2092, "url": "https://apify.com/mshopik/bluefly-scraper/api/client/nodejs", "title": "Apify API and Bluefly Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bluefly-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bluefly-scraper"}, "mshopik/morphe-us-scraper": {"id": 2093, "url": "https://apify.com/mshopik/morphe-us-scraper/api/client/nodejs", "title": "Apify API and Morphe US Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/morphe-us-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/morphe-us-scraper"}, "mshopik/olive-and-june-scraper": {"id": 2094, "url": "https://apify.com/mshopik/olive-and-june-scraper/api/client/nodejs", "title": "Apify API and Olive and June Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/olive-and-june-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/olive-and-june-scraper"}, "mshopik/the-beachwaver-co-scraper": {"id": 2095, "url": "https://apify.com/mshopik/the-beachwaver-co-scraper/api/client/nodejs", "title": "Apify API and The Beachwaver Co. Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-beachwaver-co-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/the-beachwaver-co-scraper"}, "mshopik/unclaimed-baggage-center-scraper": {"id": 2096, "url": "https://apify.com/mshopik/unclaimed-baggage-center-scraper/api/client/nodejs", "title": "Apify API and Unclaimed Baggage Center Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/unclaimed-baggage-center-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/unclaimed-baggage-center-scraper"}, "jyaba_tech/ca-court-interpreter": {"id": 2097, "url": "https://apify.com/jyaba_tech/ca-court-interpreter/api/client/nodejs", "title": "Apify API and Ca Court Interpreter interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"start_urls\": [\n        {\n            \"url\": \"https://www.courts.ca.gov/cms/courtinterpreters/newctintdb.cfm\"\n        }\n    ]\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jyaba_tech/ca-court-interpreter\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"start_urls": [{"url": "https://www.courts.ca.gov/cms/courtinterpreters/newctintdb.cfm"}]}, "actor_id": "jyaba_tech/ca-court-interpreter"}, "mshopik/blendjet-scraper": {"id": 2098, "url": "https://apify.com/mshopik/blendjet-scraper/api/client/nodejs", "title": "Apify API and BlendJet Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/blendjet-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/blendjet-scraper"}, "mshopik/commando-scraper": {"id": 2099, "url": "https://apify.com/mshopik/commando-scraper/api/client/nodejs", "title": "Apify API and commando Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/commando-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/commando-scraper"}, "mshopik/bones-coffee-company-scraper": {"id": 2100, "url": "https://apify.com/mshopik/bones-coffee-company-scraper/api/client/nodejs", "title": "Apify API and Bones Coffee Company Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bones-coffee-company-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bones-coffee-company-scraper"}, "mshopik/keychron-scraper": {"id": 2101, "url": "https://apify.com/mshopik/keychron-scraper/api/client/nodejs", "title": "Apify API and Keychron Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/keychron-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/keychron-scraper"}, "mshopik/fastgrowingtreescom-scraper": {"id": 2102, "url": "https://apify.com/mshopik/fastgrowingtreescom-scraper/api/client/nodejs", "title": "Apify API and FastGrowingTrees.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/fastgrowingtreescom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/fastgrowingtreescom-scraper"}, "devoted_dogwood/my-actor": {"id": 2103, "url": "https://apify.com/devoted_dogwood/my-actor/api/client/nodejs", "title": "Apify API and My Actor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"devoted_dogwood/my-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "devoted_dogwood/my-actor"}, "mshopik/big-life-journal-scraper": {"id": 2104, "url": "https://apify.com/mshopik/big-life-journal-scraper/api/client/nodejs", "title": "Apify API and Big Life Journal Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/big-life-journal-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/big-life-journal-scraper"}, "mshopik/bandolier-scraper": {"id": 2105, "url": "https://apify.com/mshopik/bandolier-scraper/api/client/nodejs", "title": "Apify API and Bandolier Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bandolier-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bandolier-scraper"}, "mshopik/warrior-12-scraper": {"id": 2106, "url": "https://apify.com/mshopik/warrior-12-scraper/api/client/nodejs", "title": "Apify API and Warrior 12 Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/warrior-12-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/warrior-12-scraper"}, "mshopik/rageon-worlds-scraper": {"id": 2107, "url": "https://apify.com/mshopik/rageon-worlds-scraper/api/client/nodejs", "title": "Apify API and RageOn! World's Largest Online Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/rageon-worlds-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/rageon-worlds-scraper"}, "mshopik/kprepublic-global-scraper": {"id": 2108, "url": "https://apify.com/mshopik/kprepublic-global-scraper/api/client/nodejs", "title": "Apify API and KPrepublic Global Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/kprepublic-global-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/kprepublic-global-scraper"}, "mshopik/native-scraper": {"id": 2109, "url": "https://apify.com/mshopik/native-scraper/api/client/nodejs", "title": "Apify API and Native Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/native-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/native-scraper"}, "mshopik/skull-shaver-scraper": {"id": 2110, "url": "https://apify.com/mshopik/skull-shaver-scraper/api/client/nodejs", "title": "Apify API and Skull Shaver Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/skull-shaver-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/skull-shaver-scraper"}, "mshopik/hexcladcom-scraper": {"id": 2111, "url": "https://apify.com/mshopik/hexcladcom-scraper/api/client/nodejs", "title": "Apify API and hexclad.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/hexcladcom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/hexcladcom-scraper"}, "mshopik/worldwide-cyclery-scraper": {"id": 2112, "url": "https://apify.com/mshopik/worldwide-cyclery-scraper/api/client/nodejs", "title": "Apify API and Worldwide Cyclery Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/worldwide-cyclery-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/worldwide-cyclery-scraper"}, "mshopik/the-hard-times-scraper": {"id": 2113, "url": "https://apify.com/mshopik/the-hard-times-scraper/api/client/nodejs", "title": "Apify API and The Hard Times Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-hard-times-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/the-hard-times-scraper"}, "mshopik/hatch-collection-scraper": {"id": 2114, "url": "https://apify.com/mshopik/hatch-collection-scraper/api/client/nodejs", "title": "Apify API and HATCH Collection Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/hatch-collection-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/hatch-collection-scraper"}, "mshopik/kitsch-scraper": {"id": 2115, "url": "https://apify.com/mshopik/kitsch-scraper/api/client/nodejs", "title": "Apify API and KITSCH Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/kitsch-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/kitsch-scraper"}, "mshopik/heatonist-scraper": {"id": 2116, "url": "https://apify.com/mshopik/heatonist-scraper/api/client/nodejs", "title": "Apify API and HEATONIST Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/heatonist-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/heatonist-scraper"}, "mshopik/chichiclothing-scraper": {"id": 2117, "url": "https://apify.com/mshopik/chichiclothing-scraper/api/client/nodejs", "title": "Apify API and ChiChiClothing Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/chichiclothing-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/chichiclothing-scraper"}, "curious_coder/facebook-people-search-scraper": {"id": 2118, "url": "https://apify.com/curious_coder/facebook-people-search-scraper/api/client/nodejs", "title": "Apify API and Facebook people search scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchUrl\": \"https://www.facebook.com/search/people?q=marketing&filters=eyJjaXR5OjAiOiJ7XCJuYW1lXCI6XCJ1c2Vyc19sb2NhdGlvblwiLFwiYXJnc1wiOlwiMTA2Mzc3MzM2MDY3NjM4XCJ9In0%3D\",\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/facebook-people-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"maxDelay": 3, "minDelay": 1, "searchUrl": "https://www.facebook.com/search/people?q=marketing&filters=eyJjaXR5OjAiOiJ7XCJuYW1lXCI6XCJ1c2Vyc19sb2NhdGlvblwiLFwiYXJnc1wiOlwiMTA2Mzc3MzM2MDY3NjM4XCJ9In0%3D"}, "actor_id": "curious_coder/facebook-people-search-scraper"}, "mshopik/andi-bagus-scraper": {"id": 2119, "url": "https://apify.com/mshopik/andi-bagus-scraper/api/client/nodejs", "title": "Apify API and Andi Bagus Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/andi-bagus-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/andi-bagus-scraper"}, "epctex/vimeo-video-downloader": {"id": 2120, "url": "https://apify.com/epctex/vimeo-video-downloader/api/client/nodejs", "title": "Apify API and Vimeo Video Downloader interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        \"https://vimeo.com/693117164\"\n    ],\n    \"quality\": \"medium\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"epctex/vimeo-video-downloader\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true}, "quality": "medium", "startUrls": ["https://vimeo.com/693117164"]}, "actor_id": "epctex/vimeo-video-downloader"}, "mshopik/charcoal-clothing-scraper": {"id": 2121, "url": "https://apify.com/mshopik/charcoal-clothing-scraper/api/client/nodejs", "title": "Apify API and Charcoal Clothing Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/charcoal-clothing-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/charcoal-clothing-scraper"}, "mshopik/gymshark-uk-scraper": {"id": 2122, "url": "https://apify.com/mshopik/gymshark-uk-scraper/api/client/nodejs", "title": "Apify API and Gymshark UK Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/gymshark-uk-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/gymshark-uk-scraper"}, "mshopik/nalai-and-co-scraper": {"id": 2123, "url": "https://apify.com/mshopik/nalai-and-co-scraper/api/client/nodejs", "title": "Apify API and Nalai Co Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/nalai-and-co-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/nalai-and-co-scraper"}, "hamza.alwan/gab-search-scraper": {"id": 2124, "url": "https://apify.com/hamza.alwan/gab-search-scraper/api/client/nodejs", "title": "Apify API and Gab Search Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [],\n    \"searchQueries\": [\n        \"Cat\"\n    ],\n    \"searchType\": \"TOP\",\n    \"desiredPostsCount\": 100,\n    \"desiredCommentsCount\": 0,\n    \"desiredAccountsCount\": 10,\n    \"desiredGroupsCount\": 10,\n    \"accountPostsSortBy\": \"NEWEST\",\n    \"groupPostsSortBy\": \"NEWEST\",\n    \"commentsSortBy\": \"MOST_LIKED\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"hamza.alwan/gab-search-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [], "searchType": "TOP", "searchQueries": ["Cat"], "commentsSortBy": "MOST_LIKED", "groupPostsSortBy": "NEWEST", "desiredPostsCount": 100, "accountPostsSortBy": "NEWEST", "desiredGroupsCount": 10, "desiredAccountsCount": 10, "desiredCommentsCount": 0}, "actor_id": "hamza.alwan/gab-search-scraper"}, "mshopik/katy-perry-collections-scraper": {"id": 2125, "url": "https://apify.com/mshopik/katy-perry-collections-scraper/api/client/nodejs", "title": "Apify API and Katy Perry Collections Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 100,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    },\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/katy-perry-collections-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "proxyConfig": {"useApifyProxy": true}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 100}, "actor_id": "mshopik/katy-perry-collections-scraper"}, "mshopik/lively-scraper": {"id": 2126, "url": "https://apify.com/mshopik/lively-scraper/api/client/nodejs", "title": "Apify API and LIVELY Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/lively-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/lively-scraper"}, "mshopik/boll-and-branch-scraper": {"id": 2127, "url": "https://apify.com/mshopik/boll-and-branch-scraper/api/client/nodejs", "title": "Apify API and Boll Branch Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/boll-and-branch-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/boll-and-branch-scraper"}, "mshopik/bychari-scraper": {"id": 2128, "url": "https://apify.com/mshopik/bychari-scraper/api/client/nodejs", "title": "Apify API and BYCHARI Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bychari-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bychari-scraper"}, "gabrielm/flipkey-scraper": {"id": 2129, "url": "https://apify.com/gabrielm/flipkey-scraper/api/client/nodejs", "title": "Apify API and FlipKey Listings Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"adv_proxy_usage\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"gabrielm/flipkey-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"adv_proxy_usage": {"useApifyProxy": true}}, "actor_id": "gabrielm/flipkey-scraper"}, "mshopik/staticnailscom-scraper": {"id": 2130, "url": "https://apify.com/mshopik/staticnailscom-scraper/api/client/nodejs", "title": "Apify API and staticnails.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/staticnailscom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/staticnailscom-scraper"}, "mshopik/roolee-scraper": {"id": 2131, "url": "https://apify.com/mshopik/roolee-scraper/api/client/nodejs", "title": "Apify API and ROOLEE Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/roolee-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/roolee-scraper"}, "mscraper/walgreens-scraper": {"id": 2132, "url": "https://apify.com/mscraper/walgreens-scraper/api/client/nodejs", "title": "Apify API and Walgreens Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.walgreens.com/store/c/beer/ID=20000329-tier3\"\n        }\n    ],\n    \"proxy\": {\n        \"useApifyProxy\": true,\n        \"apifyProxyCountry\": \"US\"\n    },\n    \"resultsLimit\": 50\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mscraper/walgreens-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"proxy": {"useApifyProxy": true, "apifyProxyCountry": "US"}, "startUrls": [{"url": "https://www.walgreens.com/store/c/beer/ID=20000329-tier3"}], "resultsLimit": 50}, "actor_id": "mscraper/walgreens-scraper"}, "mshopik/summerboard-scraper": {"id": 2133, "url": "https://apify.com/mshopik/summerboard-scraper/api/client/nodejs", "title": "Apify API and Summerboard Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/summerboard-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/summerboard-scraper"}, "mshopik/tamino-scraper": {"id": 2134, "url": "https://apify.com/mshopik/tamino-scraper/api/client/nodejs", "title": "Apify API and Tamino Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/tamino-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/tamino-scraper"}, "mshopik/summersalt-scraper": {"id": 2135, "url": "https://apify.com/mshopik/summersalt-scraper/api/client/nodejs", "title": "Apify API and Summersalt Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/summersalt-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/summersalt-scraper"}, "mshopik/thirdlove-scraper": {"id": 2136, "url": "https://apify.com/mshopik/thirdlove-scraper/api/client/nodejs", "title": "Apify API and ThirdLove Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/thirdlove-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/thirdlove-scraper"}, "mshopik/pvolve-scraper": {"id": 2137, "url": "https://apify.com/mshopik/pvolve-scraper/api/client/nodejs", "title": "Apify API and P.volve Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/pvolve-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/pvolve-scraper"}, "mshopik/infograpia-scraper": {"id": 2138, "url": "https://apify.com/mshopik/infograpia-scraper/api/client/nodejs", "title": "Apify API and Infograpia Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/infograpia-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/infograpia-scraper"}, "mshopik/wolf-and-shepherd-scraper": {"id": 2139, "url": "https://apify.com/mshopik/wolf-and-shepherd-scraper/api/client/nodejs", "title": "Apify API and Wolf Shepherd Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/wolf-and-shepherd-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/wolf-and-shepherd-scraper"}, "mshopik/brilliant-scraper": {"id": 2141, "url": "https://apify.com/mshopik/brilliant-scraper/api/client/nodejs", "title": "Apify API and Brilliant Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/brilliant-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/brilliant-scraper"}, "mshopik/gemplers-scraper": {"id": 2142, "url": "https://apify.com/mshopik/gemplers-scraper/api/client/nodejs", "title": "Apify API and Gempler's Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/gemplers-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/gemplers-scraper"}, "mshopik/darn-tough-scraper": {"id": 2143, "url": "https://apify.com/mshopik/darn-tough-scraper/api/client/nodejs", "title": "Apify API and Darn Tough Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/darn-tough-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/darn-tough-scraper"}, "mshopik/leesar-scraper": {"id": 2144, "url": "https://apify.com/mshopik/leesar-scraper/api/client/nodejs", "title": "Apify API and Leesa\u00ae Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/leesar-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/leesar-scraper"}, "mshopik/nomatic-scraper": {"id": 2145, "url": "https://apify.com/mshopik/nomatic-scraper/api/client/nodejs", "title": "Apify API and NOMATIC Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/nomatic-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/nomatic-scraper"}, "mshopik/fear-of-god-scraper": {"id": 2146, "url": "https://apify.com/mshopik/fear-of-god-scraper/api/client/nodejs", "title": "Apify API and Fear of God Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/fear-of-god-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/fear-of-god-scraper"}, "mshopik/opt-telescopes-scraper": {"id": 2147, "url": "https://apify.com/mshopik/opt-telescopes-scraper/api/client/nodejs", "title": "Apify API and OPT Telescopes Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/opt-telescopes-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/opt-telescopes-scraper"}, "mshopik/paw-brands-pawcom-scraper": {"id": 2148, "url": "https://apify.com/mshopik/paw-brands-pawcom-scraper/api/client/nodejs", "title": "Apify API and Paw Brands Paw.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/paw-brands-pawcom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/paw-brands-pawcom-scraper"}, "mshopik/bombas-scraper": {"id": 2149, "url": "https://apify.com/mshopik/bombas-scraper/api/client/nodejs", "title": "Apify API and Bombas Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bombas-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bombas-scraper"}, "mshopik/darling-spring-scraper": {"id": 2150, "url": "https://apify.com/mshopik/darling-spring-scraper/api/client/nodejs", "title": "Apify API and Darling Spring Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 100,\n    \"proxyConfig\": {\n        \"useApifyProxy\": true\n    },\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/darling-spring-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "proxyConfig": {"useApifyProxy": true}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 100}, "actor_id": "mshopik/darling-spring-scraper"}, "mshopik/abode-diy-security-scraper": {"id": 2151, "url": "https://apify.com/mshopik/abode-diy-security-scraper/api/client/nodejs", "title": "Apify API and abode DIY Security Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/abode-diy-security-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/abode-diy-security-scraper"}, "mshopik/pura-vida-bracelets-scraper": {"id": 2152, "url": "https://apify.com/mshopik/pura-vida-bracelets-scraper/api/client/nodejs", "title": "Apify API and Pura Vida Bracelets Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/pura-vida-bracelets-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/pura-vida-bracelets-scraper"}, "mshopik/jonathan-adler-scraper": {"id": 2153, "url": "https://apify.com/mshopik/jonathan-adler-scraper/api/client/nodejs", "title": "Apify API and Jonathan Adler Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/jonathan-adler-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/jonathan-adler-scraper"}, "mshopik/skinnydip-london-scraper": {"id": 2154, "url": "https://apify.com/mshopik/skinnydip-london-scraper/api/client/nodejs", "title": "Apify API and Skinnydip London Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/skinnydip-london-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/skinnydip-london-scraper"}, "mshopik/peak-design-scraper": {"id": 2155, "url": "https://apify.com/mshopik/peak-design-scraper/api/client/nodejs", "title": "Apify API and Peak Design Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/peak-design-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/peak-design-scraper"}, "mshopik/sugarbearhair-scraper": {"id": 2156, "url": "https://apify.com/mshopik/sugarbearhair-scraper/api/client/nodejs", "title": "Apify API and SugarBearHair Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/sugarbearhair-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/sugarbearhair-scraper"}, "mshopik/pixel-union-scraper": {"id": 2157, "url": "https://apify.com/mshopik/pixel-union-scraper/api/client/nodejs", "title": "Apify API and Pixel Union Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/pixel-union-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/pixel-union-scraper"}, "canadesk/ping-web": {"id": 2158, "url": "https://apify.com/canadesk/ping-web/api/client/nodejs", "title": "Apify API and Ping the Web interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"www.google.com\",\n    \"proxy\": {\n        \"useApifyProxy\": true\n    }\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"canadesk/ping-web\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "www.google.com", "proxy": {"useApifyProxy": true}}, "actor_id": "canadesk/ping-web"}, "mshopik/voonikcom-scraper": {"id": 2159, "url": "https://apify.com/mshopik/voonikcom-scraper/api/client/nodejs", "title": "Apify API and voonik.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/voonikcom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/voonikcom-scraper"}, "mshopik/spigen-inc-scraper": {"id": 2160, "url": "https://apify.com/mshopik/spigen-inc-scraper/api/client/nodejs", "title": "Apify API and Spigen Inc Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/spigen-inc-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/spigen-inc-scraper"}, "mshopik/carolina-girls-scraper": {"id": 2161, "url": "https://apify.com/mshopik/carolina-girls-scraper/api/client/nodejs", "title": "Apify API and Carolina Girls Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/carolina-girls-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/carolina-girls-scraper"}, "mshopik/ine-scraper": {"id": 2162, "url": "https://apify.com/mshopik/ine-scraper/api/client/nodejs", "title": "Apify API and INE Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/ine-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/ine-scraper"}, "mshopik/moxielash-scraper": {"id": 2163, "url": "https://apify.com/mshopik/moxielash-scraper/api/client/nodejs", "title": "Apify API and MoxieLash Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/moxielash-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/moxielash-scraper"}, "mshopik/danish-design-store-scraper": {"id": 2164, "url": "https://apify.com/mshopik/danish-design-store-scraper/api/client/nodejs", "title": "Apify API and Danish Design Store Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/danish-design-store-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/danish-design-store-scraper"}, "mshopik/warmly-scraper": {"id": 2165, "url": "https://apify.com/mshopik/warmly-scraper/api/client/nodejs", "title": "Apify API and Warmly Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/warmly-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/warmly-scraper"}, "mshopik/credo-scraper": {"id": 2166, "url": "https://apify.com/mshopik/credo-scraper/api/client/nodejs", "title": "Apify API and Credo Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/credo-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/credo-scraper"}, "mshopik/tomboyx-scraper": {"id": 2167, "url": "https://apify.com/mshopik/tomboyx-scraper/api/client/nodejs", "title": "Apify API and TomboyX Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/tomboyx-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/tomboyx-scraper"}, "mshopik/tracksmith-scraper": {"id": 2168, "url": "https://apify.com/mshopik/tracksmith-scraper/api/client/nodejs", "title": "Apify API and Tracksmith Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/tracksmith-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/tracksmith-scraper"}, "mshopik/shapermint-scraper": {"id": 2169, "url": "https://apify.com/mshopik/shapermint-scraper/api/client/nodejs", "title": "Apify API and Shapermint Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/shapermint-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/shapermint-scraper"}, "mshopik/dressbarn-scraper": {"id": 2170, "url": "https://apify.com/mshopik/dressbarn-scraper/api/client/nodejs", "title": "Apify API and Dressbarn Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/dressbarn-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/dressbarn-scraper"}, "mshopik/shinesty-scraper": {"id": 2171, "url": "https://apify.com/mshopik/shinesty-scraper/api/client/nodejs", "title": "Apify API and Shinesty Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/shinesty-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/shinesty-scraper"}, "mshopik/svs-scraper": {"id": 2172, "url": "https://apify.com/mshopik/svs-scraper/api/client/nodejs", "title": "Apify API and SVS Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/svs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/svs-scraper"}, "mshopik/sweat-scraper": {"id": 2173, "url": "https://apify.com/mshopik/sweat-scraper/api/client/nodejs", "title": "Apify API and SWEAT Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/sweat-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/sweat-scraper"}, "mshopik/white-fox-boutique-au-scraper": {"id": 2174, "url": "https://apify.com/mshopik/white-fox-boutique-au-scraper/api/client/nodejs", "title": "Apify API and White Fox Boutique AU Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/white-fox-boutique-au-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/white-fox-boutique-au-scraper"}, "mshopik/rad-power-bikes-scraper": {"id": 2175, "url": "https://apify.com/mshopik/rad-power-bikes-scraper/api/client/nodejs", "title": "Apify API and Rad Power Bikes Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/rad-power-bikes-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/rad-power-bikes-scraper"}, "mshopik/sodastream-scraper": {"id": 2176, "url": "https://apify.com/mshopik/sodastream-scraper/api/client/nodejs", "title": "Apify API and SodaStream Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/sodastream-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/sodastream-scraper"}, "mshopik/scandinavian-designs-scraper": {"id": 2177, "url": "https://apify.com/mshopik/scandinavian-designs-scraper/api/client/nodejs", "title": "Apify API and Scandinavian Designs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/scandinavian-designs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/scandinavian-designs-scraper"}, "mshopik/nearly-natural-scraper": {"id": 2178, "url": "https://apify.com/mshopik/nearly-natural-scraper/api/client/nodejs", "title": "Apify API and Nearly Natural Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/nearly-natural-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/nearly-natural-scraper"}, "mshopik/nanamacs-scraper": {"id": 2179, "url": "https://apify.com/mshopik/nanamacs-scraper/api/client/nodejs", "title": "Apify API and NanaMacs Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/nanamacs-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/nanamacs-scraper"}, "mshopik/mvmt-scraper": {"id": 2180, "url": "https://apify.com/mshopik/mvmt-scraper/api/client/nodejs", "title": "Apify API and MVMT Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/mvmt-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/mvmt-scraper"}, "mshopik/robinsons-scraper": {"id": 2181, "url": "https://apify.com/mshopik/robinsons-scraper/api/client/nodejs", "title": "Apify API and Robinsons Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/robinsons-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/robinsons-scraper"}, "mshopik/lume-scraper": {"id": 2182, "url": "https://apify.com/mshopik/lume-scraper/api/client/nodejs", "title": "Apify API and Lume Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/lume-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/lume-scraper"}, "mshopik/metal-unlimited-scraper": {"id": 2183, "url": "https://apify.com/mshopik/metal-unlimited-scraper/api/client/nodejs", "title": "Apify API and Metal Unlimited Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/metal-unlimited-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/metal-unlimited-scraper"}, "mshopik/quay-australia-scraper": {"id": 2184, "url": "https://apify.com/mshopik/quay-australia-scraper/api/client/nodejs", "title": "Apify API and Quay Australia Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/quay-australia-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/quay-australia-scraper"}, "mshopik/steve-madden-scraper": {"id": 2185, "url": "https://apify.com/mshopik/steve-madden-scraper/api/client/nodejs", "title": "Apify API and Steve Madden Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/steve-madden-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/steve-madden-scraper"}, "mshopik/honeylover-scraper": {"id": 2186, "url": "https://apify.com/mshopik/honeylover-scraper/api/client/nodejs", "title": "Apify API and Honeylove\u00ae Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/honeylover-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/honeylover-scraper"}, "mshopik/reason-clothing-scraper": {"id": 2187, "url": "https://apify.com/mshopik/reason-clothing-scraper/api/client/nodejs", "title": "Apify API and Reason Clothing Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/reason-clothing-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/reason-clothing-scraper"}, "mshopik/ugreen-scraper": {"id": 2188, "url": "https://apify.com/mshopik/ugreen-scraper/api/client/nodejs", "title": "Apify API and UGREEN Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/ugreen-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/ugreen-scraper"}, "mshopik/cuup-scraper": {"id": 2189, "url": "https://apify.com/mshopik/cuup-scraper/api/client/nodejs", "title": "Apify API and CUUP Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/cuup-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/cuup-scraper"}, "mshopik/juvias-place-scraper": {"id": 2190, "url": "https://apify.com/mshopik/juvias-place-scraper/api/client/nodejs", "title": "Apify API and Juvia\u2019s Place Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/juvias-place-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/juvias-place-scraper"}, "mshopik/draper-james-scraper": {"id": 2191, "url": "https://apify.com/mshopik/draper-james-scraper/api/client/nodejs", "title": "Apify API and Draper James Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/draper-james-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/draper-james-scraper"}, "mshopik/the-4ocean-bracelet-scraper": {"id": 2192, "url": "https://apify.com/mshopik/the-4ocean-bracelet-scraper/api/client/nodejs", "title": "Apify API and The 4ocean Bracelet Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-4ocean-bracelet-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/the-4ocean-bracelet-scraper"}, "mshopik/outdoor-voices-scraper": {"id": 2193, "url": "https://apify.com/mshopik/outdoor-voices-scraper/api/client/nodejs", "title": "Apify API and Outdoor Voices Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/outdoor-voices-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/outdoor-voices-scraper"}, "mshopik/zensah-scraper": {"id": 2194, "url": "https://apify.com/mshopik/zensah-scraper/api/client/nodejs", "title": "Apify API and Zensah Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/zensah-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/zensah-scraper"}, "mshopik/show-me-your-mumu-scraper": {"id": 2195, "url": "https://apify.com/mshopik/show-me-your-mumu-scraper/api/client/nodejs", "title": "Apify API and Show Me Your Mumu Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/show-me-your-mumu-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/show-me-your-mumu-scraper"}, "mshopik/jungalow-scraper": {"id": 2196, "url": "https://apify.com/mshopik/jungalow-scraper/api/client/nodejs", "title": "Apify API and Jungalow Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/jungalow-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/jungalow-scraper"}, "mshopik/promescent-scraper": {"id": 2197, "url": "https://apify.com/mshopik/promescent-scraper/api/client/nodejs", "title": "Apify API and Promescent Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/promescent-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/promescent-scraper"}, "mshopik/bebe-scraper": {"id": 2198, "url": "https://apify.com/mshopik/bebe-scraper/api/client/nodejs", "title": "Apify API and bebe Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/bebe-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/bebe-scraper"}, "mshopik/john-elliott-scraper": {"id": 2199, "url": "https://apify.com/mshopik/john-elliott-scraper/api/client/nodejs", "title": "Apify API and JOHN ELLIOTT Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/john-elliott-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/john-elliott-scraper"}, "curious_coder/facebook-event-guests-scraper": {"id": 2200, "url": "https://apify.com/curious_coder/facebook-event-guests-scraper/api/client/nodejs", "title": "Apify API and Facebook event guests scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.facebook.com/events/6760759933959274\",\n    \"minDelay\": 1,\n    \"maxDelay\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"curious_coder/facebook-event-guests-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.facebook.com/events/6760759933959274", "maxDelay": 3, "minDelay": 1}, "actor_id": "curious_coder/facebook-event-guests-scraper"}, "mshopik/dixie-mech-scraper": {"id": 2201, "url": "https://apify.com/mshopik/dixie-mech-scraper/api/client/nodejs", "title": "Apify API and Dixie Mech Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/dixie-mech-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/dixie-mech-scraper"}, "mshopik/rylee-cru-scraper": {"id": 2202, "url": "https://apify.com/mshopik/rylee-cru-scraper/api/client/nodejs", "title": "Apify API and Rylee Cru Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/rylee-cru-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/rylee-cru-scraper"}, "mshopik/deadstockca-scraper": {"id": 2203, "url": "https://apify.com/mshopik/deadstockca-scraper/api/client/nodejs", "title": "Apify API and Deadstock.ca Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/deadstockca-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/deadstockca-scraper"}, "mshopik/alex-and-ani-scraper": {"id": 2204, "url": "https://apify.com/mshopik/alex-and-ani-scraper/api/client/nodejs", "title": "Apify API and ALEX AND ANI Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/alex-and-ani-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/alex-and-ani-scraper"}, "mshopik/nonda-scraper": {"id": 2205, "url": "https://apify.com/mshopik/nonda-scraper/api/client/nodejs", "title": "Apify API and nonda Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/nonda-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/nonda-scraper"}, "mshopik/igloo-scraper": {"id": 2206, "url": "https://apify.com/mshopik/igloo-scraper/api/client/nodejs", "title": "Apify API and Igloo Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/igloo-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/igloo-scraper"}, "mshopik/topatoco-scraper": {"id": 2207, "url": "https://apify.com/mshopik/topatoco-scraper/api/client/nodejs", "title": "Apify API and TopatoCo Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/topatoco-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/topatoco-scraper"}, "mshopik/the-gld-shop-scraper": {"id": 2208, "url": "https://apify.com/mshopik/the-gld-shop-scraper/api/client/nodejs", "title": "Apify API and The GLD Shop Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/the-gld-shop-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/the-gld-shop-scraper"}, "mshopik/chubbies-shorts-scraper": {"id": 2209, "url": "https://apify.com/mshopik/chubbies-shorts-scraper/api/client/nodejs", "title": "Apify API and Chubbies Shorts Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/chubbies-shorts-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/chubbies-shorts-scraper"}, "juansgaitan/scheduler-from-spreadsheet": {"id": 2210, "url": "https://apify.com/juansgaitan/scheduler-from-spreadsheet/api/client/nodejs", "title": "Apify API and Scheduler From Spreadsheet interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"juansgaitan/scheduler-from-spreadsheet\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "juansgaitan/scheduler-from-spreadsheet"}, "lexis-solutions/tiktok-ads-scraper": {"id": 2211, "url": "https://apify.com/lexis-solutions/tiktok-ads-scraper/api/client/nodejs", "title": "Apify API and TikTok Ads Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"query\": \"apify\",\n    \"startDate\": \"2023-01-01\",\n    \"endDate\": \"2023-01-31\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"lexis-solutions/tiktok-ads-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"query": "apify", "endDate": "2023-01-31", "startDate": "2023-01-01"}, "actor_id": "lexis-solutions/tiktok-ads-scraper"}, "mshopik/village-kids-scraper": {"id": 2212, "url": "https://apify.com/mshopik/village-kids-scraper/api/client/nodejs", "title": "Apify API and Village Kids Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/village-kids-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/village-kids-scraper"}, "mshopik/brydge-scraper": {"id": 2213, "url": "https://apify.com/mshopik/brydge-scraper/api/client/nodejs", "title": "Apify API and Brydge Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/brydge-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/brydge-scraper"}, "elite_necklace/my-actor": {"id": 2214, "url": "https://apify.com/elite_necklace/my-actor/api/client/nodejs", "title": "Apify API and Test actor interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"url\": \"https://www.apify.com/\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"elite_necklace/my-actor\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"url": "https://www.apify.com/"}, "actor_id": "elite_necklace/my-actor"}, "malu_alvarado/web-scraper-task-malu-test": {"id": 2215, "url": "https://apify.com/malu_alvarado/web-scraper-task-malu-test/api/client/nodejs", "title": "Apify API and Web Scraper Task Malu Practice 1 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"category\": \"briefing\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"malu_alvarado/web-scraper-task-malu-test\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"category": "briefing"}, "actor_id": "malu_alvarado/web-scraper-task-malu-test"}, "mshopik/keysmart-premium-scraper": {"id": 2216, "url": "https://apify.com/mshopik/keysmart-premium-scraper/api/client/nodejs", "title": "Apify API and KeySmart Premium Key Holders Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/keysmart-premium-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/keysmart-premium-scraper"}, "mshopik/taylor-stitch-scraper": {"id": 2217, "url": "https://apify.com/mshopik/taylor-stitch-scraper/api/client/nodejs", "title": "Apify API and Taylor Stitch Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/taylor-stitch-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/taylor-stitch-scraper"}, "mshopik/andie-scraper": {"id": 2218, "url": "https://apify.com/mshopik/andie-scraper/api/client/nodejs", "title": "Apify API and Andie Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/andie-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/andie-scraper"}, "mshopik/hanon-scraper": {"id": 2219, "url": "https://apify.com/mshopik/hanon-scraper/api/client/nodejs", "title": "Apify API and HANON Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/hanon-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/hanon-scraper"}, "mshopik/mandaue-foam-philippines-scraper": {"id": 2220, "url": "https://apify.com/mshopik/mandaue-foam-philippines-scraper/api/client/nodejs", "title": "Apify API and Mandaue Foam Philippines Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/mandaue-foam-philippines-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/mandaue-foam-philippines-scraper"}, "mshopik/moscot-scraper": {"id": 2221, "url": "https://apify.com/mshopik/moscot-scraper/api/client/nodejs", "title": "Apify API and MOSCOT Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/moscot-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/moscot-scraper"}, "mshopik/dagne-dover-scraper": {"id": 2222, "url": "https://apify.com/mshopik/dagne-dover-scraper/api/client/nodejs", "title": "Apify API and Dagne Dover Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/dagne-dover-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/dagne-dover-scraper"}, "mshopik/petlab-co-scraper": {"id": 2223, "url": "https://apify.com/mshopik/petlab-co-scraper/api/client/nodejs", "title": "Apify API and Petlab Co. Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/petlab-co-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/petlab-co-scraper"}, "mshopik/tom-bihn-scraper": {"id": 2224, "url": "https://apify.com/mshopik/tom-bihn-scraper/api/client/nodejs", "title": "Apify API and TOM BIHN Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/tom-bihn-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/tom-bihn-scraper"}, "mshopik/zinus-scraper": {"id": 2225, "url": "https://apify.com/mshopik/zinus-scraper/api/client/nodejs", "title": "Apify API and Zinus Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/zinus-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/zinus-scraper"}, "mshopik/yogaoutletcom-scraper": {"id": 2227, "url": "https://apify.com/mshopik/yogaoutletcom-scraper/api/client/nodejs", "title": "Apify API and YogaOutlet.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/yogaoutletcom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/yogaoutletcom-scraper"}, "mshopik/oxford-shop-scraper": {"id": 2228, "url": "https://apify.com/mshopik/oxford-shop-scraper/api/client/nodejs", "title": "Apify API and Oxford Shop Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/oxford-shop-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/oxford-shop-scraper"}, "david.lukac/the-daily-beast-scraper": {"id": 2229, "url": "https://apify.com/david.lukac/the-daily-beast-scraper/api/client/nodejs", "title": "Apify API and The Daily Beast Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"startUrls\": [\n        {\n            \"url\": \"https://www.thedailybeast.com/\"\n        }\n    ],\n    \"maxArticlesPerCrawl\": 100\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"david.lukac/the-daily-beast-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"startUrls": [{"url": "https://www.thedailybeast.com/"}], "maxArticlesPerCrawl": 100}, "actor_id": "david.lukac/the-daily-beast-scraper"}, "mshopik/liewood-scraper": {"id": 2230, "url": "https://apify.com/mshopik/liewood-scraper/api/client/nodejs", "title": "Apify API and LIEWOOD Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/liewood-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/liewood-scraper"}, "mshopik/lion-brand-yarn-scraper": {"id": 2231, "url": "https://apify.com/mshopik/lion-brand-yarn-scraper/api/client/nodejs", "title": "Apify API and Lion Brand Yarn Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/lion-brand-yarn-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/lion-brand-yarn-scraper"}, "mshopik/beginning-boutique-scraper": {"id": 2232, "url": "https://apify.com/mshopik/beginning-boutique-scraper/api/client/nodejs", "title": "Apify API and Beginning Boutique Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/beginning-boutique-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/beginning-boutique-scraper"}, "mshopik/lange-hair-scraper": {"id": 2233, "url": "https://apify.com/mshopik/lange-hair-scraper/api/client/nodejs", "title": "Apify API and L'ange Hair Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/lange-hair-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/lange-hair-scraper"}, "mshopik/rebecca-minkoff-scraper": {"id": 2234, "url": "https://apify.com/mshopik/rebecca-minkoff-scraper/api/client/nodejs", "title": "Apify API and Rebecca Minkoff Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/rebecca-minkoff-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/rebecca-minkoff-scraper"}, "mshopik/fleshlight-scraper": {"id": 2235, "url": "https://apify.com/mshopik/fleshlight-scraper/api/client/nodejs", "title": "Apify API and Fleshlight Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/fleshlight-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/fleshlight-scraper"}, "mshopik/afloralcom-scraper": {"id": 2236, "url": "https://apify.com/mshopik/afloralcom-scraper/api/client/nodejs", "title": "Apify API and Afloral.com Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/afloralcom-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/afloralcom-scraper"}, "jkuzz/github-issue-notifier": {"id": 2237, "url": "https://apify.com/jkuzz/github-issue-notifier/api/client/nodejs", "title": "Apify API and GitHub Issue Notifier interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchRepos\": [],\n    \"searchKeywords\": [],\n    \"slackChannel\": \"#general\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"jkuzz/github-issue-notifier\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"searchRepos": [], "slackChannel": "#general", "searchKeywords": []}, "actor_id": "jkuzz/github-issue-notifier"}, "landgeek/landmodo-scraper": {"id": 2238, "url": "https://apify.com/landgeek/landmodo-scraper/api/client/nodejs", "title": "Apify API and Landmodo Page Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"searchUrl\": \"https://www.landmodo.com/properties?\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"landgeek/landmodo-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"searchUrl": "https://www.landmodo.com/properties?"}, "actor_id": "landgeek/landmodo-scraper"}, "qpayre/bien-ici-scraper": {"id": 2239, "url": "https://apify.com/qpayre/bien-ici-scraper/api/client/nodejs", "title": "Apify API and Bien ici Scraper \ud83c\udfe0 interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"qpayre/bien-ici-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {}, "actor_id": "qpayre/bien-ici-scraper"}, "mshopik/brooklinen-scraper": {"id": 2240, "url": "https://apify.com/mshopik/brooklinen-scraper/api/client/nodejs", "title": "Apify API and Brooklinen Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/brooklinen-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/brooklinen-scraper"}, "mshopik/chase-value-centre-scraper": {"id": 2241, "url": "https://apify.com/mshopik/chase-value-centre-scraper/api/client/nodejs", "title": "Apify API and Chase Value Centre Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/chase-value-centre-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/chase-value-centre-scraper"}, "mshopik/clare-v-scraper": {"id": 2242, "url": "https://apify.com/mshopik/clare-v-scraper/api/client/nodejs", "title": "Apify API and Clare V. Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/clare-v-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/clare-v-scraper"}, "mshopik/jimmy-jazz-scraper": {"id": 2243, "url": "https://apify.com/mshopik/jimmy-jazz-scraper/api/client/nodejs", "title": "Apify API and Jimmy Jazz Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/jimmy-jazz-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/jimmy-jazz-scraper"}, "mshopik/soko-glam-scraper": {"id": 2244, "url": "https://apify.com/mshopik/soko-glam-scraper/api/client/nodejs", "title": "Apify API and Soko Glam Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/soko-glam-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/soko-glam-scraper"}, "mshopik/kingice-scraper": {"id": 2245, "url": "https://apify.com/mshopik/kingice-scraper/api/client/nodejs", "title": "Apify API and KingIce Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/kingice-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/kingice-scraper"}, "useful-tools/wait-and-finish": {"id": 2246, "url": "https://apify.com/useful-tools/wait-and-finish/api/client/nodejs", "title": "Apify API and Wait and Finish interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"fail\": \"\",\n    \"dataset\": {\n        \"id\": null,\n        \"items\": [\n            {\n                \"id\": 1,\n                \"text\": \"My first item\"\n            },\n            {\n                \"id\": 2,\n                \"text\": \"My second item\"\n            }\n        ]\n    },\n    \"output\": {\n        \"outputMessage\": \"Actor finished\"\n    },\n    \"statusMessage\": \"\"\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"useful-tools/wait-and-finish\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"fail": "", "output": {"outputMessage": "Actor finished"}, "dataset": {"id": null, "items": [{"id": 1, "text": "My first item"}, {"id": 2, "text": "My second item"}]}, "statusMessage": ""}, "actor_id": "useful-tools/wait-and-finish"}, "mshopik/vietri-scraper": {"id": 2247, "url": "https://apify.com/mshopik/vietri-scraper/api/client/nodejs", "title": "Apify API and VIETRI Scraper interaction with Node.js", "example_code": "import { ApifyClient } from 'apify-client';\n\n// Initialize the ApifyClient with API token\nconst client = new ApifyClient({\n    token: '<YOUR_API_TOKEN>',\n});\n\n// Prepare Actor input\nconst input = {\n    \"maxRequestsPerCrawl\": 20,\n    \"extendOutputFunction\": async ({ data, item, product, images, fns, name, request, variants, context, customData, input, Apify }) => {\n      return item;\n    },\n    \"extendScraperFunction\": async ({ fns, customData, Apify, label }) => {\n     \n    },\n    \"customData\": {},\n    \"maxConcurrency\": 20,\n    \"maxRequestRetries\": 3\n};\n\n(async () => {\n    // Run the Actor and wait for it to finish\n    const run = await client.actor(\"mshopik/vietri-scraper\").call(input);\n\n    // Fetch and print Actor results from the run's dataset (if any)\n    console.log('Results from dataset');\n    const { items } = await client.dataset(run.defaultDatasetId).listItems();\n    items.forEach((item) => {\n        console.dir(item);\n    });\n})();\n", "input_example": {"customData": {}, "maxConcurrency": 20, "maxRequestRetries": 3, "maxRequestsPerCrawl": 20}, "actor_id": "mshopik/vietri-scraper"}}